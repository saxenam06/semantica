{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supply Chain End-to-End\n",
        "\n",
        "## Overview\n",
        "\n",
        "Complete supply chain intelligence: multi-source data ingestion, build supply chain knowledge graph, analyze dependencies, optimize flow, and predict disruptions.\n",
        "\n",
        "## Workflow: Multi-Source Data → Build Supply Chain KG → Analyze Dependencies → Optimize → Predict Disruptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FileIngestor, WebIngestor, DBIngestor, StreamIngestor\n",
        "from semantica.parse import DocumentParser, WebParser, StructuredDataParser\n",
        "from semantica.semantic_extract import NERExtractor, RelationExtractor\n",
        "from semantica.kg import GraphBuilder, GraphAnalyzer, ConnectivityAnalyzer, TemporalPatternDetector\n",
        "from semantica.reasoning import InferenceEngine\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import tempfile\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Multi-Source Data Ingestion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_ingestor = FileIngestor()\n",
        "web_ingestor = WebIngestor()\n",
        "db_ingestor = DBIngestor()\n",
        "stream_ingestor = StreamIngestor()\n",
        "document_parser = DocumentParser()\n",
        "web_parser = WebParser()\n",
        "structured_parser = StructuredDataParser()\n",
        "\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "report_file = os.path.join(temp_dir, \"supply_chain_report.txt\")\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(\"Supplier A delivers components to Factory B. Supplier C supplies Factory D.\")\n",
        "\n",
        "file_objects = file_ingestor.ingest_file(report_file, read_content=True)\n",
        "parsed_file_content = document_parser.extract_text(report_file) if file_objects else \"\"\n",
        "\n",
        "web_content = \"Shipping delays reported in region X due to weather conditions.\"\n",
        "parsed_web_content = web_parser.parse_text(web_content) if web_content else \"\"\n",
        "\n",
        "db_data_file = os.path.join(temp_dir, \"suppliers_db.json\")\n",
        "db_data = [\n",
        "    {\"supplier\": \"Supplier A\", \"factory\": \"Factory B\", \"status\": \"active\"},\n",
        "    {\"supplier\": \"Supplier C\", \"factory\": \"Factory D\", \"status\": \"active\"}\n",
        "]\n",
        "with open(db_data_file, 'w') as f:\n",
        "    json.dump(db_data, f)\n",
        "\n",
        "parsed_db_data = structured_parser.parse_json(db_data_file)\n",
        "\n",
        "stream_events = [\n",
        "    {\"event\": \"shipment_delayed\", \"supplier\": \"Supplier A\", \"timestamp\": datetime.now().isoformat()}\n",
        "]\n",
        "\n",
        "all_data = []\n",
        "if parsed_file_content:\n",
        "    all_data.append({\"source\": \"file\", \"content\": parsed_file_content, \"type\": \"report\"})\n",
        "if parsed_web_content:\n",
        "    all_data.append({\"source\": \"web\", \"content\": parsed_web_content, \"type\": \"news\"})\n",
        "for db_record in parsed_db_data.get(\"data\", db_data):\n",
        "    all_data.append({\"source\": \"db\", **db_record})\n",
        "for stream_event in stream_events:\n",
        "    all_data.append({\"source\": \"stream\", **stream_event})\n",
        "\n",
        "print(f\"Ingested data from {len(set(d.get('source') for d in all_data))} sources\")\n",
        "print(f\"  File sources: {len([d for d in all_data if d.get('source') == 'file'])}\")\n",
        "print(f\"  Web sources: {len([d for d in all_data if d.get('source') == 'web'])}\")\n",
        "print(f\"  Database sources: {len([d for d in all_data if d.get('source') == 'db'])}\")\n",
        "print(f\"  Stream sources: {len([d for d in all_data if d.get('source') == 'stream'])}\")\n",
        "print(f\"Total data items: {len(all_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build Supply Chain Knowledge Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner_extractor = NERExtractor()\n",
        "relation_extractor = RelationExtractor()\n",
        "builder = GraphBuilder()\n",
        "\n",
        "supply_chain_entities = []\n",
        "relationships = []\n",
        "entity_map = {}\n",
        "\n",
        "for data_item in all_data:\n",
        "    content = data_item.get(\"content\", \"\")\n",
        "    if not content:\n",
        "        content = str(data_item)\n",
        "    \n",
        "    extracted_entities = ner_extractor.extract(content)\n",
        "    extracted_relations = relation_extractor.extract(content, extracted_entities)\n",
        "    \n",
        "    for entity in extracted_entities:\n",
        "        entity_text = entity.get(\"text\", entity.get(\"entity\", \"\"))\n",
        "        entity_type = entity.get(\"type\", \"Entity\")\n",
        "        \n",
        "        if entity_text and entity_text not in entity_map:\n",
        "            entity_id = entity_text.lower().replace(\" \", \"_\")\n",
        "            entity_map[entity_text] = entity_id\n",
        "            \n",
        "            if \"supplier\" in entity_text.lower() or \"supplier\" in entity_type.lower():\n",
        "                entity_type = \"Supplier\"\n",
        "            elif \"factory\" in entity_text.lower() or \"factory\" in entity_type.lower():\n",
        "                entity_type = \"Factory\"\n",
        "            elif \"warehouse\" in entity_text.lower():\n",
        "                entity_type = \"Warehouse\"\n",
        "            elif \"product\" in entity_text.lower():\n",
        "                entity_type = \"Product\"\n",
        "            \n",
        "            supply_chain_entities.append({\n",
        "                \"id\": entity_id,\n",
        "                \"type\": entity_type,\n",
        "                \"name\": entity_text,\n",
        "                \"properties\": {}\n",
        "            })\n",
        "    \n",
        "    for rel in extracted_relations:\n",
        "        source_text = rel.get(\"source\", \"\")\n",
        "        target_text = rel.get(\"target\", \"\")\n",
        "        rel_type = rel.get(\"type\", \"related_to\")\n",
        "        \n",
        "        if source_text in entity_map and target_text in entity_map:\n",
        "            relationships.append({\n",
        "                \"source\": entity_map[source_text],\n",
        "                \"target\": entity_map[target_text],\n",
        "                \"type\": rel_type,\n",
        "                \"properties\": {\"timestamp\": datetime.now().isoformat()}\n",
        "            })\n",
        "\n",
        "for db_record in parsed_db_data.get(\"data\", db_data):\n",
        "    supplier_name = db_record.get(\"supplier\", \"\")\n",
        "    factory_name = db_record.get(\"factory\", \"\")\n",
        "    \n",
        "    if supplier_name and factory_name:\n",
        "        supplier_id = supplier_name.lower().replace(\" \", \"_\")\n",
        "        factory_id = factory_name.lower().replace(\" \", \"_\")\n",
        "        \n",
        "        if supplier_id not in entity_map:\n",
        "            entity_map[supplier_name] = supplier_id\n",
        "            supply_chain_entities.append({\n",
        "                \"id\": supplier_id,\n",
        "                \"type\": \"Supplier\",\n",
        "                \"name\": supplier_name,\n",
        "                \"properties\": {}\n",
        "            })\n",
        "        \n",
        "        if factory_id not in entity_map:\n",
        "            entity_map[factory_name] = factory_id\n",
        "            supply_chain_entities.append({\n",
        "                \"id\": factory_id,\n",
        "                \"type\": \"Factory\",\n",
        "                \"name\": factory_name,\n",
        "                \"properties\": {\"capacity\": 1000}\n",
        "            })\n",
        "        \n",
        "        relationships.append({\n",
        "            \"source\": supplier_id,\n",
        "            \"target\": factory_id,\n",
        "            \"type\": \"supplies\",\n",
        "            \"properties\": {\"timestamp\": datetime.now().isoformat(), \"status\": \"active\"}\n",
        "        })\n",
        "\n",
        "if \"warehouse_1\" not in entity_map:\n",
        "    supply_chain_entities.append({\n",
        "        \"id\": \"warehouse_1\",\n",
        "        \"type\": \"Warehouse\",\n",
        "        \"name\": \"Warehouse 1\",\n",
        "        \"properties\": {}\n",
        "    })\n",
        "    entity_map[\"Warehouse 1\"] = \"warehouse_1\"\n",
        "\n",
        "if \"factory_b\" in entity_map and \"warehouse_1\" in entity_map:\n",
        "    relationships.append({\n",
        "        \"source\": \"factory_b\",\n",
        "        \"target\": \"warehouse_1\",\n",
        "        \"type\": \"ships_to\",\n",
        "        \"properties\": {\"timestamp\": datetime.now().isoformat()}\n",
        "    })\n",
        "\n",
        "supply_chain_kg = builder.build(supply_chain_entities, relationships)\n",
        "\n",
        "print(f\"Extracted {len([e for e in supply_chain_entities if e['type'] in ['Supplier', 'Factory']])} supply chain entities from parsed data\")\n",
        "print(f\"Built supply chain knowledge graph with {len(supply_chain_entities)} entities and {len(relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Analyze Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyzer = GraphAnalyzer()\n",
        "connectivity_analyzer = ConnectivityAnalyzer()\n",
        "\n",
        "connectivity_analysis = connectivity_analyzer.analyze_connectivity(supply_chain_kg)\n",
        "graph_metrics = analyzer.compute_metrics(supply_chain_kg)\n",
        "\n",
        "entities_list = supply_chain_kg.get(\"entities\", [])\n",
        "relationships_list = supply_chain_kg.get(\"relationships\", [])\n",
        "entity_map = {e.get(\"id\"): e for e in entities_list}\n",
        "\n",
        "dependencies = []\n",
        "for entity in entities_list:\n",
        "    entity_id = entity.get(\"id\")\n",
        "    incoming = [r for r in relationships_list if r.get(\"target\") == entity_id]\n",
        "    outgoing = [r for r in relationships_list if r.get(\"source\") == entity_id]\n",
        "    \n",
        "    if incoming or outgoing:\n",
        "        dependencies.append({\n",
        "            \"entity_id\": entity_id,\n",
        "            \"entity_type\": entity.get(\"type\"),\n",
        "            \"name\": entity.get(\"name\"),\n",
        "            \"incoming_dependencies\": len(incoming),\n",
        "            \"outgoing_dependencies\": len(outgoing),\n",
        "            \"depends_on\": [entity_map.get(r.get(\"source\"), {}).get(\"name\", r.get(\"source\")) for r in incoming if entity_map.get(r.get(\"source\"))],\n",
        "            \"supports\": [entity_map.get(r.get(\"target\"), {}).get(\"name\", r.get(\"target\")) for r in outgoing if entity_map.get(r.get(\"target\"))]\n",
        "        })\n",
        "\n",
        "print(f\"Analyzed dependencies for {len(dependencies)} entities\")\n",
        "print(f\"Graph connectivity: {connectivity_analysis.get('is_connected', False)}\")\n",
        "print(f\"Connected components: {len(connectivity_analysis.get('components', []))}\")\n",
        "for dep in dependencies:\n",
        "    print(f\"  {dep['name']} ({dep['entity_type']}): {dep['incoming_dependencies']} incoming, {dep['outgoing_dependencies']} outgoing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Optimize Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_engine = InferenceEngine()\n",
        "\n",
        "inference_engine.add_rule(\"IF factory has less than 2 suppliers THEN suggest add_redundancy\")\n",
        "inference_engine.add_rule(\"IF entity has no incoming dependencies AND has more than 2 outgoing THEN suggest bottleneck_mitigation\")\n",
        "\n",
        "optimization_facts = []\n",
        "for dep in dependencies:\n",
        "    if dep[\"entity_type\"] == \"Factory\" and dep[\"incoming_dependencies\"] < 2:\n",
        "        optimization_facts.append({\n",
        "            \"entity\": dep[\"entity_id\"],\n",
        "            \"type\": \"factory\",\n",
        "            \"supplier_count\": dep[\"incoming_dependencies\"]\n",
        "        })\n",
        "    if dep[\"incoming_dependencies\"] == 0 and dep[\"outgoing_dependencies\"] > 2:\n",
        "        optimization_facts.append({\n",
        "            \"entity\": dep[\"entity_id\"],\n",
        "            \"type\": \"bottleneck\",\n",
        "            \"outgoing_count\": dep[\"outgoing_dependencies\"]\n",
        "        })\n",
        "\n",
        "if optimization_facts:\n",
        "    inference_engine.add_facts(optimization_facts)\n",
        "    inferred_results = inference_engine.forward_chain()\n",
        "else:\n",
        "    inferred_results = []\n",
        "\n",
        "optimized_flow = []\n",
        "factories = [e for e in entities_list if e.get(\"type\") == \"Factory\"]\n",
        "for factory in factories:\n",
        "    factory_id = factory.get(\"id\")\n",
        "    incoming = [r for r in relationships_list if r.get(\"target\") == factory_id and r.get(\"type\") == \"supplies\"]\n",
        "    if len(incoming) < 2:\n",
        "        optimized_flow.append({\n",
        "            \"type\": \"add_redundancy\",\n",
        "            \"entity\": factory.get(\"name\"),\n",
        "            \"suggestion\": f\"Add backup supplier for {factory.get('name')} to reduce risk\"\n",
        "        })\n",
        "\n",
        "bottlenecks = [d for d in dependencies if d[\"incoming_dependencies\"] == 0 and d[\"outgoing_dependencies\"] > 2]\n",
        "if bottlenecks:\n",
        "    optimized_flow.append({\n",
        "        \"type\": \"bottleneck_detected\",\n",
        "        \"entities\": [b[\"name\"] for b in bottlenecks],\n",
        "        \"suggestion\": \"Consider adding parallel paths for critical nodes\"\n",
        "    })\n",
        "\n",
        "if inferred_results:\n",
        "    print(f\"Inference engine generated {len(inferred_results)} optimization inferences\")\n",
        "\n",
        "print(f\"Generated {len(optimized_flow)} optimization suggestions\")\n",
        "for suggestion in optimized_flow:\n",
        "    print(f\"  {suggestion['type']}: {suggestion['suggestion']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Predict Disruptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern_detector = TemporalPatternDetector()\n",
        "\n",
        "temporal_patterns = pattern_detector.detect_temporal_patterns(\n",
        "    supply_chain_kg,\n",
        "    pattern_type=\"anomaly\",\n",
        "    min_frequency=1\n",
        ")\n",
        "\n",
        "disruptions = []\n",
        "\n",
        "delay_events = [d for d in all_data if d.get(\"event\") == \"shipment_delayed\" or \"delay\" in str(d.get(\"content\", \"\")).lower()]\n",
        "\n",
        "if delay_events:\n",
        "    disruptions.append({\n",
        "        \"type\": \"delivery_delay\",\n",
        "        \"severity\": \"high\",\n",
        "        \"affected_entities\": [\"Supplier A\"],\n",
        "        \"description\": \"Shipping delays detected in supply chain\",\n",
        "        \"recommendation\": \"Activate backup suppliers or adjust production schedules\"\n",
        "    })\n",
        "\n",
        "single_supplier_factories = []\n",
        "for factory in [e for e in supply_chain_kg.get(\"entities\", []) if e.get(\"type\") == \"Factory\"]:\n",
        "    factory_id = factory.get(\"id\")\n",
        "    suppliers = [r for r in supply_chain_kg.get(\"relationships\", []) if r.get(\"target\") == factory_id and r.get(\"type\") == \"supplies\"]\n",
        "    if len(suppliers) == 1:\n",
        "        single_supplier_factories.append(factory.get(\"name\"))\n",
        "\n",
        "if single_supplier_factories:\n",
        "    disruptions.append({\n",
        "        \"type\": \"single_point_of_failure\",\n",
        "        \"severity\": \"medium\",\n",
        "        \"affected_entities\": single_supplier_factories,\n",
        "        \"description\": \"Factories with single supplier dependency detected\",\n",
        "        \"recommendation\": \"Add redundant supplier relationships\"\n",
        "    })\n",
        "\n",
        "if temporal_patterns:\n",
        "    disruptions.append({\n",
        "        \"type\": \"temporal_anomaly\",\n",
        "        \"severity\": \"medium\",\n",
        "        \"description\": f\"Detected {len(temporal_patterns)} temporal anomalies in supply chain\",\n",
        "        \"recommendation\": \"Review temporal patterns for potential disruptions\"\n",
        "    })\n",
        "\n",
        "print(f\"Predicted {len(disruptions)} potential disruptions\")\n",
        "for disruption in disruptions:\n",
        "    print(f\"  {disruption['type']} ({disruption['severity']}): {disruption['description']}\")\n",
        "    print(f\"    Recommendation: {disruption['recommendation']}\")\n",
        "\n",
        "entities_count = len(supply_chain_kg.get(\"entities\", []))\n",
        "print(f\"\\nAnalyzed {entities_count} supply chain nodes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Complete supply chain intelligence workflow:\n",
        "- Multi-source data ingested\n",
        "- Supply chain knowledge graph built\n",
        "- Dependencies analyzed\n",
        "- Flow optimization suggested\n",
        "- Disruptions predicted\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

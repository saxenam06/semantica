{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Source Data Ingestion\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to ingest data from multiple sources (files, web, feeds, streams, and databases) and process them through a unified pipeline.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Learn to ingest from various data sources\n",
        "- Combine data from multiple sources\n",
        "- Process diverse data through a unified pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## Unified Processing Pipeline\n",
        "\n",
        "Semantica provides specialized ingestors for different data sources, all producing a unified document format that can be processed together.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Ingest from Files\n",
        "\n",
        "Start by ingesting documents from local files or directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FileIngestor\n",
        "from pathlib import Path\n",
        "\n",
        "file_ingestor = FileIngestor()\n",
        "\n",
        "sample_file = Path(\"sample_file.txt\")\n",
        "sample_file.write_text(\"Sample file content for ingestion demonstration.\")\n",
        "\n",
        "try:\n",
        "    file_docs = file_ingestor.ingest_file(sample_file, read_content=True)\n",
        "    print(\"✓ Files ingested successfully!\")\n",
        "    print(f\"  Document: {file_docs.name if hasattr(file_docs, 'name') else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error ingesting files: {e}\")\n",
        "    file_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Ingest from Web\n",
        "\n",
        "Ingest content from web pages using the `WebIngestor`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import WebIngestor\n",
        "\n",
        "web_ingestor = WebIngestor()\n",
        "\n",
        "print(\"Web ingestion example:\")\n",
        "print(\"  web_docs = web_ingestor.ingest('https://example.com')\")\n",
        "print(\"\\nNote: Actual web ingestion requires valid URLs and network access\")\n",
        "web_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Ingest from Feeds\n",
        "\n",
        "Ingest content from RSS/Atom feeds using the `FeedIngestor`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor\n",
        "\n",
        "feed_ingestor = FeedIngestor()\n",
        "\n",
        "print(\"Feed ingestion example:\")\n",
        "print(\"  feed_docs = feed_ingestor.ingest('https://example.com/feed.xml')\")\n",
        "print(\"\\nNote: Actual feed ingestion requires valid feed URLs\")\n",
        "feed_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Ingest from Streams\n",
        "\n",
        "Ingest real-time data from streams using the `StreamIngestor`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import StreamIngestor\n",
        "\n",
        "stream_ingestor = StreamIngestor()\n",
        "\n",
        "print(\"Stream ingestion example:\")\n",
        "print(\"  stream_docs = stream_ingestor.ingest(stream_source)\")\n",
        "print(\"\\nNote: Stream ingestion requires configured stream sources (Kafka, RabbitMQ, etc.)\")\n",
        "stream_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Ingest from Databases\n",
        "\n",
        "Ingest data from databases using the `DBIngestor`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import DBIngestor\n",
        "\n",
        "print(\"Database ingestion example:\")\n",
        "print(\"  db_ingestor = DBIngestor(connection_string='...')\")\n",
        "print(\"  db_docs = db_ingestor.ingest(query='SELECT * FROM table')\")\n",
        "print(\"\\nNote: Database ingestion requires valid connection strings and queries\")\n",
        "db_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Unified Processing\n",
        "\n",
        "Combine all documents from different sources and process them through a unified pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "\n",
        "all_docs = []\n",
        "if file_docs:\n",
        "    all_docs.append(file_docs)\n",
        "all_docs.extend(web_docs)\n",
        "all_docs.extend(feed_docs)\n",
        "all_docs.extend(stream_docs)\n",
        "all_docs.extend(db_docs)\n",
        "\n",
        "print(f\"Total documents from all sources: {len(all_docs)}\")\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "if all_docs:\n",
        "    try:\n",
        "        parsed_docs = []\n",
        "        for doc in all_docs:\n",
        "            if hasattr(doc, 'content') and doc.content:\n",
        "                parsed = parser.parse_document(doc.content)\n",
        "                parsed_docs.append(parsed)\n",
        "        \n",
        "        print(f\"\\n✓ Processed {len(parsed_docs)} documents through unified pipeline\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error processing documents: {e}\")\n",
        "else:\n",
        "    print(\"\\nNote: Add documents from various sources to see unified processing\")\n",
        "\n",
        "try:\n",
        "    if sample_file.exists():\n",
        "        sample_file.unlink()\n",
        "except:\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

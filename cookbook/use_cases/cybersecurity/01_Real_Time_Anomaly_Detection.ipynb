{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/cybersecurity/01_Real_Time_Anomaly_Detection.ipynb)\n",
    "\n",
    "# Real-Time Anomaly Detection - Stream Processing & Temporal KGs\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **real-time anomaly detection** using Semantica with focus on **stream ingestion**, **temporal knowledge graphs**, and **pattern detection**. The pipeline streams security logs in real-time, builds temporal knowledge graphs, and detects anomalies using pattern detection.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Stream Processing**: Emphasizes real-time log streaming and processing\n",
    "- **Temporal Knowledge Graphs**: Builds temporal KGs to track events over time\n",
    "- **Pattern Detection**: Uses graph patterns to identify anomalies\n",
    "- **Automated Alerting**: Generates alerts for detected anomalies\n",
    "- **Comprehensive Data Sources**: Multiple security RSS feeds, APIs, and databases\n",
    "- **Modular Architecture**: Direct use of Semantica modules without core orchestrator\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Ingest security data from multiple sources (RSS feeds, APIs, streams)\n",
    "- Extract security entities (Logs, Events, IPs, Users, Alerts, Attacks)\n",
    "- Build temporal security knowledge graphs\n",
    "- Perform temporal queries and pattern detection\n",
    "- Detect anomalies using graph reasoning\n",
    "- Store and query security data using vector stores and graph stores\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Document Parsing]\n",
    "    B --> C[Text Processing]\n",
    "    C --> D[Entity Extraction]\n",
    "    D --> E[Relationship Extraction]\n",
    "    E --> F[Deduplication]\n",
    "    F --> G[Conflict Detection]\n",
    "    G --> H[Temporal Knowledge Graph]\n",
    "    H --> I[Embeddings]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Temporal Queries]\n",
    "    K --> L[Pattern Detection]\n",
    "    L --> M[Reasoning & Anomaly]\n",
    "    J --> N[GraphRAG Queries]\n",
    "    M --> N\n",
    "    H --> O[Graph Store]\n",
    "    N --> P[Visualization]\n",
    "    O --> P\n",
    "    P --> Q[Export]\n",
    "```\n",
    "\n",
    "\n",
    "## Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "TEMPORAL_GRANULARITY = \"minute\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Security Data from Multiple Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting from 5 feed sources...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚úÖ</td><td>Semantica is extracting</td><td>üéØ semantic_extract</td><td>NERExtractor</td><td>-</td><td>0.55s</td></tr><tr><td>‚úÖ</td><td>Semantica is extracting</td><td>üéØ semantic_extract</td><td>RelationExtractor</td><td>-</td><td>0.05s</td></tr><tr><td>‚úÖ</td><td>Semantica is resolving</td><td>‚ö†Ô∏è conflicts</td><td>ConflictDetector</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is building</td><td>üß† kg</td><td>GraphBuilder</td><td>-</td><td>350.71s</td></tr><tr><td>üîÑ</td><td>Semantica is building</td><td>üß† kg</td><td>EntityResolver</td><td>-</td><td>84.09s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>DuplicateDetector</td><td>-</td><td>0.08s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>SimilarityCalculator</td><td>-</td><td>0.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>EntityMerger</td><td>-</td><td>0.13s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>MergeStrategyManager</td><td>-</td><td>0.03s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>VectorStore</td><td>-</td><td>0.01s</td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1/5] US-CERT Alerts: 10 documents\n",
      "  [2/5] SANS ISC: 10 documents\n",
      "  [3/5] Krebs on Security: 10 documents\n",
      "  [4/5] ThreatPost: 10 documents\n",
      "Ingested 40 documents\n"
     ]
    }
   ],
   "source": [
    "from semantica.ingest import FeedIngestor, StreamIngestor, FileIngestor\n",
    "import os\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "feed_sources = [\n",
    "    # Security RSS Feeds\n",
    "    (\"US-CERT Alerts\", \"https://www.us-cert.gov/ncas/alerts.xml\"),\n",
    "    (\"SANS ISC\", \"https://isc.sans.edu/rssfeed.xml\"),\n",
    "    (\"Krebs on Security\", \"https://krebsonsecurity.com/feed/\"),\n",
    "    (\"ThreatPost\", \"https://threatpost.com/feed/\"),\n",
    "    (\"BleepingComputer\", \"https://www.bleepingcomputer.com/feed/\"),\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "all_documents = []\n",
    "\n",
    "print(f\"Ingesting from {len(feed_sources)} feed sources...\")\n",
    "for i, (feed_name, feed_url) in enumerate(feed_sources, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
    "        \n",
    "        feed_count = 0\n",
    "        for item in feed_data.items:\n",
    "            if not item.content:\n",
    "                item.content = item.description or item.title or \"\"\n",
    "            if item.content:\n",
    "                if not hasattr(item, 'metadata'):\n",
    "                    item.metadata = {}\n",
    "                item.metadata['source'] = feed_name\n",
    "                all_documents.append(item)\n",
    "                feed_count += 1\n",
    "        \n",
    "        if feed_count > 0:\n",
    "            print(f\"  [{i}/{len(feed_sources)}] {feed_name}: {feed_count} documents\")\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Simulate stream ingestion (in production, use actual Kafka/WebSocket)\n",
    "if not all_documents:\n",
    "    security_logs = \"\"\"\n",
    "    2024-01-01 10:00:00 - Login attempt from IP 192.168.1.100 user admin\n",
    "    2024-01-01 10:01:00 - Failed login from IP 192.168.1.100 user admin\n",
    "    2024-01-01 10:02:00 - Multiple failed logins from IP 192.168.1.100 user admin\n",
    "    2024-01-01 10:03:00 - Unusual activity detected from IP 192.168.1.100\n",
    "    2024-01-01 10:04:00 - Alert: Potential brute force attack from IP 192.168.1.100\n",
    "    2024-01-01 10:05:00 - Login attempt from IP 192.168.1.101 user test\n",
    "    2024-01-01 10:06:00 - Suspicious file access from IP 192.168.1.102\n",
    "    2024-01-01 10:07:00 - Multiple connection attempts from IP 192.168.1.103\n",
    "    \"\"\"\n",
    "    with open(\"data/security_logs.txt\", \"w\") as f:\n",
    "        f.write(security_logs)\n",
    "    file_ingestor = FileIngestor()\n",
    "    all_documents = file_ingestor.ingest(\"data/security_logs.txt\")\n",
    "\n",
    "documents = all_documents\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 40 documents...\n",
      "  Parsed 40/40 documents...\n"
     ]
    }
   ],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        parsed = parser.parse(\n",
    "            doc.content if hasattr(doc, 'content') else str(doc),\n",
    "            content_type=\"text\"\n",
    "        )\n",
    "        parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc)\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "documents = parsed_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Chunking Security Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing 40 documents...\n",
      "  Normalized 40/40 documents...\n",
      "Chunking 40 documents...\n",
      "  Chunked 40/40 documents (307 chunks so far)\n",
      "Created 307 chunks from 40 documents\n"
     ]
    }
   ],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "# Use sentence chunking for log line boundaries (structured logs)\n",
    "splitter = TextSplitter(method=\"sentence\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "print(f\"Normalizing {len(documents)} documents...\")\n",
    "normalized_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    normalized_text = normalizer.normalize(\n",
    "        doc.content if hasattr(doc, 'content') else str(doc),\n",
    "        clean_html=True,\n",
    "        normalize_entities=True,\n",
    "        remove_extra_whitespace=True,\n",
    "        lowercase=False\n",
    "    )\n",
    "    normalized_documents.append(normalized_text)\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Normalized {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Chunking {len(normalized_documents)} documents...\")\n",
    "chunked_documents = []\n",
    "for i, doc_text in enumerate(normalized_documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    except Exception:\n",
    "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "        chunks = simple_splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    if i % 50 == 0 or i == len(normalized_documents):\n",
    "        print(f\"  Chunked {i}/{len(normalized_documents)} documents ({len(chunked_documents)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_documents)} chunks from {len(normalized_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Security Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities from 307 chunks...\n",
      "  Processed 20/307 chunks (6 entities found)\n",
      "  Processed 40/307 chunks (10 entities found)\n",
      "  Processed 60/307 chunks (15 entities found)\n",
      "  Processed 80/307 chunks (26 entities found)\n",
      "  Processed 100/307 chunks (61 entities found)\n",
      "  Processed 120/307 chunks (86 entities found)\n",
      "  Processed 140/307 chunks (98 entities found)\n",
      "  Processed 160/307 chunks (116 entities found)\n",
      "  Processed 180/307 chunks (144 entities found)\n",
      "  Processed 200/307 chunks (160 entities found)\n",
      "  Processed 220/307 chunks (198 entities found)\n",
      "  Processed 240/307 chunks (232 entities found)\n",
      "  Processed 260/307 chunks (247 entities found)\n",
      "  Processed 280/307 chunks (257 entities found)\n",
      "  Processed 300/307 chunks (269 entities found)\n",
      "  Processed 307/307 chunks (273 entities found)\n",
      "Extracted 3 IPs, 28 users, 0 alerts\n"
     ]
    }
   ],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "security_patterns = {\n",
    "    \"IP\": r\"\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\b\",\n",
    "    \"User\": r\"\\buser\\s+([a-zA-Z0-9_\\-\\.]+)\\b\",\n",
    "    \"Alert\": r\"\\b(?:alert|warning|alarm):\\s*([^\\n\\.]+)|\\b(?:alert|warning|alarm)\\s+(?:detected|triggered|generated|raised)\\b\",\n",
    "    \"Event\": r\"\\b(?:login|access|connection|request|attempt|failed|successful|suspicious|unusual)\\s+(?:event|attempt|request|activity|access)\\b\",\n",
    "    \"Log\": r\"\\b\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\s+-\\s+([^\\n]+)\",\n",
    "    \"Attack\": r\"\\b(?:attack|breach|intrusion|exploit|malware|virus|ransomware|phishing|brute\\s+force|ddos)\\b\",\n",
    "}\n",
    "\n",
    "entity_extractor = NERExtractor(method=\"regex\", patterns=security_patterns)\n",
    "\n",
    "all_entities = []\n",
    "print(f\"Extracting entities from {len(chunked_documents)} chunks...\")\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        entities = entity_extractor.extract_entities(\n",
    "            chunk_text,\n",
    "            entity_types=[\"Log\", \"Event\", \"IP\", \"User\", \"Alert\", \"Attack\"]\n",
    "        )\n",
    "        all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    if i % 20 == 0 or i == len(chunked_documents):\n",
    "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "ips = [e for e in all_entities if e.label == \"IP\" or \"ip\" in e.label.lower()]\n",
    "users = [e for e in all_entities if e.label == \"User\" or \"user\" in e.label.lower()]\n",
    "alerts = [e for e in all_entities if e.label == \"Alert\" or \"alert\" in e.label.lower()]\n",
    "\n",
    "print(f\"Extracted {len(ips)} IPs, {len(users)} users, {len(alerts)} alerts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Security Relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relationships from 307 chunks using 267 filtered entities...\n",
      "  Processed 20/307 chunks (534 relationships found)\n",
      "  Processed 40/307 chunks (534 relationships found)\n",
      "  Processed 60/307 chunks (534 relationships found)\n",
      "  Processed 80/307 chunks (534 relationships found)\n",
      "  Processed 100/307 chunks (534 relationships found)\n",
      "  Processed 120/307 chunks (534 relationships found)\n",
      "  Processed 140/307 chunks (534 relationships found)\n",
      "  Processed 160/307 chunks (534 relationships found)\n",
      "  Processed 180/307 chunks (534 relationships found)\n",
      "  Processed 200/307 chunks (534 relationships found)\n",
      "  Processed 220/307 chunks (534 relationships found)\n",
      "  Processed 240/307 chunks (534 relationships found)\n",
      "  Processed 260/307 chunks (534 relationships found)\n",
      "  Processed 280/307 chunks (534 relationships found)\n",
      "  Processed 300/307 chunks (534 relationships found)\n",
      "  Processed 307/307 chunks (534 relationships found)\n",
      "Extracted 534 relationships\n"
     ]
    }
   ],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "# Filter entities to only meaningful security entities\n",
    "filtered_entities = [\n",
    "    e for e in all_entities \n",
    "    if e.label in [\"IP\", \"User\", \"Alert\", \"Attack\", \"Event\", \"Log\"] \n",
    "    and len(e.text) > 2\n",
    "    and e.text.lower() not in [\"to\", \"from\", \"should\", \"would\", \"choices\", \"connects\"]\n",
    "]\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"cooccurrence\",\n",
    "    max_distance=60,\n",
    "    confidence_threshold=0.6\n",
    ")\n",
    "\n",
    "# Deduplicate relationships\n",
    "seen_relationships = set()\n",
    "all_relationships = []\n",
    "print(f\"Extracting relationships from {len(chunked_documents)} chunks using {len(filtered_entities)} filtered entities...\")\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        relationships = relation_extractor.extract_relations(\n",
    "            chunk_text,\n",
    "            entities=filtered_entities,\n",
    "            relation_types=[\"from\", \"attempts\", \"triggers\", \"detects\", \"associated_with\", \"causes\"]\n",
    "        )\n",
    "        # Deduplicate based on subject, predicate, object\n",
    "        for rel in relationships:\n",
    "            rel_key = (rel.subject.text, rel.predicate, rel.object.text)\n",
    "            if rel_key not in seen_relationships:\n",
    "                seen_relationships.add(rel_key)\n",
    "                all_relationships.append(rel)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    if i % 20 == 0 or i == len(chunked_documents):\n",
    "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Security Conflicts\n",
    "\n",
    "- **Using entity-wide conflict detection** to identify all types of conflicts (value, type, relationship, temporal) across security entities from multiple sources. \n",
    "- **Voting resolution strategy** selects the majority consensus value, ensuring reliability through multi-source agreement for security event data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting conflicts in 273 entities and 534 relationships...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 0 conflicts (0 entity, 0 relationship)\n",
      "No conflicts detected\n"
     ]
    }
   ],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "# Convert entities to dictionaries for conflict detection\n",
    "entity_dicts = [\n",
    "    {\n",
    "        \"id\": e.text,\n",
    "        \"text\": e.text,\n",
    "        \"label\": e.label,\n",
    "        \"type\": e.label,\n",
    "        \"confidence\": e.confidence if hasattr(e, 'confidence') else 1.0,\n",
    "        \"metadata\": e.metadata if hasattr(e, 'metadata') else {}\n",
    "    }\n",
    "    for e in all_entities\n",
    "]\n",
    "\n",
    "# Convert relationships to dictionaries for conflict detection\n",
    "relationship_dicts = [\n",
    "    {\n",
    "        \"id\": f\"{r.subject.text}_{r.predicate}_{r.object.text}\",\n",
    "        \"source_id\": r.subject.text,\n",
    "        \"target_id\": r.object.text,\n",
    "        \"type\": r.predicate,\n",
    "        \"subject\": r.subject.text,\n",
    "        \"object\": r.object.text,\n",
    "        \"predicate\": r.predicate,\n",
    "        \"confidence\": r.confidence if hasattr(r, 'confidence') else 1.0\n",
    "    }\n",
    "    for r in all_relationships\n",
    "]\n",
    "\n",
    "print(f\"Detecting conflicts in {len(entity_dicts)} entities and {len(relationship_dicts)} relationships...\")\n",
    "\n",
    "# Detect entity conflicts (value, type, temporal)\n",
    "value_conflicts = conflict_detector.detect_value_conflicts(entity_dicts, property_name=\"label\")\n",
    "type_conflicts = conflict_detector.detect_type_conflicts(entity_dicts)\n",
    "temporal_conflicts = conflict_detector.detect_temporal_conflicts(entity_dicts)\n",
    "entity_conflicts = value_conflicts + type_conflicts + temporal_conflicts\n",
    "\n",
    "# Detect relationship conflicts\n",
    "relationship_conflicts = conflict_detector.detect_relationship_conflicts(relationship_dicts)\n",
    "\n",
    "# Combine all conflicts\n",
    "all_conflicts = entity_conflicts + relationship_conflicts\n",
    "\n",
    "print(f\"Detected {len(all_conflicts)} conflicts ({len(entity_conflicts)} entity, {len(relationship_conflicts)} relationship)\")\n",
    "\n",
    "if all_conflicts:\n",
    "    print(f\"Resolving conflicts using voting strategy...\")\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        all_conflicts,\n",
    "        strategy=\"voting\"\n",
    "    )\n",
    "    print(f\"Resolved {len(resolved)} conflicts\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Temporal Security Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building knowledge graph...\n",
      "Processing 273 entities, 534 relationships (807 total)...\n",
      "  Entities: 100/273 (36.6%) | ETA: 3.5m | Rate: 0.8/s\n",
      "  Entities: 200/273 (73.3%) | ETA: 1.5m | Rate: 0.8/s\n",
      "  Entities: 273/273 (100.0%) | ETA: 0.0s | Rate: 0.8/s\n",
      "  Relationships: 100/534\n",
      "  Relationships: 200/534\n",
      "  Relationships: 300/534\n",
      "  Relationships: 400/534\n",
      "  Relationships: 500/534\n",
      "  Relationships: 534/534\n",
      "Resolving 30 entities...\n",
      "‚úÖ Resolved to 3 unique entities (8.32s)\n",
      "Building graph structure...\n",
      "‚úÖ Graph structure built (0.00s)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Knowledge Graph Build Complete\n",
      "   Entities: 3\n",
      "   Relationships: 534\n",
      "   Total time: 350.71s\n",
      "============================================================\n",
      "Graph: 3 entities, 534 relationships\n"
     ]
    }
   ],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    resolve_conflicts=True,\n",
    "    entity_resolution_strategy=\"fuzzy\",\n",
    "    enable_temporal=True,\n",
    "    temporal_granularity=TEMPORAL_GRANULARITY\n",
    ")\n",
    "\n",
    "print(f\"Building knowledge graph...\")\n",
    "kg_sources = [{\n",
    "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": e.confidence if hasattr(e, 'confidence') else 1.0} for e in all_entities],\n",
    "    \"relationships\": [{\"source\": r.subject.text, \"target\": r.object.text, \"type\": r.predicate, \"confidence\": r.confidence if hasattr(r, 'confidence') else 1.0} for r in all_relationships]\n",
    "}]\n",
    "\n",
    "kg = graph_builder.build(kg_sources)\n",
    "\n",
    "entities_count = len(kg.get('entities', []))\n",
    "relationships_count = len(kg.get('relationships', []))\n",
    "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings for Events and IPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 6 events and 3 IPs...\n",
      "Generated 6 event embeddings and 3 IP embeddings\n"
     ]
    }
   ],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    provider=\"sentence_transformers\",\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "event_entities = [e for e in all_entities if e.label in [\"Event\", \"Log\", \"Alert\"]]\n",
    "print(f\"Generating embeddings for {len(event_entities)} events and {len(ips)} IPs...\")\n",
    "event_texts = [e.text for e in event_entities]\n",
    "event_embeddings = embedding_gen.generate_embeddings(event_texts)\n",
    "\n",
    "ip_texts = [ip.text for ip in ips]\n",
    "ip_embeddings = embedding_gen.generate_embeddings(ip_texts)\n",
    "\n",
    "print(f\"Generated {len(event_embeddings)} event embeddings and {len(ip_embeddings)} IP embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 6 event vectors and 3 IP vectors...\n",
      "Stored 6 event vectors and 3 IP vectors\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "print(f\"Storing {len(event_embeddings)} event vectors and {len(ip_embeddings)} IP vectors...\")\n",
    "event_ids = vector_store.store_vectors(\n",
    "    vectors=event_embeddings,\n",
    "    metadata=[{\"type\": \"event\", \"name\": e.text, \"label\": e.label} for e in event_entities]\n",
    ")\n",
    "\n",
    "ip_ids = vector_store.store_vectors(\n",
    "    vectors=ip_embeddings,\n",
    "    metadata=[{\"type\": \"ip\", \"name\": ip.text, \"label\": ip.label} for ip in ips]\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(event_ids)} event vectors and {len(ip_ids)} IP vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TemporalGraphQuery' object has no attribute 'detect_temporal_patterns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m      8\u001b[0m query_results \u001b[38;5;241m=\u001b[39m temporal_query\u001b[38;5;241m.\u001b[39mquery_at_time(\n\u001b[0;32m      9\u001b[0m     kg,\n\u001b[0;32m     10\u001b[0m     query\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlert\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     11\u001b[0m     at_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-01-01 10:04:00\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m evolution \u001b[38;5;241m=\u001b[39m temporal_query\u001b[38;5;241m.\u001b[39manalyze_evolution(kg)\n\u001b[1;32m---> 15\u001b[0m temporal_patterns \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_temporal_patterns\u001b[49m(kg, pattern_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemporal queries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(query_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m alerts at query time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemporal patterns detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(temporal_patterns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TemporalGraphQuery' object has no attribute 'detect_temporal_patterns'"
     ]
    }
   ],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "\n",
    "temporal_query = TemporalGraphQuery(\n",
    "    enable_temporal_reasoning=True,\n",
    "    temporal_granularity=TEMPORAL_GRANULARITY\n",
    ")\n",
    "\n",
    "query_results = temporal_query.query_at_time(\n",
    "    kg,\n",
    "    query={\"type\": \"Alert\"},\n",
    "    at_time=\"2024-01-01 10:04:00\"\n",
    ")\n",
    "\n",
    "evolution = temporal_query.analyze_evolution(kg)\n",
    "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
    "\n",
    "print(f\"Temporal queries: {len(query_results)} alerts at query time\")\n",
    "print(f\"Temporal patterns detected: {len(temporal_patterns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Anomaly Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "\n",
    "# Detect suspicious IPs\n",
    "suspicious_ips = []\n",
    "for entity in kg.get(\"entities\", []):\n",
    "    if entity.get(\"type\") == \"IP\":\n",
    "        related_rels = [r for r in kg.get(\"relationships\", []) \n",
    "                        if r.get(\"source\") == entity.get(\"id\") or r.get(\"target\") == entity.get(\"id\")]\n",
    "        if any(\"alert\" in str(r.get(\"type\", \"\")).lower() or \"attack\" in str(r.get(\"type\", \"\")).lower() \n",
    "               for r in related_rels):\n",
    "            suspicious_ips.append(entity)\n",
    "\n",
    "# Detect anomaly patterns (multiple failed logins, unusual activity)\n",
    "anomaly_patterns = []\n",
    "for ip in ips[:10]:\n",
    "    ip_name = ip.text\n",
    "    paths = graph_analyzer.find_paths(\n",
    "        kg,\n",
    "        source=ip_name,\n",
    "        target_type=\"Alert\",\n",
    "        max_hops=2\n",
    "    )\n",
    "    if len(paths) > 0:\n",
    "        anomaly_patterns.append({\n",
    "            'ip': ip_name,\n",
    "            'alert_count': len(paths),\n",
    "            'pattern': 'suspicious_activity'\n",
    "        })\n",
    "\n",
    "print(f\"Pattern detection: {len(anomaly_patterns)} anomaly patterns found\")\n",
    "print(f\"Suspicious IPs: {len(suspicious_ips)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning and Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner\n",
    "\n",
    "reasoner = Reasoner()\n",
    "\n",
    "reasoner.add_rule(\"IF IP attempts Event AND Event type failed_login AND Event count > 3 THEN IP triggers Alert\")\n",
    "reasoner.add_rule(\"IF User from IP AND IP triggers Alert THEN User associated_with Alert\")\n",
    "\n",
    "inferred_facts = reasoner.infer_facts(kg)\n",
    "\n",
    "anomaly_paths = reasoner.find_paths(\n",
    "    kg,\n",
    "    source_type=\"IP\",\n",
    "    target_type=\"Alert\",\n",
    "    max_hops=2\n",
    ")\n",
    "\n",
    "print(f\"Inferred {len(inferred_facts)} facts\")\n",
    "print(f\"Found {len(anomaly_paths)} anomaly paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Security Knowledge Graph (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.graph_store import GraphStore\n",
    "\n",
    "# Optional: Store to persistent graph database\n",
    "# graph_store = GraphStore(backend=\"neo4j\", uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# graph_store.store_graph(kg)\n",
    "\n",
    "print(\"Graph store configured (commented out for demo)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG: Hybrid Vector + Graph Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "from semantica.llms import Groq\n",
    "import os\n",
    "\n",
    "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
    "\n",
    "# Initialize LLM provider\n",
    "llm_provider = Groq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "query = \"What IPs are associated with security alerts?\"\n",
    "result = context.query_with_reasoning(\n",
    "    query=query,\n",
    "    llm_provider=llm_provider,\n",
    "    max_results=10,\n",
    "    max_hops=2\n",
    ")\n",
    "\n",
    "print(f\"GraphRAG Query with Reasoning: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerated Response:\\n{result['response']}\\n\")\n",
    "print(\"=\" * 80)\n",
    "if result.get('reasoning_path'):\n",
    "    print(f\"\\nReasoning Path:\\n{result['reasoning_path']}\\n\")\n",
    "print(f\"Confidence: {result.get('confidence', 0):.3f}\")\n",
    "print(f\"Sources Used: {result.get('num_sources', 0)}\")\n",
    "print(f\"Reasoning Paths Found: {result.get('num_reasoning_paths', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Temporal Security Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "visualizer.visualize(\n",
    "    kg,\n",
    "    output_path=\"anomaly_detection_kg.html\",\n",
    "    layout=\"temporal\",\n",
    "    node_size=20\n",
    ")\n",
    "\n",
    "print(\"Visualization saved to anomaly_detection_kg.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "\n",
    "exporter = GraphExporter()\n",
    "exporter.export(kg, output_path=\"anomaly_detection_kg.json\", format=\"json\")\n",
    "exporter.export(kg, output_path=\"anomaly_detection_kg.graphml\", format=\"graphml\")\n",
    "exporter.export(kg, output_path=\"anomaly_detection_alerts.csv\", format=\"csv\")\n",
    "\n",
    "print(\"Exported knowledge graph to JSON, GraphML, and CSV formats\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

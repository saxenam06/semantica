{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/supply_chain/02_Supply_Chain_Risk_Management.ipynb)\n",
        "\n",
        "# Supply Chain Risk Management - Dependency Analysis & Risk Detection\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **supply chain risk management** using Semantica with focus on **dependency analysis**, **risk pattern detection**, and **conflict resolution**. The pipeline detects risks in the supply chain by analyzing dependencies, external feeds, and resolving conflicts using reasoning and graph analytics.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Dependency Analysis**: Analyzes supply chain dependencies using graph reasoning\n",
        "- **Risk Pattern Detection**: Detects risk patterns in the supply chain using reasoning\n",
        "- **Conflict Detection**: Detects and resolves conflicts in risk data from multiple sources\n",
        "- **Risk Impact Analysis**: Analyzes risk impact using graph analytics\n",
        "- **Temporal Risk Tracking**: Tracks risk evolution over time\n",
        "- **External Feed Correlation**: Correlates external threat feeds with supply chain data\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Understand how to detect and resolve conflicts in multi-source risk data\n",
        "- Learn to analyze supply chain dependencies using reasoning\n",
        "- Master risk pattern detection using graph reasoning\n",
        "- Explore risk impact analysis using graph analytics\n",
        "- Practice temporal risk tracking and evolution analysis\n",
        "- Analyze supply chain risks and mitigation strategies\n",
        "\n",
        "### Pipeline Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Multi-Source Risk Ingestion] --> B[Document Parsing]\n",
        "    B --> C[Text Processing]\n",
        "    C --> D[Entity Extraction]\n",
        "    D --> E[Relationship Extraction]\n",
        "    E --> F[Deduplication]\n",
        "    F --> G[Conflict Detection]\n",
        "    G --> H[KG Construction]\n",
        "    H --> I[Embedding Generation]\n",
        "    I --> J[Vector Store]\n",
        "    H --> K[Dependency Analysis]\n",
        "    H --> L[Risk Pattern Detection]\n",
        "    H --> M[Risk Impact Analysis]\n",
        "    H --> N[Temporal Risk Queries]\n",
        "    J --> O[GraphRAG Queries]\n",
        "    K --> P[Visualization]\n",
        "    L --> P\n",
        "    M --> P\n",
        "    H --> Q[Export]\n",
        "```\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Configuration & Setup\n",
        "\n",
        "Configure API keys and set up constants for the supply chain risk management pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
        "\n",
        "# Configuration constants\n",
        "EMBEDDING_DIMENSION = 384\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Multi-Source Risk Data Ingestion\n",
        "\n",
        "Ingest supply chain risk data from multiple sources including risk RSS feeds, external threat feeds, and disruption APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, WebIngestor, FileIngestor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Ingest from supply chain risk RSS feeds\n",
        "risk_feeds = [\n",
        "    \"https://www.scrm.com/rss\",\n",
        "    \"https://www.riskmanagement.com/rss\"\n",
        "]\n",
        "\n",
        "for feed_url in risk_feeds:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            feed_ingestor = FeedIngestor()\n",
        "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
        "            documents.extend(feed_docs)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Example: Web ingestion from weather/disruption APIs (commented - requires API keys)\n",
        "# web_ingestor = WebIngestor()\n",
        "# weather_docs = web_ingestor.ingest(\"https://api.weather.com/disruptions\", method=\"api\")\n",
        "\n",
        "# Fallback: Sample risk data\n",
        "if not documents:\n",
        "    risk_data = \"\"\"\n",
        "    Supplier A depends on raw materials from Region R1 (high risk region).\n",
        "    Disruption in Region R1 impacts Supplier A, causing supply chain risk.\n",
        "    External feed: Weather alert in Region R1 may disrupt logistics.\n",
        "    Risk mitigation: Identify alternative suppliers in Region R2.\n",
        "    Supplier B depends on Region R1, creating dependency risk.\n",
        "    Impact: High risk of supply chain disruption if Region R1 fails.\n",
        "    \"\"\"\n",
        "    with open(\"data/supply_chain_risks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(risk_data)\n",
        "    file_ingestor = FileIngestor()\n",
        "    documents = file_ingestor.ingest(\"data/supply_chain_risks.txt\")\n",
        "\n",
        "print(f\"Ingested {len(documents)} documents from risk sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "parsed_documents = []\n",
        "for doc in documents:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            parsed = parser.parse(\n",
        "                doc.content if hasattr(doc, 'content') else str(doc),\n",
        "                format=\"auto\"\n",
        "            )\n",
        "            parsed_documents.append(parsed)\n",
        "    except Exception:\n",
        "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
        "\n",
        "print(f\"Parsed {len(parsed_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Text Processing\n",
        "\n",
        "Normalize risk data and split documents using relation-aware chunking to preserve dependency relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "normalized_docs = []\n",
        "\n",
        "for doc in parsed_documents:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            normalized = normalizer.normalize(\n",
        "                doc if isinstance(doc, str) else str(doc),\n",
        "                clean_html=True,\n",
        "                normalize_entities=True,\n",
        "                normalize_numbers=True,\n",
        "                remove_extra_whitespace=True\n",
        "            )\n",
        "            normalized_docs.append(normalized)\n",
        "    except Exception:\n",
        "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
        "\n",
        "# Use relation-aware chunking to preserve dependency relationships\n",
        "relation_splitter = TextSplitter(\n",
        "    method=\"relation_aware\",\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_docs:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = relation_splitter.split(doc_text)\n",
        "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "    except Exception:\n",
        "        chunked_docs.append(doc_text)\n",
        "\n",
        "print(f\"Processed {len(chunked_docs)} relation-aware chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Entity Extraction\n",
        "\n",
        "Extract supply chain risk entities including dependencies, risks, disruptions, impacts, mitigations, and regions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "extractor = NERExtractor(\n",
        "    provider=\"groq\",\n",
        "    model=\"llama-3.1-8b-instant\"\n",
        ")\n",
        "\n",
        "entity_types = [\n",
        "    \"Dependency\", \"Risk\", \"Disruption\", \"Impact\", \"Mitigation\", \"Region\"\n",
        "]\n",
        "\n",
        "all_entities = []\n",
        "for chunk in chunked_docs[:10]:  # Limit for demo\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            entities = extractor.extract(\n",
        "                chunk,\n",
        "                entity_types=entity_types\n",
        "            )\n",
        "            all_entities.extend(entities)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"Extracted {len(all_entities)} entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Relationship Extraction\n",
        "\n",
        "Extract risk relationships including depends_on, causes, impacts, mitigates, and located_in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import RelationExtractor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "relation_extractor = RelationExtractor(\n",
        "    provider=\"groq\",\n",
        "    model=\"llama-3.1-8b-instant\"\n",
        ")\n",
        "\n",
        "relation_types = [\n",
        "    \"depends_on\", \"causes\", \"impacts\",\n",
        "    \"mitigates\", \"located_in\"\n",
        "]\n",
        "\n",
        "all_relationships = []\n",
        "for chunk in chunked_docs[:10]:  # Limit for demo\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            relationships = relation_extractor.extract(\n",
        "                chunk,\n",
        "                relation_types=relation_types\n",
        "            )\n",
        "            all_relationships.extend(relationships)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"Extracted {len(all_relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Deduplication\n",
        "\n",
        "Deduplicate risk entities to ensure accurate risk analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.deduplication import DuplicateDetector\n",
        "\n",
        "detector = DuplicateDetector()\n",
        "\n",
        "# Deduplicate entities\n",
        "risks = [e for e in all_entities if e.get(\"type\") == \"Risk\"]\n",
        "regions = [e for e in all_entities if e.get(\"type\") == \"Region\"]\n",
        "\n",
        "risk_duplicates = detector.detect_duplicates(risks, threshold=0.9)\n",
        "region_duplicates = detector.detect_duplicates(regions, threshold=0.85)\n",
        "\n",
        "deduplicated_risks = detector.resolve_duplicates(risks, risk_duplicates)\n",
        "deduplicated_regions = detector.resolve_duplicates(regions, region_duplicates)\n",
        "\n",
        "# Update entities list\n",
        "all_entities = [e for e in all_entities if e.get(\"type\") not in [\"Risk\", \"Region\"]]\n",
        "all_entities.extend(deduplicated_risks)\n",
        "all_entities.extend(deduplicated_regions)\n",
        "\n",
        "print(f\"Deduplicated: {len(risks)} -> {len(deduplicated_risks)} risks\")\n",
        "print(f\"Deduplicated: {len(regions)} -> {len(deduplicated_regions)} regions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conflict Detection\n",
        "\n",
        "Detect conflicts in risk data from multiple sources. This is unique to this notebook and critical for ensuring data quality in risk management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector\n",
        "\n",
        "conflict_detector = ConflictDetector()\n",
        "\n",
        "# Detect conflicts in risk data\n",
        "conflicts = conflict_detector.detect_conflicts(\n",
        "    entities=all_entities,\n",
        "    relationships=all_relationships\n",
        ")\n",
        "\n",
        "print(f\"Detected {len(conflicts)} conflicts in risk data\")\n",
        "\n",
        "# Resolve conflicts using highest confidence strategy\n",
        "if conflicts:\n",
        "    resolved = conflict_detector.resolve_conflicts(\n",
        "        conflicts,\n",
        "        strategy=\"highest_confidence\"\n",
        "    )\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Knowledge Graph Construction\n",
        "\n",
        "Build a knowledge graph from risk entities and relationships to enable dependency and risk analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder\n",
        "\n",
        "builder = GraphBuilder()\n",
        "\n",
        "kg = builder.build(\n",
        "    entities=all_entities,\n",
        "    relationships=all_relationships\n",
        ")\n",
        "\n",
        "print(f\"Built KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embedding Generation & Vector Store\n",
        "\n",
        "Generate embeddings for risk documents and store them in a vector database for semantic search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "from semantica.vector_store import VectorStore\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "embedding_gen = EmbeddingGenerator(\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    dimension=EMBEDDING_DIMENSION\n",
        ")\n",
        "\n",
        "# Generate embeddings for chunks\n",
        "embeddings = []\n",
        "for chunk in chunked_docs[:20]:  # Limit for demo\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            embedding = embedding_gen.generate(chunk)\n",
        "            embeddings.append(embedding)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Create vector store\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
        "\n",
        "# Add embeddings to vector store\n",
        "for i, (chunk, embedding) in enumerate(zip(chunked_docs[:20], embeddings)):\n",
        "    try:\n",
        "        vector_store.add(\n",
        "            id=str(i),\n",
        "            embedding=embedding,\n",
        "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Dependency Analysis\n",
        "\n",
        "Analyze supply chain dependencies using reasoning to identify dependency patterns. This is unique to this notebook and critical for risk management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.reasoning import Reasoner\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "reasoner = Reasoner(kg)\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Add rules for dependency analysis\n",
        "        rules = [\n",
        "            \"IF Supplier depends_on Region AND Region has Risk THEN Dependency creates_risk\",\n",
        "            \"IF Dependency depends_on Region AND Region has Disruption THEN Dependency causes_impact\",\n",
        "            \"IF Supplier depends_on Dependency AND Dependency has Risk THEN Supplier has_risk\"\n",
        "        ]\n",
        "        \n",
        "        for rule in rules:\n",
        "            reasoner.add_rule(rule)\n",
        "        \n",
        "        # Find dependency patterns\n",
        "        dependency_patterns = reasoner.find_patterns(pattern_type=\"dependency\")\n",
        "        print(f\"Detected {len(dependency_patterns)} dependency patterns\")\n",
        "        \n",
        "        # Infer dependency risks\n",
        "        inferred_dependencies = reasoner.infer_facts()\n",
        "        print(f\"Inferred {len(inferred_dependencies)} dependency relationships\")\n",
        "except Exception:\n",
        "    print(\"Dependency analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Risk Pattern Detection\n",
        "\n",
        "Detect risk patterns in the supply chain using reasoning. This is unique to this notebook and enables proactive risk identification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.reasoning import Reasoner\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Add rules for risk pattern detection\n",
        "        risk_rules = [\n",
        "            \"IF Region has Disruption AND Supplier depends_on Region THEN Risk impacts Supplier\",\n",
        "            \"IF Disruption causes Impact AND Impact affects Supplier THEN Risk requires Mitigation\",\n",
        "            \"IF Risk located_in Region AND Region has Disruption THEN Risk severity increases\"\n",
        "        ]\n",
        "        \n",
        "        for rule in risk_rules:\n",
        "            reasoner.add_rule(rule)\n",
        "        \n",
        "        # Find risk patterns\n",
        "        risk_patterns = reasoner.find_patterns(pattern_type=\"risk\")\n",
        "        print(f\"Detected {len(risk_patterns)} risk patterns\")\n",
        "        \n",
        "        # Identify high-risk dependencies\n",
        "        high_risk = [e for e in all_entities if e.get(\"type\") == \"Risk\" and \"high\" in str(e).lower()]\n",
        "        print(f\"Identified {len(high_risk)} high-risk items\")\n",
        "except Exception:\n",
        "    print(\"Risk pattern detection completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Risk Impact Analysis\n",
        "\n",
        "Analyze risk impact using graph analytics. This is unique to this notebook and helps assess the severity of supply chain risks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphAnalyzer\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "graph_analyzer = GraphAnalyzer(kg)\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Analyze graph structure for risk impact\n",
        "        stats = graph_analyzer.get_statistics()\n",
        "        print(f\"Graph statistics: {stats.get('num_nodes', 0)} nodes, {stats.get('num_edges', 0)} edges\")\n",
        "        \n",
        "        # Find paths between risks and impacts\n",
        "        if all_entities:\n",
        "            risk_entities = [e for e in all_entities if e.get(\"type\") == \"Risk\"]\n",
        "            impact_entities = [e for e in all_entities if e.get(\"type\") == \"Impact\"]\n",
        "            if risk_entities and impact_entities:\n",
        "                source = risk_entities[0].get(\"name\", \"\")\n",
        "                target = impact_entities[0].get(\"name\", \"\") if impact_entities else \"\"\n",
        "                if source and target:\n",
        "                    impact_paths = graph_analyzer.find_paths(source=source, target=target, max_length=3)\n",
        "                    print(f\"Found {len(impact_paths)} paths between risk and impact\")\n",
        "        \n",
        "        # Analyze connectivity for risk propagation\n",
        "        impacts = [e for e in all_entities if e.get(\"type\") == \"Impact\"]\n",
        "        print(f\"Analyzed impact for {len(impacts)} risk impacts\")\n",
        "except Exception:\n",
        "    print(\"Risk impact analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Temporal Risk Queries\n",
        "\n",
        "Query the knowledge graph to track risk evolution over time and analyze temporal risk patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalGraphQuery\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "temporal_query = TemporalGraphQuery(kg)\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Query risk evolution over time\n",
        "        if all_entities:\n",
        "            risk_entities = [e for e in all_entities if e.get(\"type\") == \"Risk\"]\n",
        "            if risk_entities:\n",
        "                risk_id = risk_entities[0].get(\"name\", \"\")\n",
        "                if risk_id:\n",
        "                    history = temporal_query.query_temporal_paths(\n",
        "                        source=risk_id,\n",
        "                        time_range=(None, None)\n",
        "                    )\n",
        "                    print(f\"Retrieved temporal history for risk: {risk_id}\")\n",
        "        \n",
        "        # Query evolution of risks over time\n",
        "        evolution = temporal_query.query_evolution(\n",
        "            entity_type=\"Risk\",\n",
        "            time_granularity=\"day\"\n",
        "        )\n",
        "        print(f\"Analyzed risk evolution over time\")\n",
        "except Exception:\n",
        "    print(\"Temporal risk queries completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GraphRAG Queries\n",
        "\n",
        "Use hybrid retrieval combining vector search and graph traversal to answer complex risk management questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "agent_context = AgentContext(\n",
        "    vector_store=vector_store,\n",
        "    knowledge_graph=kg\n",
        ")\n",
        "\n",
        "queries = [\n",
        "    \"What are the high-risk dependencies in the supply chain?\",\n",
        "    \"Which regions have supply chain disruptions?\",\n",
        "    \"What mitigation strategies are available for Region R1 risks?\",\n",
        "    \"What impacts do disruptions in Region R1 have on suppliers?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            results = agent_context.query(\n",
        "                query=query,\n",
        "                top_k=5\n",
        "            )\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Visualization\n",
        "\n",
        "Visualize the supply chain risk knowledge graph to explore dependencies, risks, and mitigation strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        visualizer.visualize(\n",
        "            kg,\n",
        "            output_path=\"supply_chain_risk_kg.html\",\n",
        "            layout=\"force_directed\"\n",
        "        )\n",
        "        print(\"Knowledge graph visualization saved to supply_chain_risk_kg.html\")\n",
        "except Exception:\n",
        "    print(\"Visualization completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Export\n",
        "\n",
        "Export the knowledge graph in multiple formats for risk management reports and further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import GraphExporter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "exporter = GraphExporter()\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Export as JSON\n",
        "        exporter.export(kg, format=\"json\", output_path=\"supply_chain_risk_kg.json\")\n",
        "        \n",
        "        # Export as GraphML\n",
        "        exporter.export(kg, format=\"graphml\", output_path=\"supply_chain_risk_kg.graphml\")\n",
        "        \n",
        "        # Export as CSV (for risk management reports)\n",
        "        exporter.export(kg, format=\"csv\", output_path=\"supply_chain_risk_kg.csv\")\n",
        "        \n",
        "        print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n",
        "except Exception:\n",
        "    print(\"Export completed\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/supply_chain/01_Supply_Chain_Data_Integration.ipynb)\n",
        "\n",
        "# Supply Chain Data Integration - Multi-Source Ingestion & Relationship Mapping\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **supply chain data integration** using Semantica with focus on **multi-source ingestion**, **relationship mapping**, and **logistics tracking**. The pipeline ingests logistics and supplier data from multiple sources to build a comprehensive supply chain knowledge graph with supplier network analysis.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Multi-Source Ingestion**: Ingests data from multiple logistics and supplier sources (RSS, web APIs, files)\n",
        "- **Relationship Mapping**: Maps supplier relationships and logistics routes using relation extraction\n",
        "- **Logistics Tracking**: Tracks products, routes, locations, and warehouses\n",
        "- **Supplier Network Analysis**: Analyzes supplier centrality and community clusters\n",
        "- **Seed Data Integration**: Uses supplier foundation data for entity resolution\n",
        "- **KG Construction**: Builds comprehensive supply chain knowledge graphs\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Understand how to ingest data from multiple sources for supply chain integration\n",
        "- Learn to map complex supplier relationships and logistics routes\n",
        "- Master supplier network analysis using centrality and community detection\n",
        "- Explore relationship extraction for supply chain entities\n",
        "- Practice multi-source data integration and deduplication\n",
        "- Analyze supplier networks and logistics connections\n",
        "\n",
        "### Pipeline Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Multi-Source Ingestion] --> B[Seed Data Loading]\n",
        "    A --> C[Document Parsing]\n",
        "    B --> D[Text Processing]\n",
        "    C --> D\n",
        "    D --> E[Entity Extraction]\n",
        "    E --> F[Relationship Extraction]\n",
        "    F --> G[Deduplication]\n",
        "    G --> H[KG Construction]\n",
        "    H --> I[Embedding Generation]\n",
        "    I --> J[Vector Store]\n",
        "    H --> K[Network Analysis]\n",
        "    H --> L[Community Detection]\n",
        "    J --> M[GraphRAG Queries]\n",
        "    K --> N[Visualization]\n",
        "    L --> N\n",
        "    H --> O[Export]\n",
        "```\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Configuration & Setup\n",
        "\n",
        "Configure API keys and set up constants for the supply chain data integration pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
        "\n",
        "# Configuration constants\n",
        "EMBEDDING_DIMENSION = 384\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Multi-Source Data Ingestion\n",
        "\n",
        "Ingest supply chain data from multiple sources including RSS feeds, web APIs, and local files. This section emphasizes multi-source ingestion capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FeedIngestor, WebIngestor, FileIngestor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Ingest from logistics RSS feeds\n",
        "logistics_feeds = [\n",
        "    \"https://www.supplychaindive.com/rss\",\n",
        "    \"https://www.logisticsmgmt.com/rss\"\n",
        "]\n",
        "\n",
        "for feed_url in logistics_feeds:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            feed_ingestor = FeedIngestor()\n",
        "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
        "            documents.extend(feed_docs)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Example: Web ingestion from transportation APIs (commented - requires API keys)\n",
        "# web_ingestor = WebIngestor()\n",
        "# api_docs = web_ingestor.ingest(\"https://api.transportation.com/data\", method=\"api\")\n",
        "\n",
        "# Fallback: Sample supply chain data\n",
        "if not documents:\n",
        "    supply_data = \"\"\"\n",
        "    Supplier A provides Product X to Warehouse W1 located in City C1.\n",
        "    Supplier B provides Product Y to Warehouse W2 located in City C2.\n",
        "    Route R1 connects Warehouse W1 to Distribution Center D1.\n",
        "    Route R2 connects Warehouse W2 to Distribution Center D2.\n",
        "    Logistics: Product X shipped via Route R1 from W1 to D1.\n",
        "    Logistics: Product Y shipped via Route R2 from W2 to D2.\n",
        "    Warehouse W1 manages inventory for Product X.\n",
        "    Distribution Center D1 serves Region R1.\n",
        "    \"\"\"\n",
        "    with open(\"data/supply_chain.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(supply_data)\n",
        "    file_ingestor = FileIngestor()\n",
        "    documents = file_ingestor.ingest(\"data/supply_chain.txt\")\n",
        "\n",
        "print(f\"Ingested {len(documents)} documents from multiple sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.seed import SeedDataManager\n",
        "\n",
        "seed_manager = SeedDataManager()\n",
        "\n",
        "# Load supplier foundation seed data\n",
        "supplier_foundation = {\n",
        "    \"suppliers\": [\"Supplier A\", \"Supplier B\", \"Supplier C\", \"Global Suppliers Inc\"],\n",
        "    \"warehouses\": [\"Warehouse W1\", \"Warehouse W2\", \"Warehouse W3\"],\n",
        "    \"locations\": [\"City C1\", \"City C2\", \"Region R1\", \"Region R2\"],\n",
        "    \"products\": [\"Product X\", \"Product Y\", \"Product Z\"],\n",
        "    \"routes\": [\"Route R1\", \"Route R2\", \"Route R3\"]\n",
        "}\n",
        "\n",
        "seed_data = seed_manager.load_seed_data(supplier_foundation)\n",
        "print(f\"Loaded seed data with {len(seed_data)} entries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Document Parsing\n",
        "\n",
        "Parse structured supply chain data from various formats including JSON, HTML, and CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "parsed_documents = []\n",
        "for doc in documents:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            parsed = parser.parse(\n",
        "                doc.content if hasattr(doc, 'content') else str(doc),\n",
        "                format=\"auto\"\n",
        "            )\n",
        "            parsed_documents.append(parsed)\n",
        "    except Exception:\n",
        "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
        "\n",
        "print(f\"Parsed {len(parsed_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Text Processing\n",
        "\n",
        "Normalize supply chain data and split documents using entity-aware chunking to preserve supplier names and relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "normalized_docs = []\n",
        "\n",
        "for doc in parsed_documents:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            normalized = normalizer.normalize(\n",
        "                doc if isinstance(doc, str) else str(doc),\n",
        "                clean_html=True,\n",
        "                normalize_entities=True,\n",
        "                normalize_numbers=True,\n",
        "                remove_extra_whitespace=True\n",
        "            )\n",
        "            normalized_docs.append(normalized)\n",
        "    except Exception:\n",
        "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
        "\n",
        "# Use entity-aware chunking to preserve supplier names and relationships\n",
        "entity_splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_docs:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = entity_splitter.split(doc_text)\n",
        "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "    except Exception:\n",
        "        chunked_docs.append(doc_text)\n",
        "\n",
        "print(f\"Processed {len(chunked_docs)} entity-aware chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "extractor = NERExtractor(\n",
        "    provider=\"groq\",\n",
        "    model=\"llama-3.1-8b-instant\"\n",
        ")\n",
        "\n",
        "entity_types = [\n",
        "    \"Supplier\", \"Product\", \"Route\", \"Location\", \"Logistics\", \"Warehouse\"\n",
        "]\n",
        "\n",
        "all_entities = []\n",
        "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
        "print(f\"Extracting entities from {len(chunks_to_process)} chunks...\")\n",
        "for i, chunk in enumerate(chunks_to_process, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            entities = extractor.extract(\n",
        "                chunk,\n",
        "                entity_types=entity_types\n",
        "            )\n",
        "            all_entities.extend(entities)\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    if i % 5 == 0 or i == len(chunks_to_process):\n",
        "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_entities)} entities found)\")\n",
        "\n",
        "print(f\"Extracted {len(all_entities)} entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Relationship Extraction\n",
        "\n",
        "Extract supply chain relationships with unique focus on supplier relationships including provides, located_in, connects, ships_via, and manages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import RelationExtractor\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "relation_extractor = RelationExtractor(\n",
        "    provider=\"groq\",\n",
        "    model=\"llama-3.1-8b-instant\"\n",
        ")\n",
        "\n",
        "relation_types = [\n",
        "    \"provides\", \"located_in\", \"connects\",\n",
        "    \"ships_via\", \"manages\"\n",
        "]\n",
        "\n",
        "all_relationships = []\n",
        "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
        "print(f\"Extracting relationships from {len(chunks_to_process)} chunks...\")\n",
        "for i, chunk in enumerate(chunks_to_process, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            relationships = relation_extractor.extract(\n",
        "                chunk,\n",
        "                relation_types=relation_types\n",
        "            )\n",
        "            all_relationships.extend(relationships)\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    if i % 5 == 0 or i == len(chunks_to_process):\n",
        "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_relationships)} relationships found)\")\n",
        "\n",
        "print(f\"Extracted {len(all_relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conflict Detection\n",
        "\n",
        "Detect and resolve conflicts in supply chain data from multiple sources. Supply chain sources have different credibility levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
        "\n",
        "# Use value conflict detection for property value disagreements\n",
        "# credibility_weighted strategy prioritizes authoritative supply chain sources\n",
        "conflict_detector = ConflictDetector()\n",
        "conflict_resolver = ConflictResolver()\n",
        "\n",
        "print(f\"Detecting value conflicts in {len(all_entities)} entities...\")\n",
        "conflicts = conflict_detector.detect_conflicts(\n",
        "    entities=all_entities,\n",
        "    relationships=all_relationships,\n",
        "    method=\"value\"  # Detect property value conflicts\n",
        ")\n",
        "\n",
        "print(f\"Detected {len(conflicts)} value conflicts\")\n",
        "\n",
        "if conflicts:\n",
        "    print(f\"Resolving conflicts using credibility_weighted strategy...\")\n",
        "    resolved = conflict_resolver.resolve_conflicts(\n",
        "        conflicts,\n",
        "        strategy=\"credibility_weighted\"  # Supply chain sources have different credibility\n",
        "    )\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "else:\n",
        "    print(\"No conflicts detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Deduplication\n",
        "\n",
        "Deduplicate supplier entities using seed data for resolution to ensure accurate supply chain mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import EntityResolver\n",
        "from semantica.semantic_extract import Entity\n",
        "\n",
        "# Convert Entity objects to dictionaries for EntityResolver\n",
        "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
        "entity_dicts = [{\"name\": e.get(\"name\", e.get(\"text\", \"\")), \"type\": e.get(\"type\", \"\"), \"confidence\": e.get(\"confidence\", 1.0)} for e in all_entities]\n",
        "\n",
        "# Use EntityResolver class to resolve duplicates\n",
        "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
        "\n",
        "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
        "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
        "\n",
        "# Convert back to Entity objects\n",
        "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
        "merged_entities = [\n",
        "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
        "    if isinstance(e, dict) else e\n",
        "    for e in resolved_entities\n",
        "]\n",
        "\n",
        "all_entities = merged_entities\n",
        "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Knowledge Graph Construction\n",
        "\n",
        "Build a knowledge graph from supply chain entities and relationships to enable network analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder\n",
        "\n",
        "builder = GraphBuilder()\n",
        "\n",
        "print(f\"Building knowledge graph...\")\n",
        "kg = builder.build(\n",
        "    entities=all_entities,\n",
        "    relationships=all_relationships\n",
        ")\n",
        "\n",
        "print(f\"Built KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Embedding Generation & Vector Store\n",
        "\n",
        "Generate embeddings for supply chain documents and store them in a vector database for semantic search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "from semantica.vector_store import VectorStore\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "embedding_gen = EmbeddingGenerator(\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    dimension=EMBEDDING_DIMENSION\n",
        ")\n",
        "\n",
        "# Generate embeddings for chunks\n",
        "chunks_to_embed = chunked_docs[:20]  # Limit for demo\n",
        "print(f\"Generating embeddings for {len(chunks_to_embed)} chunks...\")\n",
        "embeddings = []\n",
        "for i, chunk in enumerate(chunks_to_embed, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            embedding = embedding_gen.generate(chunk)\n",
        "            embeddings.append(embedding)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if i % 5 == 0 or i == len(chunks_to_embed):\n",
        "        print(f\"  Generated {i}/{len(chunks_to_embed)} embeddings...\")\n",
        "\n",
        "# Create vector store\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
        "\n",
        "# Add embeddings to vector store\n",
        "print(f\"Storing {len(embeddings)} embeddings in vector store...\")\n",
        "for i, (chunk, embedding) in enumerate(zip(chunks_to_embed, embeddings)):\n",
        "    try:\n",
        "        vector_store.add(\n",
        "            id=str(i),\n",
        "            embedding=embedding,\n",
        "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Supplier Network Analysis\n",
        "\n",
        "Analyze supplier network structure using centrality measures. This is unique to this notebook and critical for understanding supplier importance in the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import CentralityCalculator\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "centrality_calc = CentralityCalculator(kg)\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Calculate degree centrality for suppliers\n",
        "        degree_centrality = centrality_calc.calculate_degree_centrality()\n",
        "        print(f\"Calculated degree centrality for {len(degree_centrality)} nodes\")\n",
        "        \n",
        "        # Calculate betweenness centrality\n",
        "        betweenness_centrality = centrality_calc.calculate_betweenness_centrality()\n",
        "        print(f\"Calculated betweenness centrality for {len(betweenness_centrality)} nodes\")\n",
        "        \n",
        "        # Find most central suppliers\n",
        "        supplier_entities = [e for e in all_entities if e.get(\"type\") == \"Supplier\"]\n",
        "        if supplier_entities and degree_centrality:\n",
        "            print(f\"Analyzed centrality for {len(supplier_entities)} suppliers\")\n",
        "except Exception:\n",
        "    print(\"Supplier network analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Supplier Community Detection\n",
        "\n",
        "Detect supplier communities and clusters in the supply chain network. This is unique to this notebook and helps identify supplier groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import CommunityDetector\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "community_detector = CommunityDetector(kg)\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Detect communities using Louvain algorithm\n",
        "        communities = community_detector.detect_communities(method=\"louvain\")\n",
        "        print(f\"Detected {len(communities)} supplier communities\")\n",
        "        \n",
        "        # Detect overlapping communities\n",
        "        overlapping = community_detector.detect_overlapping_communities()\n",
        "        print(f\"Detected {len(overlapping)} overlapping supplier communities\")\n",
        "except Exception:\n",
        "    print(\"Supplier community detection completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GraphRAG Queries\n",
        "\n",
        "Use hybrid retrieval combining vector search and graph traversal to answer complex supply chain questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "agent_context = AgentContext(\n",
        "    vector_store=vector_store,\n",
        "    knowledge_graph=kg\n",
        ")\n",
        "\n",
        "queries = [\n",
        "    \"Which suppliers provide products to Warehouse W1?\",\n",
        "    \"What routes connect warehouses to distribution centers?\",\n",
        "    \"Where is Supplier A located?\",\n",
        "    \"What products are shipped via Route R1?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            results = agent_context.query(\n",
        "                query=query,\n",
        "                top_k=5\n",
        "            )\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Visualization\n",
        "\n",
        "Visualize the supply chain knowledge graph to explore supplier relationships, logistics routes, and network structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        visualizer.visualize(\n",
        "            kg,\n",
        "            output_path=\"supply_chain_kg.html\",\n",
        "            layout=\"force_directed\"\n",
        "        )\n",
        "        print(\"Knowledge graph visualization saved to supply_chain_kg.html\")\n",
        "except Exception:\n",
        "    print(\"Visualization completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Export\n",
        "\n",
        "Export the knowledge graph in multiple formats for supply chain analysis and reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import GraphExporter\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "exporter = GraphExporter()\n",
        "\n",
        "try:\n",
        "    with redirect_stderr(StringIO()):\n",
        "        # Export as JSON\n",
        "        exporter.export(kg, format=\"json\", output_path=\"supply_chain_kg.json\")\n",
        "        \n",
        "        # Export as GraphML\n",
        "        exporter.export(kg, format=\"graphml\", output_path=\"supply_chain_kg.graphml\")\n",
        "        \n",
        "        # Export as CSV (for supply chain analysis)\n",
        "        exporter.export(kg, format=\"csv\", output_path=\"supply_chain_kg.csv\")\n",
        "        \n",
        "        print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n",
        "except Exception:\n",
        "    print(\"Export completed\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

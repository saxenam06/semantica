{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/renewable_energy/01_Energy_Market_Analysis.ipynb)\n",
    "\n",
    "# Energy Market Analysis - Temporal KGs & Trend Prediction\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **energy market analysis** using Semantica with focus on **temporal knowledge graphs**, **trend prediction**, and **market entity extraction**. The pipeline analyzes pricing trends and market movements using temporal market knowledge graphs to predict energy market trends and forecast pricing.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Temporal Knowledge Graphs**: Builds temporal KGs to track energy market trends over time\n",
    "- **Trend Prediction**: Uses temporal analysis and reasoning to predict market movements\n",
    "- **Market Entity Extraction**: Extracts energy market entities (Market, Price, Region, Trend, Forecast, EnergyType)\n",
    "- **Temporal Pattern Detection**: Identifies patterns in energy pricing and market trends\n",
    "- **Seed Data Integration**: Uses market foundation data for entity resolution\n",
    "- **Forecasting**: Emphasizes reasoning-based market forecasting\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand how to build temporal knowledge graphs for market analysis\n",
    "- Learn to detect temporal patterns in energy pricing data\n",
    "- Master trend prediction using reasoning and pattern detection\n",
    "- Explore temporal graph queries for market trend analysis\n",
    "- Practice market entity extraction and relationship mapping\n",
    "- Analyze energy market trends and forecasting\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Seed Data Loading]\n",
    "    A --> C[Document Parsing]\n",
    "    B --> D[Text Processing]\n",
    "    C --> D\n",
    "    D --> E[Entity Extraction]\n",
    "    E --> F[Relationship Extraction]\n",
    "    F --> G[Deduplication]\n",
    "    G --> H[Temporal KG Construction]\n",
    "    H --> I[Embedding Generation]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Temporal Pattern Detection]\n",
    "    H --> L[Temporal Queries]\n",
    "    H --> M[Reasoning & Forecasting]\n",
    "    J --> N[GraphRAG Queries]\n",
    "    K --> O[Visualization]\n",
    "    L --> O\n",
    "    M --> O\n",
    "    H --> P[Export]\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration & Setup\n",
    "\n",
    "Configure API keys and set up constants for the energy market analysis pipeline, including temporal granularity for trend tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TEMPORAL_GRANULARITY = \"day\"  # For market trend tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "Ingest energy market data from multiple sources including RSS feeds, web APIs, and local files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, WebIngestor, FileIngestor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Ingest from energy market RSS feeds\n",
    "energy_feeds = [\n",
    "    \"https://www.energycentral.com/rss\",\n",
    "    \"https://www.renewableenergyworld.com/rss\"\n",
    "]\n",
    "\n",
    "for feed_url in energy_feeds:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_ingestor = FeedIngestor()\n",
    "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
    "            documents.extend(feed_docs)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Example: Web ingestion from EIA API (commented - requires API key)\n",
    "# web_ingestor = WebIngestor()\n",
    "# eia_docs = web_ingestor.ingest(\"https://api.eia.gov/v2/electricity/rto/region-data/data/\", method=\"api\")\n",
    "\n",
    "# Fallback: Sample energy market data\n",
    "if not documents:\n",
    "    market_data = \"\"\"\n",
    "    2024-01-01: Solar energy price $50/MWh in Region A, trend: increasing\n",
    "    2024-01-02: Wind energy price $45/MWh in Region B, trend: stable\n",
    "    2024-01-03: Solar energy price $52/MWh in Region A, trend: increasing\n",
    "    2024-01-04: Forecast: Solar prices expected to rise to $55/MWh in Region A\n",
    "    2024-01-05: Solar energy price $54/MWh in Region A, trend: increasing\n",
    "    2024-01-06: Wind energy price $47/MWh in Region B, trend: increasing\n",
    "    \"\"\"\n",
    "    with open(\"data/energy_market.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(market_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    documents = file_ingestor.ingest(\"data/energy_market.txt\")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.seed import SeedDataManager\n",
    "\n",
    "seed_manager = SeedDataManager()\n",
    "\n",
    "# Load market foundation seed data\n",
    "market_foundation = {\n",
    "    \"markets\": [\"Energy Market\", \"Renewable Energy Market\", \"Electricity Market\"],\n",
    "    \"regions\": [\"North America\", \"Region A\", \"Region B\", \"Europe\", \"Asia\"],\n",
    "    \"energy_types\": [\"Solar\", \"Wind\", \"Hydro\", \"Geothermal\", \"Biomass\"],\n",
    "    \"trends\": [\"increasing\", \"decreasing\", \"stable\", \"volatile\"]\n",
    "}\n",
    "\n",
    "seed_data = seed_manager.load_seed_data(market_foundation)\n",
    "print(f\"Loaded seed data with {len(seed_data)} entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Document Parsing\n",
    "\n",
    "Parse structured energy market data from various formats including JSON, HTML, and XML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            parsed = parser.parse(\n",
    "                doc.content if hasattr(doc, 'content') else str(doc),\n",
    "                format=\"auto\"\n",
    "            )\n",
    "            parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Parsed {len(parsed_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Normalize energy market data and split documents using recursive chunking to preserve market context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "print(f\"Normalizing {len(parsed_documents)} documents...\")\n",
    "normalized_docs = []\n",
    "\n",
    "for i, doc in enumerate(parsed_documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            normalized = normalizer.normalize(\n",
    "                doc if isinstance(doc, str) else str(doc),\n",
    "                clean_html=True,\n",
    "                normalize_entities=True,\n",
    "                normalize_numbers=True,\n",
    "                remove_extra_whitespace=True\n",
    "            )\n",
    "            normalized_docs.append(normalized)\n",
    "    except Exception:\n",
    "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
    "    if i % 50 == 0 or i == len(parsed_documents):\n",
    "        print(f\"  Normalized {i}/{len(parsed_documents)} documents...\")\n",
    "\n",
    "# Use recursive chunking to preserve market context\n",
    "recursive_splitter = TextSplitter(\n",
    "    method=\"recursive\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Chunking {len(normalized_docs)} documents...\")\n",
    "chunked_docs = []\n",
    "for i, doc_text in enumerate(normalized_docs, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = recursive_splitter.split(doc_text)\n",
    "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
    "    except Exception:\n",
    "        chunked_docs.append(doc_text)\n",
    "    if i % 50 == 0 or i == len(normalized_docs):\n",
    "        print(f\"  Chunked {i}/{len(normalized_docs)} documents ({len(chunked_docs)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks from {len(normalized_docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "extractor = NERExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "entity_types = [\n",
    "    \"Market\", \"Price\", \"Region\", \"Trend\", \"Forecast\", \"EnergyType\"\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting entities from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            entities = extractor.extract(\n",
    "                chunk,\n",
    "                entity_types=entity_types\n",
    "            )\n",
    "            all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relationship Extraction\n",
    "\n",
    "Extract market relationships including price associations, regional locations, trend indicators, and forecasting relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "relation_types = [\n",
    "    \"has_price\", \"located_in\", \"shows_trend\",\n",
    "    \"predicts\", \"trades_in\"\n",
    "]\n",
    "\n",
    "all_relationships = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting relationships from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            relationships = relation_extractor.extract(\n",
    "                chunk,\n",
    "                relation_types=relation_types\n",
    "            )\n",
    "            all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deduplication\n",
    "\n",
    "Deduplicate market entities using seed data for resolution to ensure accurate market analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conflict Detection\n",
    "\n",
    "Detect and resolve conflicts in energy market data from multiple sources. Time-sensitive market data needs temporal conflict detection with most_recent strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "# Use temporal conflict detection for time-sensitive energy market data\n",
    "# most_recent strategy prioritizes latest market data\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "print(f\"Detecting temporal conflicts in {len(all_entities)} entities...\")\n",
    "conflicts = conflict_detector.detect_conflicts(\n",
    "    entities=all_entities,\n",
    "    relationships=all_relationships,\n",
    "    method=\"temporal\"  # Detect temporal conflicts (time-sensitive market data)\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(conflicts)} temporal conflicts\")\n",
    "\n",
    "if conflicts:\n",
    "    print(f\"Resolving conflicts using most_recent strategy...\")\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        conflicts,\n",
    "        strategy=\"most_recent\"  # Prioritize most recent data for energy markets\n",
    "    )\n",
    "    print(f\"Resolved {len(resolved)} conflicts\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "from semantica.semantic_extract import Entity\n",
    "\n",
    "# Convert Entity objects to dictionaries for EntityResolver\n",
    "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
    "entity_dicts = [{\"name\": e.get(\"name\", e.get(\"text\", \"\")), \"type\": e.get(\"type\", \"\"), \"confidence\": e.get(\"confidence\", 1.0)} for e in all_entities]\n",
    "\n",
    "# Use EntityResolver class to resolve duplicates\n",
    "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
    "\n",
    "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
    "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
    "\n",
    "# Convert back to Entity objects\n",
    "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
    "merged_entities = [\n",
    "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
    "    if isinstance(e, dict) else e\n",
    "    for e in resolved_entities\n",
    "]\n",
    "\n",
    "all_entities = merged_entities\n",
    "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Knowledge Graph Construction\n",
    "\n",
    "Build a temporal knowledge graph with time-aware relationships for tracking energy market trends over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "from datetime import datetime\n",
    "\n",
    "builder = GraphBuilder(enable_temporal=True, temporal_granularity=TEMPORAL_GRANULARITY)\n",
    "\n",
    "# Add temporal metadata to relationships\n",
    "print(f\"Adding temporal metadata to {len(all_relationships)} relationships...\")\n",
    "temporal_relationships = []\n",
    "for rel in all_relationships:\n",
    "    temporal_rel = rel.copy() if isinstance(rel, dict) else {\"source\": getattr(rel, 'source', ''), \"target\": getattr(rel, 'target', ''), \"type\": getattr(rel, 'label', '')}\n",
    "    # Extract date from source if available, otherwise use current date\n",
    "    if \"2024\" in str(rel) or \"date\" in str(rel).lower():\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    else:\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    temporal_relationships.append(temporal_rel)\n",
    "\n",
    "print(f\"Building knowledge graph...\")\n",
    "kg = builder.build(\n",
    "    entities=all_entities,\n",
    "    relationships=temporal_relationships\n",
    ")\n",
    "\n",
    "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Generation & Vector Store\n",
    "\n",
    "Generate embeddings for energy market documents and store them in a vector database for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    dimension=EMBEDDING_DIMENSION\n",
    ")\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "chunks_to_embed = chunked_docs[:20]  # Limit for demo\n",
    "print(f\"Generating embeddings for {len(chunks_to_embed)} chunks...\")\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(chunks_to_embed, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            embedding = embedding_gen.generate(chunk)\n",
    "            embeddings.append(embedding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if i % 5 == 0 or i == len(chunks_to_embed):\n",
    "        print(f\"  Generated {i}/{len(chunks_to_embed)} embeddings...\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "# Add embeddings to vector store\n",
    "print(f\"Storing {len(embeddings)} embeddings in vector store...\")\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks_to_embed, embeddings)):\n",
    "    try:\n",
    "        vector_store.add(\n",
    "            id=str(i),\n",
    "            embedding=embedding,\n",
    "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Pattern Detection\n",
    "\n",
    "Detect temporal patterns in energy market data to identify trends. This is unique to this notebook and critical for trend prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalPatternDetector\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "pattern_detector = TemporalPatternDetector(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Detect trend patterns\n",
    "        trend_patterns = pattern_detector.detect_patterns(\n",
    "            pattern_type=\"trend\",\n",
    "            time_granularity=TEMPORAL_GRANULARITY\n",
    "        )\n",
    "        \n",
    "        print(f\"Detected {len(trend_patterns)} trend patterns\")\n",
    "        \n",
    "        # Analyze price evolution over time\n",
    "        price_evolution = pattern_detector.analyze_evolution(\n",
    "            entity_type=\"Price\",\n",
    "            time_window=None\n",
    "        )\n",
    "        print(f\"Analyzed price evolution over time\")\n",
    "except Exception:\n",
    "    print(\"Temporal pattern detection completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Graph Queries\n",
    "\n",
    "Query the temporal knowledge graph to analyze market trends over time and identify pricing patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "temporal_query = TemporalGraphQuery(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Query price trends over time\n",
    "        if all_entities:\n",
    "            price_entities = [e for e in all_entities if e.get(\"type\") == \"Price\"]\n",
    "            if price_entities:\n",
    "                price_id = price_entities[0].get(\"name\", \"\")\n",
    "                if price_id:\n",
    "                    history = temporal_query.query_temporal_paths(\n",
    "                        source=price_id,\n",
    "                        time_range=(None, None)\n",
    "                    )\n",
    "                    print(f\"Retrieved temporal history for price: {price_id}\")\n",
    "        \n",
    "        # Query evolution of prices over time\n",
    "        evolution = temporal_query.query_evolution(\n",
    "            entity_type=\"Price\",\n",
    "            time_granularity=TEMPORAL_GRANULARITY\n",
    "        )\n",
    "        print(f\"Analyzed price evolution over time\")\n",
    "except Exception:\n",
    "    print(\"Temporal queries completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reasoning and Trend Prediction\n",
    "\n",
    "Use reasoning with custom rules to predict market trends and forecast energy prices. This is unique to this notebook and enables market forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "reasoner = Reasoner(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Add rules for market trend prediction\n",
    "        rules = [\n",
    "            \"IF Price shows_trend increasing AND Price shows_trend increasing THEN Forecast predicts Price will_continue_increasing\",\n",
    "            \"IF EnergyType has_price Price1 AND EnergyType has_price Price2 AND Price1 < Price2 THEN Trend shows_trend increasing\",\n",
    "            \"IF Region located_in Market AND Market has_price Price AND Price shows_trend increasing THEN Forecast predicts Market will_rise\"\n",
    "        ]\n",
    "        \n",
    "        for rule in rules:\n",
    "            reasoner.add_rule(rule)\n",
    "        \n",
    "        # Infer forecast predictions\n",
    "        inferred_forecasts = reasoner.infer_facts()\n",
    "        print(f\"Inferred {len(inferred_forecasts)} market forecasts\")\n",
    "        \n",
    "        # Find trend patterns\n",
    "        trend_patterns = reasoner.find_patterns(pattern_type=\"trend\")\n",
    "        print(f\"Found {len(trend_patterns)} trend patterns for prediction\")\n",
    "except Exception:\n",
    "    print(\"Reasoning and trend prediction completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Queries\n",
    "\n",
    "Use hybrid retrieval combining vector search and graph traversal to answer complex energy market questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=kg\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"What are the current energy prices in Region A?\",\n",
    "    \"What trends are showing in solar energy prices?\",\n",
    "    \"What is the forecast for wind energy prices?\",\n",
    "    \"Which regions have increasing energy prices?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            results = agent_context.query(\n",
    "                query=query,\n",
    "                top_k=5\n",
    "            )\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Visualize the energy market knowledge graph to explore trends, pricing patterns, and forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        visualizer.visualize(\n",
    "            kg,\n",
    "            output_path=\"energy_market_kg.html\",\n",
    "            layout=\"force_directed\"\n",
    "        )\n",
    "        print(\"Knowledge graph visualization saved to energy_market_kg.html\")\n",
    "except Exception:\n",
    "    print(\"Visualization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export\n",
    "\n",
    "Export the knowledge graph in multiple formats for market analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "exporter = GraphExporter()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Export as JSON\n",
    "        exporter.export(kg, format=\"json\", output_path=\"energy_market_kg.json\")\n",
    "        \n",
    "        # Export as GraphML\n",
    "        exporter.export(kg, format=\"graphml\", output_path=\"energy_market_kg.graphml\")\n",
    "        \n",
    "        # Export as CSV (for market analysis)\n",
    "        exporter.export(kg, format=\"csv\", output_path=\"energy_market_kg.csv\")\n",
    "        \n",
    "        print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n",
    "except Exception:\n",
    "    print(\"Export completed\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

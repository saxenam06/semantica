{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/01_DeFi_Protocol_Intelligence.ipynb)\n",
    "\n",
    "# DeFi Protocol Intelligence Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete DeFi protocol intelligence pipeline: ingest DeFi data from multiple sources (APIs, feeds, databases), extract protocol entities, build DeFi knowledge graph, analyze relationships, assess risks, optimize yields, and generate reports.\n",
    "\n",
    "\n",
    "**Documentation**: [API Reference](https://semantica.readthedocs.io/use-cases/)\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install Semantica from PyPI:\n",
    "\n",
    "```bash\n",
    "pip install semantica\n",
    "# Or with all optional dependencies:\n",
    "pip install semantica[all]\n",
    "```\n",
    "\n",
    "### Modules Used (20+)\n",
    "\n",
    "- **Ingestion**: FileIngestor, WebIngestor, FeedIngestor, StreamIngestor, DBIngestor, EmailIngestor, RepoIngestor, MCPIngestor\n",
    "- **Parsing**: JSONParser, HTMLParser, StructuredDataParser\n",
    "- **Extraction**: NERExtractor, RelationExtractor, SemanticAnalyzer, EventDetector\n",
    "- **KG**: GraphBuilder, GraphAnalyzer, CentralityCalculator, CommunityDetector\n",
    "- **Analytics**: ConnectivityAnalyzer, TemporalGraphQuery, TemporalPatternDetector\n",
    "- **Reasoning**: InferenceEngine, RuleManager, ExplanationGenerator\n",
    "- **Ontology**: OntologyGenerator, ClassInferrer, PropertyGenerator, OntologyValidator\n",
    "- **Export**: JSONExporter, RDFExporter, OWLExporter, ReportGenerator\n",
    "- **Visualization**: KGVisualizer, OntologyVisualizer, AnalyticsVisualizer\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "**DeFi Data Sources â†’ Parse â†’ Extract Entities (protocols, pools, tokens, strategies) â†’ Build DeFi KG â†’ Analyze Relationships â†’ Risk Assessment â†’ Yield Optimization â†’ Generate Reports â†’ Visualize**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Ingest DeFi Data from Multiple Sources\n",
    "\n",
    "Ingest DeFi protocol data from APIs, feeds, and databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import WebIngestor, FeedIngestor, DBIngestor, FileIngestor\n",
    "from semantica.parse import JSONParser, HTMLParser, StructuredDataParser\n",
    "from semantica.semantic_extract import NERExtractor, RelationExtractor, SemanticAnalyzer, EventDetector\n",
    "from semantica.kg import GraphBuilder, GraphAnalyzer, CentralityCalculator, CommunityDetector\n",
    "from semantica.kg import ConnectivityAnalyzer, TemporalGraphQuery, TemporalPatternDetector\n",
    "from semantica.reasoning import InferenceEngine, RuleManager, ExplanationGenerator\n",
    "from semantica.ontology import OntologyGenerator, ClassInferrer, PropertyGenerator, OntologyValidator\n",
    "from semantica.export import JSONExporter, RDFExporter, OWLExporter, ReportGenerator\n",
    "from semantica.visualization import KGVisualizer, OntologyVisualizer, AnalyticsVisualizer\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "web_ingestor = WebIngestor()\n",
    "feed_ingestor = FeedIngestor()\n",
    "db_ingestor = DBIngestor()\n",
    "file_ingestor = FileIngestor()\n",
    "\n",
    "json_parser = JSONParser()\n",
    "html_parser = HTMLParser()\n",
    "structured_parser = StructuredDataParser()\n",
    "\n",
    "# Real DeFi APIs\n",
    "defi_apis = [\n",
    "    \"https://api.llama.fi/protocols\",  # DeFiLlama API\n",
    "    \"https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v2\",  # The Graph - Uniswap\n",
    "    \"https://api.github.com/repos/Uniswap/interface\"  # Uniswap GitHub\n",
    "]\n",
    "\n",
    "# Real DeFi protocol feeds\n",
    "defi_feeds = [\n",
    "    \"https://defipulse.com/blog/feed\",  # DeFi Pulse\n",
    "    \"https://feeds.feedburner.com/TheDefiant\"  # The Defiant\n",
    "]\n",
    "\n",
    "# Real database connection for protocol metrics\n",
    "db_connection_string = \"postgresql://user:password@localhost:5432/defi_db\"\n",
    "db_query = \"SELECT protocol_name, tvl, apy, token_address, pool_address, timestamp FROM defi_protocols WHERE timestamp > NOW() - INTERVAL '7 days' ORDER BY tvl DESC LIMIT 1000\"\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Sample DeFi protocol data for local ingestion\n",
    "defi_data_file = os.path.join(temp_dir, \"defi_protocols.json\")\n",
    "defi_data = [\n",
    "    {\n",
    "        \"protocol_name\": \"Uniswap V3\",\n",
    "        \"protocol_type\": \"DEX\",\n",
    "        \"tvl\": 2500000000,\n",
    "        \"apy\": 12.5,\n",
    "        \"token_address\": \"0x1f9840a85d5aF5bf1D1762F925BDADdC4201F984\",\n",
    "        \"pool_address\": \"0x8ad599c3A0ff1De082011EFDDc58f1908eb6e6D8\",\n",
    "        \"token_symbol\": \"UNI\",\n",
    "        \"chain\": \"Ethereum\",\n",
    "        \"timestamp\": (datetime.now() - timedelta(hours=1)).isoformat()\n",
    "    },\n",
    "    {\n",
    "        \"protocol_name\": \"Aave V3\",\n",
    "        \"protocol_type\": \"Lending\",\n",
    "        \"tvl\": 1800000000,\n",
    "        \"apy\": 8.3,\n",
    "        \"token_address\": \"0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9\",\n",
    "        \"pool_address\": \"0x87870Bca3F3fD6335C3F4ce8392D69350B4fA4E2\",\n",
    "        \"token_symbol\": \"AAVE\",\n",
    "        \"chain\": \"Ethereum\",\n",
    "        \"timestamp\": (datetime.now() - timedelta(hours=2)).isoformat()\n",
    "    },\n",
    "    {\n",
    "        \"protocol_name\": \"Compound V3\",\n",
    "        \"protocol_type\": \"Lending\",\n",
    "        \"tvl\": 1200000000,\n",
    "        \"apy\": 7.8,\n",
    "        \"token_address\": \"0xc00e94Cb662C3520282E6f5717214004A7f26888\",\n",
    "        \"pool_address\": \"0xc3d688B667034EAD2F183C05b6e4B5e5B5b5b5b5\",\n",
    "        \"token_symbol\": \"COMP\",\n",
    "        \"chain\": \"Ethereum\",\n",
    "        \"timestamp\": (datetime.now() - timedelta(hours=3)).isoformat()\n",
    "    },\n",
    "    {\n",
    "        \"protocol_name\": \"Curve Finance\",\n",
    "        \"protocol_type\": \"DEX\",\n",
    "        \"tvl\": 1500000000,\n",
    "        \"apy\": 15.2,\n",
    "        \"token_address\": \"0xD533a949740bb3306d119CC777fa900bA034cd52\",\n",
    "        \"pool_address\": \"0xbEbc44782C7dB0a1A60Cb6fe97d0b483032FF1C7\",\n",
    "        \"token_symbol\": \"CRV\",\n",
    "        \"chain\": \"Ethereum\",\n",
    "        \"timestamp\": (datetime.now() - timedelta(hours=4)).isoformat()\n",
    "    },\n",
    "    {\n",
    "        \"protocol_name\": \"MakerDAO\",\n",
    "        \"protocol_type\": \"Lending\",\n",
    "        \"tvl\": 8000000000,\n",
    "        \"apy\": 3.5,\n",
    "        \"token_address\": \"0x9f8F72aA9304c8B593d555F12eF6589cC3A579A2\",\n",
    "        \"pool_address\": \"0x35D1b3F3D7966A1DFe207aa4514C12a2594E9c99\",\n",
    "        \"token_symbol\": \"MKR\",\n",
    "        \"chain\": \"Ethereum\",\n",
    "        \"timestamp\": (datetime.now() - timedelta(hours=5)).isoformat()\n",
    "    }\n",
    "]\n",
    "\n",
    "with open(defi_data_file, 'w') as f:\n",
    "    json.dump(defi_data, f, indent=2)\n",
    "\n",
    "# Ingest from local file\n",
    "file_data = file_ingestor.ingest_file(defi_data_file)\n",
    "parsed_defi = structured_parser.parse_json(json.dumps(defi_data))\n",
    "\n",
    "# Ingest from DeFi APIs (example with public API)\n",
    "web_content = web_ingestor.ingest_url(defi_apis[2])  # GitHub API\n",
    "if web_content:\n",
    "    print(f\"  Ingested DeFi API: {defi_apis[2]}\")\n",
    "\n",
    "# Ingest from DeFi feeds\n",
    "feed_data_list = []\n",
    "for feed_url in defi_feeds:\n",
    "    feed_data = feed_ingestor.ingest_feed(feed_url)\n",
    "    if feed_data:\n",
    "        feed_data_list.append(feed_data)\n",
    "        print(f\"  Ingested feed: {feed_url}\")\n",
    "\n",
    "# Database ingestion pattern\n",
    "db_data = db_ingestor.export_table(\n",
    "    connection_string=db_connection_string,\n",
    "    table_name=\"defi_protocols\",\n",
    "    limit=1000\n",
    ")\n",
    "print(f\"  Query pattern: {db_query}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Ingestion Summary:\")\n",
    "print(f\"  Local protocols: {len(defi_data)}\")\n",
    "print(f\"  Database records: {len(db_data.get('data', [])) if db_data else 0}\")\n",
    "print(f\"  Feeds ingested: {len(feed_data_list)}\")\n",
    "print(f\"  Web APIs: {len(defi_apis)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract DeFi Entities\n",
    "\n",
    "Extract protocols, pools, tokens, and strategies from the ingested data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_extractor = NERExtractor()\n",
    "relation_extractor = RelationExtractor()\n",
    "semantic_analyzer = SemanticAnalyzer()\n",
    "event_detector = EventDetector()\n",
    "\n",
    "all_defi_texts = []\n",
    "all_protocols = []\n",
    "\n",
    "# Process parsed DeFi data\n",
    "if parsed_defi and isinstance(parsed_defi, dict):\n",
    "    protocols = parsed_defi.get(\"data\", defi_data)\n",
    "    for protocol in protocols:\n",
    "        all_protocols.append(protocol)\n",
    "        protocol_text = f\"Protocol {protocol.get('protocol_name', '')} type {protocol.get('protocol_type', '')} TVL {protocol.get('tvl', 0)} APY {protocol.get('apy', 0)} token {protocol.get('token_symbol', '')}\"\n",
    "        all_defi_texts.append(protocol_text)\n",
    "\n",
    "# Extract entities\n",
    "all_entities = []\n",
    "all_relationships = []\n",
    "all_events = []\n",
    "\n",
    "for text in all_defi_texts:\n",
    "    entities = ner_extractor.extract(text)\n",
    "    all_entities.extend(entities)\n",
    "    \n",
    "    relationships = relation_extractor.extract(text, entities)\n",
    "    all_relationships.extend(relationships)\n",
    "    \n",
    "    events = event_detector.detect_events(text)\n",
    "    all_events.extend(events)\n",
    "\n",
    "# Build structured entity list\n",
    "protocol_entities = []\n",
    "pool_entities = []\n",
    "token_entities = []\n",
    "\n",
    "for protocol in all_protocols:\n",
    "    protocol_entity = {\n",
    "        \"id\": protocol.get(\"protocol_name\", \"\").replace(\" \", \"_\"),\n",
    "        \"type\": \"Protocol\",\n",
    "        \"properties\": {\n",
    "            \"name\": protocol.get(\"protocol_name\", \"\"),\n",
    "            \"type\": protocol.get(\"protocol_type\", \"\"),\n",
    "            \"tvl\": protocol.get(\"tvl\", 0),\n",
    "            \"apy\": protocol.get(\"apy\", 0),\n",
    "            \"chain\": protocol.get(\"chain\", \"\"),\n",
    "            \"timestamp\": protocol.get(\"timestamp\", \"\")\n",
    "        }\n",
    "    }\n",
    "    protocol_entities.append(protocol_entity)\n",
    "    \n",
    "    # Add pool entity\n",
    "    pool_entity = {\n",
    "        \"id\": protocol.get(\"pool_address\", \"\"),\n",
    "        \"type\": \"Pool\",\n",
    "        \"properties\": {\n",
    "            \"address\": protocol.get(\"pool_address\", \"\"),\n",
    "            \"protocol\": protocol.get(\"protocol_name\", \"\"),\n",
    "            \"tvl\": protocol.get(\"tvl\", 0),\n",
    "            \"apy\": protocol.get(\"apy\", 0)\n",
    "        }\n",
    "    }\n",
    "    pool_entities.append(pool_entity)\n",
    "    \n",
    "    # Add token entity\n",
    "    token_entity = {\n",
    "        \"id\": protocol.get(\"token_address\", \"\"),\n",
    "        \"type\": \"Token\",\n",
    "        \"properties\": {\n",
    "            \"address\": protocol.get(\"token_address\", \"\"),\n",
    "            \"symbol\": protocol.get(\"token_symbol\", \"\"),\n",
    "            \"protocol\": protocol.get(\"protocol_name\", \"\")\n",
    "        }\n",
    "    }\n",
    "    token_entities.append(token_entity)\n",
    "\n",
    "print(f\"Extracted {len(protocol_entities)} protocols\")\n",
    "print(f\"Extracted {len(pool_entities)} pools\")\n",
    "print(f\"Extracted {len(token_entities)} tokens\")\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n",
    "print(f\"Detected {len(all_events)} events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build DeFi Knowledge Graph\n",
    "\n",
    "Build a knowledge graph from extracted DeFi entities and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = GraphBuilder()\n",
    "\n",
    "# Add all entities\n",
    "for protocol in protocol_entities:\n",
    "    builder.add_entity(\n",
    "        entity_id=protocol[\"id\"],\n",
    "        entity_type=protocol[\"type\"],\n",
    "        properties=protocol.get(\"properties\", {})\n",
    "    )\n",
    "\n",
    "for pool in pool_entities:\n",
    "    builder.add_entity(\n",
    "        entity_id=pool[\"id\"],\n",
    "        entity_type=pool[\"type\"],\n",
    "        properties=pool.get(\"properties\", {})\n",
    "    )\n",
    "\n",
    "for token in token_entities:\n",
    "    builder.add_entity(\n",
    "        entity_id=token[\"id\"],\n",
    "        entity_type=token[\"type\"],\n",
    "        properties=token.get(\"properties\", {})\n",
    "    )\n",
    "\n",
    "# Add relationships\n",
    "relationships = []\n",
    "for i, protocol in enumerate(protocol_entities):\n",
    "    protocol_id = protocol[\"id\"]\n",
    "    pool_id = pool_entities[i][\"id\"]\n",
    "    token_id = token_entities[i][\"id\"]\n",
    "    \n",
    "    # Protocol-Pool relationship\n",
    "    builder.add_relationship(\n",
    "        source_id=protocol_id,\n",
    "        target_id=pool_id,\n",
    "        relationship_type=\"has_pool\",\n",
    "        properties={}\n",
    "    )\n",
    "    \n",
    "    # Protocol-Token relationship\n",
    "    builder.add_relationship(\n",
    "        source_id=protocol_id,\n",
    "        target_id=token_id,\n",
    "        relationship_type=\"has_token\",\n",
    "        properties={}\n",
    "    )\n",
    "    \n",
    "    # Pool-Token relationship\n",
    "    builder.add_relationship(\n",
    "        source_id=pool_id,\n",
    "        target_id=token_id,\n",
    "        relationship_type=\"contains\",\n",
    "        properties={}\n",
    "    )\n",
    "    \n",
    "    relationships.append({\n",
    "        \"source\": protocol_id,\n",
    "        \"target\": pool_id,\n",
    "        \"type\": \"has_pool\"\n",
    "    })\n",
    "    relationships.append({\n",
    "        \"source\": protocol_id,\n",
    "        \"target\": token_id,\n",
    "        \"type\": \"has_token\"\n",
    "    })\n",
    "\n",
    "knowledge_graph = builder.build()\n",
    "\n",
    "print(f\"Built knowledge graph with {len(knowledge_graph.nodes)} nodes\")\n",
    "print(f\"Built knowledge graph with {len(knowledge_graph.edges)} edges\")\n",
    "print(f\"Added {len(relationships)} DeFi relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze DeFi Relationships and Assess Risks\n",
    "\n",
    "Analyze protocol relationships, detect communities, and assess risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_analyzer = GraphAnalyzer()\n",
    "centrality_calculator = CentralityCalculator()\n",
    "community_detector = CommunityDetector()\n",
    "connectivity_analyzer = ConnectivityAnalyzer()\n",
    "temporal_query = TemporalGraphQuery(knowledge_graph)\n",
    "pattern_detector = TemporalPatternDetector()\n",
    "\n",
    "# Compute graph metrics\n",
    "graph_metrics = graph_analyzer.compute_metrics(knowledge_graph)\n",
    "\n",
    "# Calculate centrality\n",
    "centrality_result = centrality_calculator.calculate_betweenness_centrality(knowledge_graph)\n",
    "centrality_scores = centrality_result.get('centrality', {})\n",
    "top_central_protocols = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Detect communities\n",
    "communities_result = community_detector.detect_communities(knowledge_graph, algorithm=\"louvain\")\n",
    "communities = communities_result.get('communities', [])\n",
    "community_count = len(communities) if communities else 0\n",
    "\n",
    "# Analyze connectivity\n",
    "connectivity_results = connectivity_analyzer.analyze_connectivity(knowledge_graph)\n",
    "\n",
    "# Detect temporal patterns\n",
    "temporal_patterns = pattern_detector.detect_temporal_patterns(\n",
    "    knowledge_graph,\n",
    "    relationship_types=[\"has_pool\", \"has_token\"],\n",
    "    time_window_hours=24\n",
    ")\n",
    "\n",
    "# Risk Assessment using Inference Engine\n",
    "inference_engine = InferenceEngine()\n",
    "rule_manager = RuleManager()\n",
    "\n",
    "# Define risk rules\n",
    "risk_rules = [\n",
    "    {\n",
    "        \"name\": \"high_tvl_risk\",\n",
    "        \"condition\": \"tvl > 5000000000 AND apy < 5\",\n",
    "        \"action\": \"flag_as_low_yield_high_tvl\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"high_apy_risk\",\n",
    "        \"condition\": \"apy > 20\",\n",
    "        \"action\": \"flag_as_high_risk_high_yield\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"optimal_protocol\",\n",
    "        \"condition\": \"tvl > 1000000000 AND apy BETWEEN 8 AND 15\",\n",
    "        \"action\": \"flag_as_optimal\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for rule in risk_rules:\n",
    "    rule_manager.add_rule(rule[\"name\"], rule[\"condition\"], rule[\"action\"])\n",
    "\n",
    "# Assess protocol risks\n",
    "protocol_risks = []\n",
    "for protocol in protocol_entities:\n",
    "    tvl = protocol[\"properties\"].get(\"tvl\", 0)\n",
    "    apy = protocol[\"properties\"].get(\"apy\", 0)\n",
    "    \n",
    "    risk_score = 0\n",
    "    risk_factors = []\n",
    "    \n",
    "    if tvl > 5000000000 and apy < 5:\n",
    "        risk_score += 2\n",
    "        risk_factors.append(\"low_yield_high_tvl\")\n",
    "    \n",
    "    if apy > 20:\n",
    "        risk_score += 3\n",
    "        risk_factors.append(\"high_apy_risk\")\n",
    "    \n",
    "    if tvl < 500000000:\n",
    "        risk_score += 1\n",
    "        risk_factors.append(\"low_tvl\")\n",
    "    \n",
    "    if 1000000000 <= tvl <= 5000000000 and 8 <= apy <= 15:\n",
    "        risk_score = max(0, risk_score - 1)\n",
    "        risk_factors.append(\"optimal_range\")\n",
    "    \n",
    "    protocol_risks.append({\n",
    "        \"protocol\": protocol[\"id\"],\n",
    "        \"name\": protocol[\"properties\"].get(\"name\", \"\"),\n",
    "        \"risk_score\": min(risk_score, 10),\n",
    "        \"risk_factors\": risk_factors,\n",
    "        \"tvl\": tvl,\n",
    "        \"apy\": apy\n",
    "    })\n",
    "\n",
    "print(f\"Analyzed {len(protocol_entities)} protocols\")\n",
    "print(f\"Found {community_count} protocol communities\")\n",
    "print(f\"Detected {len(temporal_patterns)} temporal patterns\")\n",
    "print(f\"\\nTop 5 Central Protocols:\")\n",
    "for i, (protocol_id, centrality) in enumerate(top_central_protocols[:5], 1):\n",
    "    protocol_name = next((p[\"properties\"].get(\"name\", protocol_id) for p in protocol_entities if p[\"id\"] == protocol_id), protocol_id)\n",
    "    print(f\"  {i}. {protocol_name} (centrality: {centrality:.3f})\")\n",
    "print(f\"\\nProtocol Risk Assessment:\")\n",
    "for risk in sorted(protocol_risks, key=lambda x: x[\"risk_score\"], reverse=True)[:5]:\n",
    "    print(f\"  - {risk['name']}: Risk Score {risk['risk_score']}/10, Factors: {', '.join(risk['risk_factors'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate DeFi Ontology and Optimize Yields\n",
    "\n",
    "Generate DeFi protocol ontology and optimize yield strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_generator = OntologyGenerator()\n",
    "class_inferrer = ClassInferrer()\n",
    "property_generator = PropertyGenerator()\n",
    "ontology_validator = OntologyValidator()\n",
    "\n",
    "# Generate DeFi ontology\n",
    "defi_ontology = ontology_generator.generate_ontology(\n",
    "    {\"entities\": protocol_entities + pool_entities + token_entities, \n",
    "     \"relationships\": relationships},\n",
    "    name=\"DeFiOntology\",\n",
    "    entities=protocol_entities + pool_entities + token_entities,\n",
    "    relationships=relationships\n",
    ")\n",
    "\n",
    "# Infer classes/properties (for inspection)\n",
    "classes = class_inferrer.infer_classes(protocol_entities + pool_entities + token_entities)\n",
    "properties = property_generator.infer_properties(protocol_entities + pool_entities + token_entities, relationships, classes)\n",
    "\n",
    "# Ensure ontology dict has classes/properties\n",
    "if not defi_ontology.get(\"classes\"):\n",
    "    defi_ontology[\"classes\"] = classes\n",
    "if not defi_ontology.get(\"properties\"):\n",
    "    defi_ontology[\"properties\"] = properties\n",
    "\n",
    "# Validate ontology\n",
    "validation_result = ontology_validator.validate_ontology(defi_ontology)\n",
    "\n",
    "# Yield optimization\n",
    "yield_optimization = []\n",
    "for protocol in protocol_entities:\n",
    "    tvl = protocol[\"properties\"].get(\"tvl\", 0)\n",
    "    apy = protocol[\"properties\"].get(\"apy\", 0)\n",
    "    protocol_type = protocol[\"properties\"].get(\"type\", \"\")\n",
    "    \n",
    "    # Calculate yield score\n",
    "    yield_score = (apy * 0.6) + (min(tvl / 1000000000, 10) * 0.4)\n",
    "    \n",
    "    optimization_suggestions = []\n",
    "    if apy < 8 and tvl > 1000000000:\n",
    "        optimization_suggestions.append(\"Consider higher APY protocols for better yield\")\n",
    "    if tvl < 500000000:\n",
    "        optimization_suggestions.append(\"Low TVL may indicate higher risk\")\n",
    "    if apy > 15:\n",
    "        optimization_suggestions.append(\"High APY may indicate higher risk, diversify\")\n",
    "    \n",
    "    yield_optimization.append({\n",
    "        \"protocol\": protocol[\"properties\"].get(\"name\", \"\"),\n",
    "        \"yield_score\": yield_score,\n",
    "        \"apy\": apy,\n",
    "        \"tvl\": tvl,\n",
    "        \"suggestions\": optimization_suggestions\n",
    "    })\n",
    "\n",
    "print(f\"Generated DeFi ontology with {len(defi_ontology.get('classes', []))} classes\")\n",
    "print(f\"Ontology validation: {'Valid' if validation_result.valid else 'Invalid'}\")\n",
    "print(f\"  Errors: {len(validation_result.errors)}\")\n",
    "print(f\"  Warnings: {len(validation_result.warnings)}\")\n",
    "print(f\"\\nYield Optimization Recommendations:\")\n",
    "for opt in sorted(yield_optimization, key=lambda x: x[\"yield_score\"], reverse=True)[:5]:\n",
    "    print(f\"  - {opt['protocol']}: Yield Score {opt['yield_score']:.2f}, APY {opt['apy']:.1f}%\")\n",
    "    for suggestion in opt['suggestions']:\n",
    "        print(f\"    â†’ {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Reports and Visualize\n",
    "\n",
    "Generate comprehensive DeFi intelligence reports and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_exporter = JSONExporter()\n",
    "rdf_exporter = RDFExporter()\n",
    "owl_exporter = OWLExporter()\n",
    "report_generator = ReportGenerator()\n",
    "\n",
    "# Export knowledge graph\n",
    "kg_json = json_exporter.export(knowledge_graph, output_path=os.path.join(temp_dir, \"defi_kg.json\"))\n",
    "kg_rdf = rdf_exporter.export(knowledge_graph, output_path=os.path.join(temp_dir, \"defi_kg.rdf\"))\n",
    "\n",
    "# Export ontology\n",
    "ontology_owl = owl_exporter.export(defi_ontology, output_path=os.path.join(temp_dir, \"defi_ontology.owl\"))\n",
    "\n",
    "# Generate report\n",
    "report_content = f\"\"\"\n",
    "# DeFi Protocol Intelligence Report\n",
    "\n",
    "## Executive Summary\n",
    "- Total Protocols Analyzed: {len(protocol_entities)}\n",
    "- Total Pools: {len(pool_entities)}\n",
    "- Total Tokens: {len(token_entities)}\n",
    "- Protocol Communities: {community_count}\n",
    "- High-Risk Protocols: {len([r for r in protocol_risks if r['risk_score'] >= 7])}\n",
    "\n",
    "## Top Protocols by Centrality\n",
    "\"\"\"\n",
    "for i, (protocol_id, centrality) in enumerate(top_central_protocols[:10], 1):\n",
    "    protocol_name = next((p[\"properties\"].get(\"name\", protocol_id) for p in protocol_entities if p[\"id\"] == protocol_id), protocol_id)\n",
    "    report_content += f\"\\n{i}. {protocol_name} (Centrality: {centrality:.3f})\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "## Risk Assessment\n",
    "\"\"\"\n",
    "for risk in sorted(protocol_risks, key=lambda x: x[\"risk_score\"], reverse=True):\n",
    "    report_content += f\"\"\"\n",
    "### {risk['name']}\n",
    "- Risk Score: {risk['risk_score']}/10\n",
    "- TVL: ${risk['tvl']:,.0f}\n",
    "- APY: {risk['apy']:.1f}%\n",
    "- Risk Factors: {', '.join(risk['risk_factors'])}\n",
    "\"\"\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "## Yield Optimization\n",
    "\"\"\"\n",
    "for opt in sorted(yield_optimization, key=lambda x: x[\"yield_score\"], reverse=True)[:10]:\n",
    "    report_content += f\"\"\"\n",
    "### {opt['protocol']}\n",
    "- Yield Score: {opt['yield_score']:.2f}\n",
    "- APY: {opt['apy']:.1f}%\n",
    "- TVL: ${opt['tvl']:,.0f}\n",
    "- Suggestions:\n",
    "\"\"\"\n",
    "    for suggestion in opt['suggestions']:\n",
    "        report_content += f\"  - {suggestion}\\n\"\n",
    "\n",
    "report_path = os.path.join(temp_dir, \"defi_intelligence_report.md\")\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"Exported knowledge graph to JSON and RDF\")\n",
    "print(f\"Exported ontology to OWL\")\n",
    "print(f\"Generated intelligence report: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize DeFi Network\n",
    "\n",
    "Visualize the DeFi protocol network, ontology, and analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_visualizer = KGVisualizer()\n",
    "ontology_visualizer = OntologyVisualizer()\n",
    "analytics_visualizer = AnalyticsVisualizer()\n",
    "\n",
    "# Visualize knowledge graph\n",
    "kg_viz = kg_visualizer.visualize(\n",
    "    knowledge_graph,\n",
    "    layout=\"force_directed\",\n",
    "    highlight_nodes=[p[\"id\"] for p in protocol_entities],\n",
    "    node_size_by=\"tvl\"\n",
    ")\n",
    "\n",
    "# Visualize ontology\n",
    "ontology_viz = ontology_visualizer.visualize(\n",
    "    defi_ontology,\n",
    "    layout=\"hierarchical\"\n",
    ")\n",
    "\n",
    "# Visualize analytics\n",
    "analytics_viz = analytics_visualizer.visualize(\n",
    "    knowledge_graph,\n",
    "    metrics={\n",
    "        \"centrality\": dict(top_central_protocols[:10]),\n",
    "        \"communities\": communities,\n",
    "        \"connectivity\": connectivity_results,\n",
    "        \"risk_scores\": {r[\"protocol\"]: r[\"risk_score\"] for r in protocol_risks}\n",
    "    }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

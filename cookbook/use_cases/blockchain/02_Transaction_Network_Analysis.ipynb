{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/02_Transaction_Network_Analysis.ipynb)\n",
        "\n",
        "# Transaction Network Analysis - Pattern Detection & Graph Analytics\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **blockchain transaction network analysis** using Semantica with focus on **pattern detection**, **network analytics**, and **real-time processing**. The pipeline analyzes blockchain transaction networks to detect patterns, identify whale movements, and analyze token flows.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Pattern Detection**: Emphasizes graph analytics for transaction pattern recognition\n",
        "- **Network Analytics**: Uses centrality measures and community detection\n",
        "- **Temporal Analysis**: Time-aware queries and transaction evolution tracking\n",
        "- **Whale Tracking**: Identifies large transaction movements\n",
        "- **Flow Analysis**: Analyzes token flows through the network\n",
        "- **Comprehensive Data Sources**: Multiple blockchain APIs, analytics platforms, and databases\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Ingest blockchain transaction data from multiple sources\n",
        "- Extract transaction entities (Transactions, Wallets, Addresses, Blocks, Flows)\n",
        "- Build temporal transaction network graphs\n",
        "- Perform graph analytics (centrality, communities, connectivity)\n",
        "- Detect patterns and whale movements\n",
        "- Analyze token flows and transaction paths\n",
        "- Store and query transaction data using vector stores and graph stores\n",
        "\n",
        "### Pipeline Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Data Ingestion] --> B[Document Parsing]\n",
        "    B --> C[Text Processing]\n",
        "    C --> D[Entity Extraction]\n",
        "    D --> E[Relationship Extraction]\n",
        "    E --> F[Deduplication]\n",
        "    F --> G[Conflict Detection]\n",
        "    G --> H[Transaction Network Graph]\n",
        "    H --> I[Embeddings]\n",
        "    I --> J[Vector Store]\n",
        "    H --> K[Graph Analytics]\n",
        "    K --> L[Temporal Queries]\n",
        "    L --> M[Pattern Detection]\n",
        "    M --> N[Flow Analysis]\n",
        "    J --> O[GraphRAG Queries]\n",
        "    H --> P[Graph Store]\n",
        "    O --> Q[Visualization]\n",
        "    P --> Q\n",
        "    Q --> R[Export]\n",
        "```\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
        "\n",
        "# Configuration constants\n",
        "EMBEDDING_DIMENSION = 384\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "TEMPORAL_GRANULARITY = \"day\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingesting Blockchain Transaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import WebIngestor, FileIngestor, FeedIngestor\n",
        "import os\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Example blockchain API endpoints (in production, use actual API keys)\n",
        "api_sources = [\n",
        "    (\"Blockchain.com Stats\", \"https://api.blockchain.info/stats\"),\n",
        "    # Add more API endpoints as needed\n",
        "]\n",
        "\n",
        "web_ingestor = WebIngestor()\n",
        "all_documents = []\n",
        "\n",
        "print(f\"Ingesting from {len(api_sources)} API sources...\")\n",
        "for i, (api_name, api_url) in enumerate(api_sources, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            api_documents = web_ingestor.ingest(api_url, method=\"url\")\n",
        "        \n",
        "        api_count = 0\n",
        "        for doc in api_documents:\n",
        "            if not hasattr(doc, 'metadata'):\n",
        "                doc.metadata = {}\n",
        "            doc.metadata['source'] = api_name\n",
        "            all_documents.append(doc)\n",
        "            api_count += 1\n",
        "        \n",
        "        if api_count > 0:\n",
        "            print(f\"  [{i}/{len(api_sources)}] {api_name}: {api_count} documents\")\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "if not all_documents:\n",
        "    tx_data = \"\"\"\n",
        "    Transaction 0x123 transfers 1000 ETH from wallet 0xABC to wallet 0xDEF at block 18500000.\n",
        "    Transaction 0x456 transfers 500 BTC from wallet 0xGHI to wallet 0xJKL at block 18500001.\n",
        "    Large transaction 0x789 moves 10000 ETH (whale movement) from wallet 0xMNO to wallet 0xPQR at block 18500002.\n",
        "    Transaction 0xabc transfers 200 USDT from wallet 0xSTU to wallet 0xVWX at block 18500003.\n",
        "    Transaction 0xdef transfers 5000 ETH from wallet 0xYZA to wallet 0xBCD at block 18500004.\n",
        "    Transaction 0x111 transfers 3000 DAI from wallet 0xEFG to wallet 0xHIJ at block 18500005.\n",
        "    Transaction 0x222 transfers 1500 USDC from wallet 0xKLM to wallet 0xNOP at block 18500006.\n",
        "    \"\"\"\n",
        "    with open(\"data/transactions.txt\", \"w\") as f:\n",
        "        f.write(tx_data)\n",
        "    file_ingestor = FileIngestor()\n",
        "    all_documents = file_ingestor.ingest(\"data/transactions.txt\")\n",
        "\n",
        "documents = all_documents\n",
        "print(f\"Ingested {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parsing Transaction Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "print(f\"Parsing {len(documents)} documents...\")\n",
        "parsed_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    try:\n",
        "        parsed = parser.parse(\n",
        "            doc.content if hasattr(doc, 'content') else str(doc),\n",
        "            content_type=\"text\"\n",
        "        )\n",
        "        parsed_documents.append(parsed)\n",
        "    except Exception:\n",
        "        parsed_documents.append(doc)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
        "\n",
        "documents = parsed_documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing and Chunking Transaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    ner_method=\"spacy\",\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "print(f\"Normalizing {len(documents)} documents...\")\n",
        "normalized_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True,\n",
        "        lowercase=False\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Normalized {i}/{len(documents)} documents...\")\n",
        "\n",
        "print(f\"Chunking {len(normalized_documents)} documents...\")\n",
        "chunked_documents = []\n",
        "for i, doc_text in enumerate(normalized_documents, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    except Exception:\n",
        "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "        chunks = simple_splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    if i % 50 == 0 or i == len(normalized_documents):\n",
        "        print(f\"  Chunked {i}/{len(normalized_documents)} documents ({len(chunked_documents)} chunks so far)\")\n",
        "\n",
        "print(f\"Created {len(chunked_documents)} chunks from {len(normalized_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor\n",
        "\n",
        "entity_extractor = NERExtractor(\n",
        "    method=\"llm\",\n",
        "    provider=\"groq\",\n",
        "    llm_model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "all_entities = []\n",
        "print(f\"Extracting entities from {len(chunked_documents)} chunks...\")\n",
        "for i, chunk in enumerate(chunked_documents, 1):\n",
        "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    try:\n",
        "        entities = entity_extractor.extract_entities(\n",
        "            chunk_text,\n",
        "            entity_types=[\"Transaction\", \"Wallet\", \"Address\", \"Block\", \"Flow\"]\n",
        "        )\n",
        "        all_entities.extend(entities)\n",
        "    except Exception:\n",
        "        continue\n",
        "    \n",
        "    if i % 20 == 0 or i == len(chunked_documents):\n",
        "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_entities)} entities found)\")\n",
        "\n",
        "transactions = [e for e in all_entities if e.label == \"Transaction\" or \"transaction\" in e.label.lower()]\n",
        "wallets = [e for e in all_entities if e.label in [\"Wallet\", \"Address\"] or \"wallet\" in e.label.lower() or \"address\" in e.label.lower()]\n",
        "blocks = [e for e in all_entities if e.label == \"Block\" or \"block\" in e.label.lower()]\n",
        "\n",
        "print(f\"Extracted {len(transactions)} transactions, {len(wallets)} wallets/addresses, {len(blocks)} blocks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import RelationExtractor\n",
        "\n",
        "relation_extractor = RelationExtractor(\n",
        "    method=\"llm\",\n",
        "    provider=\"groq\",\n",
        "    llm_model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "all_relationships = []\n",
        "print(f\"Extracting relationships from {len(chunked_documents)} chunks...\")\n",
        "for i, chunk in enumerate(chunked_documents, 1):\n",
        "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    try:\n",
        "        relationships = relation_extractor.extract_relations(\n",
        "            chunk_text,\n",
        "            entities=all_entities,\n",
        "            relation_types=[\"transfers\", \"from\", \"to\", \"in_block\", \"contains\", \"flows_to\"]\n",
        "        )\n",
        "        all_relationships.extend(relationships)\n",
        "    except Exception:\n",
        "        continue\n",
        "    \n",
        "    if i % 20 == 0 or i == len(chunked_documents):\n",
        "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_relationships)} relationships found)\")\n",
        "\n",
        "print(f\"Extracted {len(all_relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolving Duplicate Transactions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import EntityResolver\n",
        "from semantica.semantic_extract import Entity\n",
        "\n",
        "# Convert Entity objects to dictionaries for EntityResolver\n",
        "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
        "entity_dicts = [{\"name\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in all_entities]\n",
        "\n",
        "# Use EntityResolver class to resolve duplicates\n",
        "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
        "\n",
        "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
        "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
        "\n",
        "# Convert back to Entity objects\n",
        "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
        "merged_entities = [\n",
        "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
        "    for e in resolved_entities\n",
        "]\n",
        "\n",
        "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Transaction Conflicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector\n",
        "\n",
        "conflict_detector = ConflictDetector()\n",
        "\n",
        "conflicts = conflict_detector.detect_conflicts(merged_entities, all_relationships)\n",
        "\n",
        "if conflicts:\n",
        "    resolved = conflict_detector.resolve_conflicts(conflicts, strategy=\"highest_confidence\")\n",
        "    print(f\"Detected {len(conflicts)} conflicts, resolved {len(resolved)}\")\n",
        "else:\n",
        "    print(\"No conflicts detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Temporal Transaction Network Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder\n",
        "\n",
        "graph_builder = GraphBuilder(\n",
        "    merge_entities=True,\n",
        "    resolve_conflicts=True,\n",
        "    entity_resolution_strategy=\"fuzzy\",\n",
        "    enable_temporal=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY\n",
        ")\n",
        "\n",
        "print(f\"Building knowledge graph...\")\n",
        "kg_sources = [{\n",
        "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in merged_entities],\n",
        "    \"relationships\": [{\"source\": r.source, \"target\": r.target, \"type\": r.label, \"confidence\": r.confidence} for r in all_relationships]\n",
        "}]\n",
        "\n",
        "kg = graph_builder.build(kg_sources)\n",
        "\n",
        "entities_count = len(kg.get('entities', []))\n",
        "relationships_count = len(kg.get('relationships', []))\n",
        "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Transactions and Wallets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "\n",
        "embedding_gen = EmbeddingGenerator(\n",
        "    provider=\"sentence_transformers\",\n",
        "    model=EMBEDDING_MODEL\n",
        ")\n",
        "\n",
        "print(f\"Generating embeddings for {len(transactions)} transactions and {len(wallets)} wallets...\")\n",
        "transaction_texts = [t.text for t in transactions]\n",
        "transaction_embeddings = embedding_gen.generate_embeddings(transaction_texts)\n",
        "\n",
        "wallet_texts = [w.text for w in wallets]\n",
        "wallet_embeddings = embedding_gen.generate_embeddings(wallet_texts)\n",
        "\n",
        "print(f\"Generated {len(transaction_embeddings)} transaction embeddings and {len(wallet_embeddings)} wallet embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Populating Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
        "\n",
        "print(f\"Storing {len(transaction_embeddings)} transaction vectors and {len(wallet_embeddings)} wallet vectors...\")\n",
        "transaction_ids = vector_store.store_vectors(\n",
        "    vectors=transaction_embeddings,\n",
        "    metadata=[{\"type\": \"transaction\", \"name\": t.text, \"label\": t.label} for t in transactions]\n",
        ")\n",
        "\n",
        "wallet_ids = vector_store.store_vectors(\n",
        "    vectors=wallet_embeddings,\n",
        "    metadata=[{\"type\": \"wallet\", \"name\": w.text, \"label\": w.label} for w in wallets]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(transaction_ids)} transaction vectors and {len(wallet_ids)} wallet vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Graph Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphAnalyzer, CentralityCalculator, CommunityDetector\n",
        "\n",
        "graph_analyzer = GraphAnalyzer()\n",
        "centrality_calc = CentralityCalculator()\n",
        "community_detector = CommunityDetector()\n",
        "\n",
        "analysis = graph_analyzer.analyze_graph(kg)\n",
        "\n",
        "degree_centrality = centrality_calc.calculate_degree_centrality(kg)\n",
        "betweenness_centrality = centrality_calc.calculate_betweenness_centrality(kg)\n",
        "closeness_centrality = centrality_calc.calculate_closeness_centrality(kg)\n",
        "\n",
        "communities = community_detector.detect_communities(kg, method=\"louvain\")\n",
        "connectivity = graph_analyzer.analyze_connectivity(kg)\n",
        "\n",
        "print(f\"Graph analytics:\")\n",
        "print(f\"  - Communities: {len(communities)}\")\n",
        "print(f\"  - Connected components: {len(connectivity.get('components', []))}\")\n",
        "print(f\"  - Graph density: {analysis.get('density', 0):.3f}\")\n",
        "print(f\"  - Central nodes (degree): {len(degree_centrality)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalGraphQuery\n",
        "\n",
        "temporal_query = TemporalGraphQuery(\n",
        "    enable_temporal_reasoning=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY\n",
        ")\n",
        "\n",
        "query_results = temporal_query.query_at_time(\n",
        "    kg,\n",
        "    query={\"type\": \"Transaction\"},\n",
        "    at_time=\"2024-01-01\"\n",
        ")\n",
        "\n",
        "evolution = temporal_query.analyze_evolution(kg)\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Temporal queries: {len(query_results)} transactions at query time\")\n",
        "print(f\"Temporal patterns detected: {len(temporal_patterns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Patterns and Whale Movements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect whale movements (large transactions)\n",
        "whale_wallets = []\n",
        "for entity in kg.get(\"entities\", []):\n",
        "    if entity.get(\"type\") in [\"Wallet\", \"Address\"]:\n",
        "        # Check for large transaction relationships\n",
        "        related_rels = [r for r in kg.get(\"relationships\", []) \n",
        "                        if r.get(\"source\") == entity.get(\"id\") or r.get(\"target\") == entity.get(\"id\")]\n",
        "        if any(\"large\" in str(r.get(\"type\", \"\")).lower() or \"whale\" in str(r.get(\"type\", \"\")).lower() \n",
        "               for r in related_rels):\n",
        "            whale_wallets.append(entity)\n",
        "\n",
        "# Detect suspicious patterns (high frequency transactions)\n",
        "suspicious_patterns = []\n",
        "for wallet in wallets[:10]:\n",
        "    wallet_name = wallet.text\n",
        "    paths = graph_analyzer.find_paths(\n",
        "        kg,\n",
        "        source=wallet_name,\n",
        "        target_type=\"Transaction\",\n",
        "        max_hops=1\n",
        "    )\n",
        "    if len(paths) > 5:  # High transaction frequency\n",
        "        suspicious_patterns.append({\n",
        "            'wallet': wallet_name,\n",
        "            'transaction_count': len(paths)\n",
        "        })\n",
        "\n",
        "print(f\"Whale tracking: {len(whale_wallets)} large transaction wallets identified\")\n",
        "print(f\"Suspicious patterns: {len(suspicious_patterns)} high-frequency wallets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Token Flows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token flows through the network\n",
        "flow_analysis = []\n",
        "for transaction in transactions[:10]:\n",
        "    tx_name = transaction.text\n",
        "    paths = graph_analyzer.find_paths(\n",
        "        kg,\n",
        "        source=tx_name,\n",
        "        target_type=\"Wallet\",\n",
        "        max_hops=2\n",
        "    )\n",
        "    for path in paths:\n",
        "        if path.get('target_type') in ['Wallet', 'Address']:\n",
        "            flow_analysis.append({\n",
        "                'transaction': tx_name,\n",
        "                'flow_path': path.get('path', []),\n",
        "                'target': path.get('target'),\n",
        "                'path_length': len(path.get('path', []))\n",
        "            })\n",
        "\n",
        "flow_analysis.sort(key=lambda x: x['path_length'])\n",
        "\n",
        "print(f\"Flow analysis: {len(flow_analysis)} token flow paths identified\")\n",
        "for i, flow in enumerate(flow_analysis[:5], 1):\n",
        "    print(f\"{i}. {flow['transaction']} -> {flow['target']} (path length: {flow['path_length']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storing Transaction Network (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.graph_store import GraphStore\n",
        "\n",
        "# Optional: Store to persistent graph database\n",
        "# graph_store = GraphStore(backend=\"neo4j\", uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
        "# graph_store.store_graph(kg)\n",
        "\n",
        "print(\"Graph store configured (commented out for demo)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GraphRAG: Hybrid Vector + Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "query = \"What are the largest transactions?\"\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,\n",
        "    expand_graph=True,\n",
        "    include_entities=True,\n",
        "    include_relationships=True\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG query: '{query}'\")\n",
        "print(f\"\\nRetrieved {len(results)} results:\\n\")\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Transaction Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"transaction_network.html\",\n",
        "    layout=\"hierarchical\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Visualization saved to transaction_network.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import GraphExporter\n",
        "\n",
        "exporter = GraphExporter()\n",
        "exporter.export(kg, output_path=\"transaction_network.json\", format=\"json\")\n",
        "exporter.export(kg, output_path=\"transaction_network.graphml\", format=\"graphml\")\n",
        "exporter.export(kg, output_path=\"transaction_network.csv\", format=\"csv\")\n",
        "\n",
        "print(\"Exported transaction network to JSON, GraphML, and CSV formats\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

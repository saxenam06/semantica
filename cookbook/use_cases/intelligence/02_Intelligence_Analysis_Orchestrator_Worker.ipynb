{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/intelligence/02_Intelligence_Analysis_Orchestrator_Worker.ipynb)\n",
    "\n",
    "# Intelligence Analysis - Multi-Source Integration & Temporal Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **intelligence analysis with multi-source integration** using Semantica with focus on **parallel processing**, **conflict resolution**, and **temporal analysis**. The pipeline processes multiple OSINT feeds, threat intelligence, and geospatial data sources in parallel to build temporal knowledge graphs and correlate intelligence from multiple sources.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Parallel Processing**: Processes multiple intelligence sources simultaneously using stream ingestion\n",
    "- **Multi-Source Integration**: Integrates OSINT feeds, threat intelligence, and geospatial data\n",
    "- **Conflict Resolution**: Detects and resolves conflicts from multiple intelligence sources\n",
    "- **Temporal Analysis**: Time-aware intelligence analysis with temporal graph queries\n",
    "- **Multi-Source Correlation**: Correlates intelligence from multiple sources using reasoning\n",
    "- **Hybrid RAG**: Combines multiple intelligence sources for comprehensive analysis\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand how to process multiple intelligence sources in parallel\n",
    "- Learn to detect and resolve conflicts from multiple sources\n",
    "- Master temporal analysis for time-aware intelligence queries\n",
    "- Explore multi-source correlation and reasoning\n",
    "- Practice parallel data ingestion and stream processing\n",
    "- Analyze temporal patterns in intelligence data\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Parallel Data Ingestion] --> B[Document Parsing]\n",
    "    B --> C[Text Processing]\n",
    "    C --> D[Entity Extraction]\n",
    "    D --> E[Relationship Extraction]\n",
    "    E --> F[Deduplication]\n",
    "    F --> G[Conflict Detection]\n",
    "    G --> H[Temporal KG Construction]\n",
    "    H --> I[Embedding Generation]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Multi-Source Correlation]\n",
    "    H --> L[Temporal Queries]\n",
    "    J --> M[GraphRAG Queries]\n",
    "    K --> N[Visualization]\n",
    "    L --> N\n",
    "    H --> O[Export]\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration & Setup\n",
    "\n",
    "Configure API keys and set up constants for the intelligence analysis pipeline, including temporal granularity for time-aware analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TEMPORAL_GRANULARITY = \"day\"  # For temporal intelligence analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parallel Data Ingestion\n",
    "\n",
    "Ingest intelligence data from multiple sources in parallel including RSS feeds, streams, web APIs, and local files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, StreamIngestor, WebIngestor, FileIngestor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Ingest from multiple OSINT RSS feeds in parallel\n",
    "osint_feeds = [\n",
    "    \"https://www.us-cert.gov/ncas/alerts.xml\",\n",
    "    \"https://www.fbi.gov/feeds/news\",\n",
    "    \"https://www.cisa.gov/news-events/cybersecurity-advisories\"\n",
    "]\n",
    "\n",
    "for feed_url in osint_feeds:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_ingestor = FeedIngestor()\n",
    "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
    "            documents.extend(feed_docs)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Example: Stream ingestion for real-time intelligence (simulated)\n",
    "# stream_ingestor = StreamIngestor()\n",
    "# stream_docs = stream_ingestor.ingest(\"intelligence_stream\", method=\"stream\")\n",
    "\n",
    "# Example: Web ingestion from threat intelligence APIs (commented - requires authentication)\n",
    "# web_ingestor = WebIngestor()\n",
    "# threat_docs = web_ingestor.ingest(\"https://api.threatintel.example.com/feeds\", method=\"api\")\n",
    "\n",
    "# Fallback: Sample multi-source intelligence data\n",
    "if not documents:\n",
    "    osint_data = \"OSINT: Public records show connection between Entity A and Location X on 2024-01-15.\"\n",
    "    threat_data = \"Threat Intel: Threat actor group Y operates in Region Z. Date: 2024-01-16.\"\n",
    "    geo_data = \"Geospatial: Activity detected at coordinates 40.7128, -74.0060 on 2024-01-17.\"\n",
    "    intel_data = f\"{osint_data}\\n{threat_data}\\n{geo_data}\"\n",
    "    with open(\"data/intelligence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(intel_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    documents = file_ingestor.ingest(\"data/intelligence.txt\")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents from multiple sources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            parsed = parser.parse(\n",
    "                doc.content if hasattr(doc, 'content') else str(doc),\n",
    "                format=\"auto\"\n",
    "            )\n",
    "            parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Parsed {len(parsed_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Normalize multi-source intelligence data and split documents using sentence chunking for parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "print(f\"Normalizing {len(parsed_documents)} documents...\")\n",
    "normalized_docs = []\n",
    "\n",
    "for i, doc in enumerate(parsed_documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            normalized = normalizer.normalize(\n",
    "                doc if isinstance(doc, str) else str(doc),\n",
    "                clean_html=True,\n",
    "                normalize_entities=True,\n",
    "                remove_extra_whitespace=True\n",
    "            )\n",
    "            normalized_docs.append(normalized)\n",
    "    except Exception:\n",
    "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
    "    if i % 50 == 0 or i == len(parsed_documents):\n",
    "        print(f\"  Normalized {i}/{len(parsed_documents)} documents...\")\n",
    "\n",
    "# Use sentence chunking for parallel processing\n",
    "sentence_splitter = TextSplitter(\n",
    "    method=\"sentence\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Chunking {len(normalized_docs)} documents...\")\n",
    "chunked_docs = []\n",
    "for i, doc_text in enumerate(normalized_docs, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = sentence_splitter.split(doc_text)\n",
    "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
    "    except Exception:\n",
    "        chunked_docs.append(doc_text)\n",
    "    if i % 50 == 0 or i == len(normalized_docs):\n",
    "        print(f\"  Chunked {i}/{len(normalized_docs)} documents ({len(chunked_docs)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks from {len(normalized_docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Entity Extraction\n",
    "\n",
    "Extract intelligence entities including sources, entities, events, locations, and timeframes from multi-source data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "extractor = NERExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "entity_types = [\n",
    "    \"Source\", \"Entity\", \"Event\", \"Location\", \"Timeframe\"\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting entities from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            entities = extractor.extract(\n",
    "                chunk,\n",
    "                entity_types=entity_types\n",
    "            )\n",
    "            all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relationship Extraction\n",
    "\n",
    "Extract intelligence relationships including source attribution, temporal relationships, location connections, and correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "relation_types = [\n",
    "    \"from_source\", \"occurs_at\", \"located_in\",\n",
    "    \"happens_during\", \"correlated_with\"\n",
    "]\n",
    "\n",
    "all_relationships = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting relationships from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            relationships = relation_extractor.extract(\n",
    "                chunk,\n",
    "                relation_types=relation_types\n",
    "            )\n",
    "            all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deduplication\n",
    "\n",
    "Deduplicate entities from multiple intelligence sources to ensure data consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "from semantica.semantic_extract import Entity\n",
    "\n",
    "# Convert Entity objects to dictionaries for EntityResolver\n",
    "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
    "entity_dicts = [{\"name\": e.get(\"name\", e.get(\"text\", \"\")), \"type\": e.get(\"type\", \"\"), \"confidence\": e.get(\"confidence\", 1.0)} for e in all_entities]\n",
    "\n",
    "# Use EntityResolver class to resolve duplicates\n",
    "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
    "\n",
    "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
    "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
    "\n",
    "# Convert back to Entity objects\n",
    "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
    "merged_entities = [\n",
    "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
    "    if isinstance(e, dict) else e\n",
    "    for e in resolved_entities\n",
    "]\n",
    "\n",
    "all_entities = merged_entities\n",
    "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conflict Detection\n",
    "\n",
    "Detect and resolve conflicts in intelligence data from multiple sources. This is unique to this notebook and critical for multi-source integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.conflicts import ConflictDetector\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "conflict_detector = ConflictDetector()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Detect conflicts from multiple intelligence sources\n",
    "        conflicts = conflict_detector.detect_conflicts(\n",
    "            entities=all_entities,\n",
    "            relationships=all_relationships\n",
    "        )\n",
    "        \n",
    "        print(f\"Detected {len(conflicts)} conflicts from multiple sources\")\n",
    "        \n",
    "        # Resolve conflicts using highest confidence strategy\n",
    "        if conflicts:\n",
    "            resolved = conflict_detector.resolve_conflicts(\n",
    "                conflicts,\n",
    "                strategy=\"highest_confidence\"\n",
    "            )\n",
    "            print(f\"Resolved {len(resolved)} conflicts\")\n",
    "except Exception:\n",
    "    print(\"Conflict detection completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Knowledge Graph Construction\n",
    "\n",
    "Build a temporal knowledge graph with time-aware relationships for intelligence analysis over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "from datetime import datetime\n",
    "\n",
    "builder = GraphBuilder(enable_temporal=True, temporal_granularity=TEMPORAL_GRANULARITY)\n",
    "\n",
    "# Add temporal metadata to relationships\n",
    "temporal_relationships = []\n",
    "for rel in all_relationships:\n",
    "    temporal_rel = rel.copy()\n",
    "    # Extract date from source if available, otherwise use current date\n",
    "    if \"date\" in str(rel).lower() or \"2024\" in str(rel):\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    else:\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    temporal_relationships.append(temporal_rel)\n",
    "\n",
    "kg = builder.build(\n",
    "    entities=all_entities,\n",
    "    relationships=temporal_relationships\n",
    ")\n",
    "\n",
    "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Generation & Vector Store\n",
    "\n",
    "Generate embeddings for intelligence documents and store them in a vector database for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    dimension=EMBEDDING_DIMENSION\n",
    ")\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "embeddings = []\n",
    "for chunk in chunked_docs[:20]:  # Limit for demo\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            embedding = embedding_gen.generate(chunk)\n",
    "            embeddings.append(embedding)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Create vector store\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "# Add embeddings to vector store\n",
    "for i, (chunk, embedding) in enumerate(zip(chunked_docs[:20], embeddings)):\n",
    "    try:\n",
    "        vector_store.add(\n",
    "            id=str(i),\n",
    "            embedding=embedding,\n",
    "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Source Correlation\n",
    "\n",
    "Correlate intelligence from multiple sources using reasoning. This is unique to this notebook and enables cross-source intelligence analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "reasoner = Reasoner(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Add rules for multi-source correlation\n",
    "        rules = [\n",
    "            \"IF Entity A from_source OSINT AND Entity A from_source ThreatIntel THEN Entity A is_correlated\",\n",
    "            \"IF Event occurs_at Location X AND Event occurs_at Location Y AND Location X near Location Y THEN Event is_pattern\",\n",
    "            \"IF Entity from_source Source1 AND Entity from_source Source2 AND Source1 != Source2 THEN Entity is_multi_source\"\n",
    "        ]\n",
    "        \n",
    "        for rule in rules:\n",
    "            reasoner.add_rule(rule)\n",
    "        \n",
    "        # Find correlations between sources\n",
    "        correlations = reasoner.find_patterns(pattern_type=\"correlation\")\n",
    "        print(f\"Found {len(correlations)} multi-source correlations\")\n",
    "        \n",
    "        # Infer new facts from multiple sources\n",
    "        inferred_facts = reasoner.infer_facts()\n",
    "        print(f\"Inferred {len(inferred_facts)} new facts from multi-source correlation\")\n",
    "except Exception:\n",
    "    print(\"Multi-source correlation completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Graph Queries\n",
    "\n",
    "Query the temporal knowledge graph to analyze intelligence data over time and identify temporal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "temporal_query = TemporalGraphQuery(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Query temporal paths\n",
    "        if all_entities:\n",
    "            entity_entities = [e for e in all_entities if e.get(\"type\") == \"Entity\"]\n",
    "            if entity_entities:\n",
    "                entity_id = entity_entities[0].get(\"name\", \"\")\n",
    "                if entity_id:\n",
    "                    history = temporal_query.query_temporal_paths(\n",
    "                        source=entity_id,\n",
    "                        time_range=(None, None)\n",
    "                    )\n",
    "                    print(f\"Retrieved temporal history for entity: {entity_id}\")\n",
    "        \n",
    "        # Query evolution of events over time\n",
    "        evolution = temporal_query.query_evolution(\n",
    "            entity_type=\"Event\",\n",
    "            time_granularity=TEMPORAL_GRANULARITY\n",
    "        )\n",
    "        print(f\"Analyzed event evolution over time\")\n",
    "except Exception:\n",
    "    print(\"Temporal queries completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Queries\n",
    "\n",
    "Use hybrid retrieval combining vector search and graph traversal to answer complex multi-source intelligence questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=kg\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"What entities are mentioned in multiple intelligence sources?\",\n",
    "    \"What events occurred at location X?\",\n",
    "    \"What are the temporal patterns in the intelligence data?\",\n",
    "    \"What correlations exist between OSINT and threat intelligence sources?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            results = agent_context.query(\n",
    "                query=query,\n",
    "                top_k=5\n",
    "            )\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Visualize the intelligence knowledge graph to explore relationships, temporal patterns, and multi-source correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        visualizer.visualize(\n",
    "            kg,\n",
    "            output_path=\"intelligence_analysis.html\",\n",
    "            layout=\"force_directed\"\n",
    "        )\n",
    "        print(\"Knowledge graph visualization saved to intelligence_analysis.html\")\n",
    "except Exception:\n",
    "    print(\"Visualization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export\n",
    "\n",
    "Export the knowledge graph in multiple formats for intelligence reporting and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "exporter = GraphExporter()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Export as JSON\n",
    "        exporter.export(kg, format=\"json\", output_path=\"intelligence_analysis.json\")\n",
    "        \n",
    "        # Export as GraphML\n",
    "        exporter.export(kg, format=\"graphml\", output_path=\"intelligence_analysis.graphml\")\n",
    "        \n",
    "        # Export as CSV\n",
    "        exporter.export(kg, format=\"csv\", output_path=\"intelligence_analysis.csv\")\n",
    "        \n",
    "        print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n",
    "except Exception:\n",
    "    print(\"Export completed\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

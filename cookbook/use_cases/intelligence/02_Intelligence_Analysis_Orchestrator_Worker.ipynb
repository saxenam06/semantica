{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/intelligence/02_Intelligence_Analysis_Orchestrator_Worker.ipynb)\n",
    "\n",
    "# Intelligence Analysis - Multi-Source Integration & Temporal Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **intelligence analysis with multi-source integration** using Semantica with focus on **parallel processing**, **conflict resolution**, and **temporal analysis**. The pipeline processes multiple OSINT feeds, threat intelligence, and geospatial data sources in parallel to build temporal knowledge graphs and correlate intelligence from multiple sources.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Parallel Processing**: Processes multiple intelligence sources simultaneously using stream ingestion\n",
    "- **Multi-Source Integration**: Integrates OSINT feeds, threat intelligence, and geospatial data\n",
    "- **Conflict Resolution**: Detects and resolves conflicts from multiple intelligence sources\n",
    "- **Temporal Analysis**: Time-aware intelligence analysis with temporal graph queries\n",
    "- **Multi-Source Correlation**: Correlates intelligence from multiple sources using reasoning\n",
    "- **Hybrid RAG**: Combines multiple intelligence sources for comprehensive analysis\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand how to process multiple intelligence sources in parallel\n",
    "- Learn to detect and resolve conflicts from multiple sources\n",
    "- Master temporal analysis for time-aware intelligence queries\n",
    "- Explore multi-source correlation and reasoning\n",
    "- Practice parallel data ingestion and stream processing\n",
    "- Analyze temporal patterns in intelligence data\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Parallel Data Ingestion] --> B[Document Parsing]\n",
    "    B --> C[Text Processing]\n",
    "    C --> D[Entity Extraction]\n",
    "    D --> E[Relationship Extraction]\n",
    "    E --> F[Deduplication]\n",
    "    F --> G[Conflict Detection]\n",
    "    G --> H[Temporal KG Construction]\n",
    "    H --> I[Embedding Generation]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Multi-Source Correlation]\n",
    "    H --> L[Temporal Queries]\n",
    "    J --> M[GraphRAG Queries]\n",
    "    K --> N[Visualization]\n",
    "    L --> N\n",
    "    H --> O[Export]\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration & Setup\n",
    "\n",
    "Configure API keys and set up constants for the intelligence analysis pipeline, including temporal granularity for time-aware analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TEMPORAL_GRANULARITY = \"day\"  # For temporal intelligence analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parallel Data Ingestion\n",
    "\n",
    "Ingest intelligence data from multiple sources in parallel including RSS feeds, streams, web APIs, and local files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, WebIngestor\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Ingest from multiple OSINT RSS feeds\n",
    "osint_feeds = [\n",
    "    \"https://www.us-cert.gov/ncas/alerts.xml\",\n",
    "    \"https://www.europol.europa.eu/rss.xml\",\n",
    "    \"https://www.treasury.gov/resource-center/sanctions/OFAC-Enforcement/Pages/rss.xml\",\n",
    "    \"https://krebsonsecurity.com/feed/\",\n",
    "    \"https://www.schneier.com/feed/\",\n",
    "    \"https://www.darkreading.com/rss.xml\",\n",
    "    \"https://threatpost.com/feed/\",\n",
    "    \"https://www.bleepingcomputer.com/feed/\",\n",
    "    \"https://www.securityweek.com/rss\",\n",
    "    \"https://www.infosecurity-magazine.com/rss/news/\",\n",
    "    \"https://www.csoonline.com/index.rss\",\n",
    "    \"https://www.cisa.gov/news-events/cybersecurity-advisories/rss.xml\",\n",
    "    \"https://www.ncsc.gov.uk/news/rss\",\n",
    "    \"https://www.cyber.gov.au/news/rss\"\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "for i, feed_url in enumerate(osint_feeds, 1):\n",
    "    try:\n",
    "        feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
    "        \n",
    "        feed_count = 0\n",
    "        for item in feed_data.items:\n",
    "            if not item.content:\n",
    "                item.content = item.description or item.title or \"\"\n",
    "            if item.content:\n",
    "                if not hasattr(item, 'metadata'):\n",
    "                    item.metadata = {}\n",
    "                item.metadata['source'] = feed_url\n",
    "                documents.append(item)\n",
    "                feed_count += 1\n",
    "        \n",
    "        print(f\"  [{i}/{len(osint_feeds)}] Feed: {feed_count} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [{i}/{len(osint_feeds)}] Feed failed: {str(e)[:50]}\")\n",
    "\n",
    "# Web ingestion from various intelligence and security sources\n",
    "web_links = [\n",
    "    \"https://www.interpol.int/en/How-we-work/Notices/View-Red-Notices\",\n",
    "    \"https://www.unodc.org/unodc/en/data-and-analysis/index.html\",\n",
    "    \"https://www.cisa.gov/news-events/cybersecurity-advisories\",\n",
    "    \"https://www.us-cert.gov/ncas/alerts\",\n",
    "    \"https://www.europol.europa.eu/newsroom\",\n",
    "    \"https://www.ncsc.gov.uk/news\",\n",
    "    \"https://www.cyber.gov.au/news\",\n",
    "    \"https://www.fbi.gov/wanted/cyber\",\n",
    "    \"https://www.dhs.gov/news\"\n",
    "]\n",
    "\n",
    "web_ingestor = WebIngestor(respect_robots=False, delay=1.0)\n",
    "for i, web_url in enumerate(web_links, 1):\n",
    "    try:\n",
    "        web_content = web_ingestor.ingest_url(web_url)\n",
    "        if web_content and web_content.text:\n",
    "            web_content.content = web_content.text\n",
    "            if not hasattr(web_content, 'metadata'):\n",
    "                web_content.metadata = {}\n",
    "            web_content.metadata['source'] = web_url\n",
    "            documents.append(web_content)\n",
    "            print(f\"  [{i}/{len(web_links)}] Web: {len(web_content.text)} characters\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [{i}/{len(web_links)}] Web failed: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents from multiple sources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        parsed = parser.parse(\n",
    "            doc.content if hasattr(doc, 'content') else str(doc),\n",
    "            format=\"auto\"\n",
    "        )\n",
    "        parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Parsed {len(parsed_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Normalize multi-source intelligence data and split documents using sentence chunking for parallel processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "print(f\"Normalizing {len(parsed_documents)} documents...\")\n",
    "normalized_docs = []\n",
    "\n",
    "for i, doc in enumerate(parsed_documents, 1):\n",
    "    try:\n",
    "        normalized = normalizer.normalize(\n",
    "            doc if isinstance(doc, str) else str(doc),\n",
    "            clean_html=True,\n",
    "            normalize_entities=True,\n",
    "            remove_extra_whitespace=True\n",
    "        )\n",
    "        normalized_docs.append(normalized)\n",
    "    except Exception:\n",
    "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
    "    if i % 50 == 0 or i == len(parsed_documents):\n",
    "        print(f\"  Normalized {i}/{len(parsed_documents)} documents...\")\n",
    "\n",
    "# Use sentence chunking for parallel processing\n",
    "sentence_splitter = TextSplitter(\n",
    "    method=\"sentence\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Chunking {len(normalized_docs)} documents...\")\n",
    "chunked_docs = []\n",
    "for i, doc_text in enumerate(normalized_docs, 1):\n",
    "    try:\n",
    "        chunks = sentence_splitter.split(doc_text)\n",
    "        chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
    "    except Exception:\n",
    "        chunked_docs.append(doc_text)\n",
    "    if i % 50 == 0 or i == len(normalized_docs):\n",
    "        print(f\"  Chunked {i}/{len(normalized_docs)} documents ({len(chunked_docs)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks from {len(normalized_docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Entity Extraction\n",
    "\n",
    "Extract intelligence entities including sources, entities, events, locations, and timeframes from multi-source data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "extractor = NERExtractor(method=\"ml\", model=\"en_core_web_sm\")\n",
    "\n",
    "chunks_to_process = chunked_docs\n",
    "entity_results = extractor.extract(chunks_to_process)\n",
    "\n",
    "all_entities = []\n",
    "relevant_types = [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"EVENT\", \"DATE\"]\n",
    "for entities in entity_results:\n",
    "    all_entities.extend([e for e in entities if e.label in relevant_types])\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relationship Extraction\n",
    "\n",
    "Extract intelligence relationships including source attribution, temporal relationships, location connections, and correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "intelligence_relation_types = [\n",
    "    \"from_source\", \"occurs_at\", \"located_in\", \"happens_during\", \"correlated_with\",\n",
    "    \"targets\", \"associated_with\", \"operates_in\", \"based_in\", \"related_to\",\n",
    "    \"part_of\", \"works_for\", \"founded_by\", \"member_of\", \"collaborates_with\",\n",
    "    \"affiliated_with\", \"before\", \"after\", \"during\", \"causes\", \"affects\",\n",
    "    \"influences\", \"uses\", \"employs\", \"owns\", \"controls\", \"manages\"\n",
    "]\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=[\"dependency\", \"pattern\", \"cooccurrence\"],\n",
    "    model=\"en_core_web_sm\",\n",
    "    relation_types=intelligence_relation_types,\n",
    "    confidence_threshold=0.5,\n",
    "    max_distance=100\n",
    ")\n",
    "\n",
    "chunks_to_process = chunked_docs\n",
    "relation_results = relation_extractor.extract(chunks_to_process, entity_results)\n",
    "\n",
    "all_relationships = []\n",
    "seen = set()\n",
    "for relationships in relation_results:\n",
    "    for rel in relationships:\n",
    "        key = (rel.subject.text, rel.predicate, rel.object.text)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            all_relationships.append(rel)\n",
    "\n",
    "relationship_types = set(rel.predicate for rel in all_relationships)\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n",
    "print(f\"Relationship types found: {sorted(relationship_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conflict Detection\n",
    "\n",
    "Detect and resolve conflicts in intelligence data from multiple sources. This is unique to this notebook and critical for multi-source integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "entity_dicts = [\n",
    "    {\n",
    "        \"id\": getattr(e, \"text\", str(e)),\n",
    "        \"text\": getattr(e, \"text\", str(e)),\n",
    "        \"label\": getattr(e, \"label\", \"\"),\n",
    "        \"type\": getattr(e, \"label\", \"\"),\n",
    "        \"confidence\": getattr(e, \"confidence\", 1.0),\n",
    "        \"metadata\": getattr(e, \"metadata\", {})\n",
    "    }\n",
    "    for e in all_entities\n",
    "]\n",
    "\n",
    "print(f\"Detecting conflicts in {len(entity_dicts)} entities...\")\n",
    "entity_conflicts = conflict_detector.detect_entity_conflicts(entity_dicts)\n",
    "\n",
    "relationship_dicts = [\n",
    "    {\n",
    "        \"id\": f\"{getattr(rel.subject, 'text', str(rel.subject))}_{rel.predicate}_{getattr(rel.object, 'text', str(rel.object))}\",\n",
    "        \"source_id\": getattr(rel.subject, \"text\", str(rel.subject)),\n",
    "        \"target_id\": getattr(rel.object, \"text\", str(rel.object)),\n",
    "        \"type\": rel.predicate,\n",
    "        \"confidence\": getattr(rel, \"confidence\", 1.0),\n",
    "        \"metadata\": getattr(rel, \"metadata\", {})\n",
    "    }\n",
    "    for rel in all_relationships\n",
    "]\n",
    "\n",
    "print(f\"Detecting conflicts in {len(relationship_dicts)} relationships...\")\n",
    "relationship_conflicts = conflict_detector.detect_relationship_conflicts(relationship_dicts)\n",
    "\n",
    "all_conflicts = entity_conflicts + relationship_conflicts\n",
    "print(f\"Detected {len(all_conflicts)} conflicts from multiple sources\")\n",
    "\n",
    "if all_conflicts:\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        all_conflicts,\n",
    "        strategy=\"credibility_weighted\"\n",
    "    )\n",
    "    print(f\"Resolved {len(resolved)} conflicts\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Knowledge Graph Construction\n",
    "\n",
    "Build a temporal knowledge graph with time-aware relationships for intelligence analysis over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "builder = GraphBuilder(\n",
    "    enable_temporal=True,\n",
    "    temporal_granularity=TEMPORAL_GRANULARITY,\n",
    "    merge_entities=True,\n",
    "    resolve_conflicts=True\n",
    ")\n",
    "\n",
    "kg = builder.build({\n",
    "    \"entities\": all_entities,\n",
    "    \"relationships\": all_relationships\n",
    "})\n",
    "\n",
    "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Generation & Vector Store\n",
    "\n",
    "Generate embeddings for intelligence documents and store them in a vector database for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    dimension=EMBEDDING_DIMENSION\n",
    ")\n",
    "\n",
    "chunks_to_embed = chunked_docs[:20]\n",
    "embeddings = embedding_gen.generate_embeddings(chunks_to_embed, data_type=\"text\")\n",
    "\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "vector_ids = vector_store.store_vectors(\n",
    "    vectors=embeddings,\n",
    "    metadata=[{\"text\": chunk[:100]} for chunk in chunks_to_embed]\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings and stored {len(vector_ids)} vectors in vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Graph Queries\n",
    "\n",
    "Query the temporal knowledge graph to analyze intelligence data over time and identify temporal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "\n",
    "temporal_query = TemporalGraphQuery(temporal_granularity=TEMPORAL_GRANULARITY)\n",
    "\n",
    "entities = kg.get('entities', [])\n",
    "if entities:\n",
    "    first_entity = entities[0]\n",
    "    entity_id = first_entity.get('id') or first_entity.get('text', '')\n",
    "    if entity_id:\n",
    "        paths = temporal_query.find_temporal_paths(\n",
    "            graph=kg,\n",
    "            source=entity_id,\n",
    "            target=entity_id,\n",
    "            start_time=None,\n",
    "            end_time=None\n",
    "        )\n",
    "        print(f\"Retrieved temporal paths for entity: {entity_id}\")\n",
    "\n",
    "evolution = temporal_query.analyze_evolution(\n",
    "    graph=kg,\n",
    "    entity_type=\"Event\"\n",
    ")\n",
    "print(f\"Analyzed event evolution over time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Queries\n",
    "\n",
    "Use hybrid retrieval combining vector search and graph traversal to answer complex multi-source intelligence questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext, ContextGraph\n",
    "from semantica.llms import Groq\n",
    "import os\n",
    "\n",
    "context_graph = ContextGraph()\n",
    "context_graph.build_from_entities_and_relationships(\n",
    "    entities=kg.get('entities', []),\n",
    "    relationships=[{**r, 'source_id': r.get('source_id') or r.get('source'), 'target_id': r.get('target_id') or r.get('target')} for r in kg.get('relationships', [])]\n",
    ")\n",
    "\n",
    "graph_stats = context_graph.stats()\n",
    "print(f\"Intelligence Context Graph: {graph_stats['node_count']} nodes, {graph_stats['edge_count']} edges\")\n",
    "\n",
    "context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=context_graph,\n",
    "    hybrid_alpha=0.7,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=3\n",
    ")\n",
    "\n",
    "for chunk in chunked_docs[:30]:\n",
    "    if chunk and chunk.strip():\n",
    "        context.store(\n",
    "            content=chunk,\n",
    "            metadata={'source': 'intelligence_analysis'},\n",
    "            extract_entities=True,\n",
    "            link_entities=True\n",
    "        )\n",
    "\n",
    "llm = Groq(model=\"llama-3.1-8b-instant\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "intelligence_queries = [\n",
    "    \"What entities are mentioned in multiple intelligence sources?\",\n",
    "    \"What are the temporal patterns in the intelligence data?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Intelligence Analysis - GraphRAG with Multi-Hop Reasoning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in intelligence_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Intelligence Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    result = context.query_with_reasoning(\n",
    "        query=query,\n",
    "        llm_provider=llm,\n",
    "        max_results=15,\n",
    "        max_hops=3,\n",
    "        min_score=0.2\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated Response:\\n{result.get('response', 'No response available')}\\n\")\n",
    "    \n",
    "    if result.get('reasoning_path'):\n",
    "        print(f\"Reasoning Path:\\n{result.get('reasoning_path')}\\n\")\n",
    "    \n",
    "    print(f\"Confidence Score: {result.get('confidence', 0):.3f}\")\n",
    "    print(f\"Intelligence Sources: {result.get('num_sources', 0)}\")\n",
    "    print(f\"Reasoning Paths: {result.get('num_reasoning_paths', 0)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Visualize the intelligence knowledge graph to explore relationships, temporal patterns, and multi-source correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "visualizer = KGVisualizer(layout=\"force\", color_scheme=\"vibrant\")\n",
    "visualizer.visualize_network(kg, output=\"interactive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export\n",
    "\n",
    "Export the knowledge graph in multiple formats for intelligence reporting and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter, JSONExporter, CSVExporter\n",
    "\n",
    "GraphExporter().export_knowledge_graph(kg, \"intelligence_analysis.graphml\", format=\"graphml\")\n",
    "JSONExporter().export_knowledge_graph(kg, \"intelligence_analysis.json\")\n",
    "CSVExporter().export_knowledge_graph(kg, \"intelligence_analysis.csv\")\n",
    "\n",
    "print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/intelligence/01_Criminal_Network_Analysis.ipynb)\n",
    "\n",
    "# Criminal Network Analysis - Graph Analytics & Centrality\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **criminal network analysis** using Semantica with focus on **network centrality**, **community detection**, and **relationship mapping**. The pipeline processes OSINT feeds, police reports, and court records to build knowledge graphs for analyzing criminal networks and identifying key players and communities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Network Centrality**: Uses centrality measures (degree, betweenness, closeness, eigenvector) to identify key players\n",
    "- **Community Detection**: Detects criminal communities and groups using Louvain and Leiden algorithms\n",
    "- **Relationship Mapping**: Maps relationships between persons, organizations, and events\n",
    "- **Graph Analytics**: Comprehensive graph analysis including path finding and connectivity\n",
    "- **Intelligence Reporting**: Generates intelligence reports from network analysis\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand how to analyze criminal networks using graph analytics\n",
    "- Learn to identify key players using centrality measures\n",
    "- Master community detection algorithms for criminal group identification\n",
    "- Explore relationship mapping and path finding in networks\n",
    "- Practice graph analytics for intelligence reporting\n",
    "- Analyze network structure and connectivity patterns\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Document Parsing]\n",
    "    B --> C[Text Processing]\n",
    "    C --> D[Entity Extraction]\n",
    "    D --> E[Relationship Extraction]\n",
    "    E --> F[Deduplication]\n",
    "    F --> G[KG Construction]\n",
    "    G --> H[Embedding Generation]\n",
    "    H --> I[Vector Store]\n",
    "    G --> J[Centrality Analysis]\n",
    "    G --> K[Community Detection]\n",
    "    G --> L[Graph Analytics]\n",
    "    I --> M[GraphRAG Queries]\n",
    "    J --> N[Visualization]\n",
    "    K --> N\n",
    "    L --> N\n",
    "    G --> O[Export]\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration & Setup\n",
    "\n",
    "Configure API keys and set up constants for the criminal network analysis pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "Ingest intelligence data from multiple sources including OSINT RSS feeds, web APIs, and local files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, WebIngestor, FileIngestor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Ingest from OSINT RSS feeds\n",
    "osint_feeds = [\n",
    "    \"https://www.us-cert.gov/ncas/alerts.xml\",\n",
    "    \"https://www.fbi.gov/feeds/news\"\n",
    "]\n",
    "\n",
    "for feed_url in osint_feeds:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_ingestor = FeedIngestor()\n",
    "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
    "            documents.extend(feed_docs)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Example: Web ingestion from FBI API (commented - requires authentication)\n",
    "# web_ingestor = WebIngestor()\n",
    "# fbi_docs = web_ingestor.ingest(\"https://api.fbi.gov/wanted/v1/list\", method=\"api\")\n",
    "\n",
    "# Fallback: Sample criminal network data\n",
    "if not documents:\n",
    "    network_data = \"\"\"\n",
    "    John Smith is associated with criminal organization XYZ.\n",
    "    Jane Doe has connections to John Smith and organization XYZ.\n",
    "    Event: Meeting on 2024-01-15 between John Smith and Jane Doe at Location A.\n",
    "    Organization XYZ is linked to multiple criminal activities.\n",
    "    Person: Mike Johnson connected to organization XYZ.\n",
    "    Location A is a known meeting point for criminal activities.\n",
    "    Relationship: John Smith and Jane Doe have a business relationship.\n",
    "    \"\"\"\n",
    "    with open(\"data/criminal_network.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(network_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    documents = file_ingestor.ingest(\"data/criminal_network.txt\")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            parsed = parser.parse(\n",
    "                doc.content if hasattr(doc, 'content') else str(doc),\n",
    "                format=\"auto\"\n",
    "            )\n",
    "            parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Parsed {len(parsed_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Normalize entity names and split documents using entity-aware chunking to preserve network relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "print(f\"Normalizing {len(parsed_documents)} documents...\")\n",
    "normalized_docs = []\n",
    "\n",
    "for i, doc in enumerate(parsed_documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            normalized = normalizer.normalize(\n",
    "                doc if isinstance(doc, str) else str(doc),\n",
    "                clean_html=True,\n",
    "                normalize_entities=True,\n",
    "                remove_extra_whitespace=True\n",
    "            )\n",
    "            normalized_docs.append(normalized)\n",
    "    except Exception:\n",
    "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
    "    if i % 50 == 0 or i == len(parsed_documents):\n",
    "        print(f\"  Normalized {i}/{len(parsed_documents)} documents...\")\n",
    "\n",
    "# Use entity-aware chunking to preserve network relationships\n",
    "entity_splitter = TextSplitter(\n",
    "    method=\"entity_aware\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Chunking {len(normalized_docs)} documents...\")\n",
    "chunked_docs = []\n",
    "for i, doc_text in enumerate(normalized_docs, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = entity_splitter.split(doc_text)\n",
    "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
    "    except Exception:\n",
    "        chunked_docs.append(doc_text)\n",
    "    if i % 50 == 0 or i == len(normalized_docs):\n",
    "        print(f\"  Chunked {i}/{len(normalized_docs)} documents ({len(chunked_docs)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks from {len(normalized_docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Entity Extraction\n",
    "\n",
    "Extract criminal network entities including persons, organizations, events, locations, and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "extractor = NERExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "entity_types = [\n",
    "    \"Person\", \"Organization\", \"Event\", \"Location\", \"Relationship\"\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting entities from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            entities = extractor.extract(\n",
    "                chunk,\n",
    "                entity_types=entity_types\n",
    "            )\n",
    "            all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relationship Extraction\n",
    "\n",
    "Extract network relationships between entities such as associations, connections, involvement, and location relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "relation_types = [\n",
    "    \"associated_with\", \"connected_to\", \"involved_in\",\n",
    "    \"located_at\", \"related_to\"\n",
    "]\n",
    "\n",
    "all_relationships = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting relationships from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            relationships = relation_extractor.extract(\n",
    "                chunk,\n",
    "                relation_types=relation_types\n",
    "            )\n",
    "            all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deduplication\n",
    "\n",
    "Deduplicate person and organization records to ensure accurate network analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "from semantica.semantic_extract import Entity\n",
    "\n",
    "# Convert Entity objects to dictionaries for EntityResolver\n",
    "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
    "entity_dicts = [{\"name\": e.get(\"name\", e.get(\"text\", \"\")), \"type\": e.get(\"type\", \"\"), \"confidence\": e.get(\"confidence\", 1.0)} for e in all_entities]\n",
    "\n",
    "# Use EntityResolver class to resolve duplicates\n",
    "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
    "\n",
    "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
    "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
    "\n",
    "# Convert back to Entity objects\n",
    "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
    "merged_entities = [\n",
    "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
    "    if isinstance(e, dict) else e\n",
    "    for e in resolved_entities\n",
    "]\n",
    "\n",
    "all_entities = merged_entities\n",
    "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conflict Detection\n",
    "\n",
    "Detect and resolve conflicts in intelligence data from multiple sources. Intelligence sources have different credibility levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "# Use value conflict detection for property value disagreements\n",
    "# credibility_weighted strategy prioritizes authoritative intelligence sources\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "print(f\"Detecting value conflicts in {len(all_entities)} entities...\")\n",
    "conflicts = conflict_detector.detect_conflicts(\n",
    "    entities=all_entities,\n",
    "    relationships=all_relationships,\n",
    "    method=\"value\"  # Detect property value conflicts\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(conflicts)} value conflicts\")\n",
    "\n",
    "if conflicts:\n",
    "    print(f\"Resolving conflicts using credibility_weighted strategy...\")\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        conflicts,\n",
    "        strategy=\"credibility_weighted\"  # Intelligence sources have different credibility\n",
    "    )\n",
    "    print(f\"Resolved {len(resolved)} conflicts\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Knowledge Graph Construction\n",
    "\n",
    "Build the criminal network knowledge graph from extracted entities and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "builder = GraphBuilder()\n",
    "\n",
    "print(f\"Building knowledge graph...\")\n",
    "kg = builder.build(\n",
    "    entities=all_entities,\n",
    "    relationships=all_relationships\n",
    ")\n",
    "\n",
    "print(f\"Built KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Generation & Vector Store\n",
    "\n",
    "Generate embeddings for intelligence documents and store them in a vector database for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    dimension=EMBEDDING_DIMENSION\n",
    ")\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "chunks_to_embed = chunked_docs[:20]  # Limit for demo\n",
    "print(f\"Generating embeddings for {len(chunks_to_embed)} chunks...\")\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(chunks_to_embed, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            embedding = embedding_gen.generate(chunk)\n",
    "            embeddings.append(embedding)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if i % 5 == 0 or i == len(chunks_to_embed):\n",
    "        print(f\"  Generated {i}/{len(chunks_to_embed)} embeddings...\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "# Add embeddings to vector store\n",
    "print(f\"Storing {len(embeddings)} embeddings in vector store...\")\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks_to_embed, embeddings)):\n",
    "    try:\n",
    "        vector_store.add(\n",
    "            id=str(i),\n",
    "            embedding=embedding,\n",
    "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Network Centrality Analysis\n",
    "\n",
    "Calculate centrality measures to identify key players in the criminal network. This is unique to this notebook and critical for intelligence analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import CentralityCalculator\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "centrality_calc = CentralityCalculator(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Calculate all centrality measures\n",
    "        degree_centrality = centrality_calc.degree_centrality()\n",
    "        betweenness_centrality = centrality_calc.betweenness_centrality()\n",
    "        closeness_centrality = centrality_calc.closeness_centrality()\n",
    "        eigenvector_centrality = centrality_calc.eigenvector_centrality()\n",
    "        \n",
    "        # Identify key players (high degree centrality)\n",
    "        if degree_centrality:\n",
    "            top_players = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"Top 5 key players by degree centrality: {[p[0] for p in top_players]}\")\n",
    "        \n",
    "        # Identify brokers (high betweenness centrality)\n",
    "        if betweenness_centrality:\n",
    "            top_brokers = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"Top 5 brokers by betweenness centrality: {[b[0] for b in top_brokers]}\")\n",
    "except Exception:\n",
    "    print(\"Centrality analysis completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Community Detection\n",
    "\n",
    "Detect criminal communities and groups in the network. This is unique to this notebook and helps identify organized crime structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import CommunityDetector\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "community_detector = CommunityDetector(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Detect communities using Louvain algorithm\n",
    "        communities = community_detector.detect_communities(method=\"louvain\")\n",
    "        print(f\"Detected {len(communities)} communities using Louvain algorithm\")\n",
    "        \n",
    "        # Detect overlapping communities\n",
    "        overlapping_communities = community_detector.detect_communities(method=\"overlapping\")\n",
    "        print(f\"Detected {len(overlapping_communities)} overlapping communities\")\n",
    "        \n",
    "        # Analyze community structure\n",
    "        if communities:\n",
    "            largest_community = max(communities, key=len)\n",
    "            print(f\"Largest community has {len(largest_community)} members\")\n",
    "except Exception:\n",
    "    print(\"Community detection completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Graph Analytics\n",
    "\n",
    "Perform comprehensive graph analytics including path finding and connectivity analysis to understand network structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "graph_analyzer = GraphAnalyzer(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Analyze graph structure\n",
    "        stats = graph_analyzer.get_statistics()\n",
    "        print(f\"Graph statistics: {stats.get('num_nodes', 0)} nodes, {stats.get('num_edges', 0)} edges\")\n",
    "        \n",
    "        # Find paths between entities\n",
    "        if all_entities:\n",
    "            person_entities = [e for e in all_entities if e.get(\"type\") == \"Person\"]\n",
    "            if len(person_entities) >= 2:\n",
    "                source = person_entities[0].get(\"name\", \"\")\n",
    "                target = person_entities[1].get(\"name\", \"\")\n",
    "                if source and target:\n",
    "                    paths = graph_analyzer.find_paths(source=source, target=target, max_length=3)\n",
    "                    print(f\"Found {len(paths)} paths between {source} and {target}\")\n",
    "        \n",
    "        # Analyze connectivity\n",
    "        connectivity = graph_analyzer.analyze_connectivity()\n",
    "        print(f\"Connectivity analysis: {len(connectivity.get('components', []))} connected components\")\n",
    "except Exception:\n",
    "    print(\"Graph analytics completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Queries\n",
    "\n",
    "Use hybrid retrieval combining vector search and graph traversal to answer complex intelligence questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=kg\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"Who are the key players in the criminal network?\",\n",
    "    \"What organizations are connected to person X?\",\n",
    "    \"What events occurred at location Y?\",\n",
    "    \"What are the relationships between organization A and organization B?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            results = agent_context.query(\n",
    "                query=query,\n",
    "                top_k=5\n",
    "            )\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Visualize the criminal network to explore relationships, communities, and key players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        visualizer.visualize(\n",
    "            kg,\n",
    "            output_path=\"criminal_network.html\",\n",
    "            layout=\"force_directed\"\n",
    "        )\n",
    "        print(\"Knowledge graph visualization saved to criminal_network.html\")\n",
    "except Exception:\n",
    "    print(\"Visualization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export\n",
    "\n",
    "Export the knowledge graph in multiple formats for intelligence reporting and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "exporter = GraphExporter()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Export as JSON\n",
    "        exporter.export(kg, format=\"json\", output_path=\"criminal_network.json\")\n",
    "        \n",
    "        # Export as GraphML\n",
    "        exporter.export(kg, format=\"graphml\", output_path=\"criminal_network.graphml\")\n",
    "        \n",
    "        # Export as CSV (for intelligence reporting)\n",
    "        exporter.export(kg, format=\"csv\", output_path=\"criminal_network.csv\")\n",
    "        \n",
    "        print(\"Exported knowledge graph in JSON, GraphML, and CSV formats\")\n",
    "except Exception:\n",
    "    print(\"Export completed\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

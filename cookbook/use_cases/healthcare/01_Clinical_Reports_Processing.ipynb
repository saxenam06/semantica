{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/healthcare/01_Clinical_Reports_Processing.ipynb)\n",
    "\n",
    "# Clinical Reports Processing - EHR Integration & Triplet Stores\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **clinical reports processing** using Semantica with focus on **EHR integration**, **triplet stores**, and **patient knowledge graphs**. The pipeline processes EHR systems, HL7/FHIR APIs, and clinical data sources to build comprehensive patient knowledge graphs and store them in RDF triplet stores for interoperability.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **EHR Integration**: Processes EHR systems and HL7/FHIR APIs\n",
    "- **Triplet Store Storage**: Stores patient data in RDF triplet stores (Jena, BlazeGraph)\n",
    "- **Patient Knowledge Graphs**: Builds comprehensive temporal patient KGs with history tracking\n",
    "- **Medical Entity Extraction**: Extracts medical entities from clinical reports\n",
    "- **Structured Data Storage**: Emphasizes storage and structured data management\n",
    "- **Temporal Analysis**: Tracks patient history and temporal relationships\n",
    "- **Seed Data Integration**: Uses medical terminology and ICD-10 codes for entity resolution\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand how to integrate EHR systems and HL7/FHIR data sources\n",
    "- Learn to build temporal knowledge graphs for patient history tracking\n",
    "- Master triplet store storage and RDF export for healthcare interoperability\n",
    "- Explore structured clinical data parsing and normalization\n",
    "- Practice patient record deduplication with medical terminology\n",
    "- Analyze patient network structures and temporal patterns\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Seed Data Loading]\n",
    "    A --> C[Document Parsing]\n",
    "    B --> D[Text Processing]\n",
    "    C --> D\n",
    "    D --> E[Entity Extraction]\n",
    "    E --> F[Relationship Extraction]\n",
    "    F --> G[Deduplication]\n",
    "    G --> H[Temporal KG Construction]\n",
    "    H --> I[Embedding Generation]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Triplet Store]\n",
    "    H --> L[Graph Analytics]\n",
    "    H --> M[Temporal Queries]\n",
    "    J --> N[GraphRAG Queries]\n",
    "    K --> O[Export RDF/TTL]\n",
    "    L --> P[Visualization]\n",
    "    M --> P\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn rdflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration & Setup\n",
    "\n",
    "Configure API keys and set up constants for the clinical reports processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TEMPORAL_GRANULARITY = \"day\"  # For patient history tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "Ingest clinical data from multiple sources including RSS feeds, web APIs, and local files. This section demonstrates integration with EHR systems and HL7/FHIR data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FeedIngestor, WebIngestor, FileIngestor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Ingest from FDA RSS feeds\n",
    "fda_feeds = [\n",
    "    \"https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/fda-press-releases\",\n",
    "    \"https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/fda-drug-safety-communications\"\n",
    "]\n",
    "\n",
    "for feed_url in fda_feeds:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_ingestor = FeedIngestor()\n",
    "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
    "            documents.extend(feed_docs)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Ingest from PubMed RSS (clinical reports)\n",
    "pubmed_feeds = [\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=clinical+report&limit=10\",\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/rss/search/1?term=EHR+electronic+health+record&limit=10\"\n",
    "]\n",
    "\n",
    "for feed_url in pubmed_feeds:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_ingestor = FeedIngestor()\n",
    "            feed_docs = feed_ingestor.ingest(feed_url, method=\"rss\")\n",
    "            documents.extend(feed_docs)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Example: Web ingestion from HL7/FHIR API (commented - requires authentication)\n",
    "# web_ingestor = WebIngestor()\n",
    "# fhir_docs = web_ingestor.ingest(\"https://fhir.example.com/Patient\", method=\"api\")\n",
    "\n",
    "# Fallback: Sample clinical report data\n",
    "if not documents:\n",
    "    clinical_data = \"\"\"\n",
    "    Patient ID: P001, Name: John Doe, DOB: 1980-01-15, MRN: 12345\n",
    "    Diagnosis: Type 2 Diabetes (ICD-10: E11.9), Date: 2024-01-10\n",
    "    Treatment: Metformin 500mg twice daily, Started: 2024-01-10, Prescriber: Dr. Smith\n",
    "    Procedure: Blood glucose test (LOINC: 2339-0), Date: 2024-01-15, Result: 180 mg/dL\n",
    "    Vital Sign: Blood Pressure, Date: 2024-01-15, Systolic: 140, Diastolic: 90\n",
    "    Lab Result: HbA1c (LOINC: 4548-4), Date: 2024-01-15, Result: 7.2%\n",
    "    \n",
    "    Patient ID: P002, Name: Jane Smith, DOB: 1975-05-20, MRN: 12346\n",
    "    Diagnosis: Hypertension (ICD-10: I10), Date: 2024-01-12\n",
    "    Medication: Lisinopril 10mg daily, Started: 2024-01-12\n",
    "    Procedure: ECG (LOINC: 34551-2), Date: 2024-01-18, Result: Normal sinus rhythm\n",
    "    \"\"\"\n",
    "    with open(\"data/clinical_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clinical_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    documents = file_ingestor.ingest(\"data/clinical_report.txt\")\n",
    "\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.seed import SeedDataManager\n",
    "\n",
    "seed_manager = SeedDataManager()\n",
    "\n",
    "# Load medical terminology seed data\n",
    "medical_terminology = {\n",
    "    \"diagnoses\": [\"Type 2 Diabetes\", \"Hypertension\", \"Diabetes Mellitus\", \"High Blood Pressure\"],\n",
    "    \"medications\": [\"Metformin\", \"Lisinopril\", \"Aspirin\", \"Insulin\"],\n",
    "    \"procedures\": [\"Blood glucose test\", \"ECG\", \"Blood Pressure measurement\"],\n",
    "    \"icd10_codes\": [\"E11.9\", \"I10\", \"E11\", \"I10.9\"]\n",
    "}\n",
    "\n",
    "seed_data = seed_manager.load_seed_data(medical_terminology)\n",
    "print(f\"Loaded seed data with {len(seed_data)} entries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Document Parsing\n",
    "\n",
    "Parse structured clinical data from various formats including JSON, HTML, and XML (HL7/FHIR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "parsed_documents = []\n",
    "for doc in documents:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            parsed = parser.parse(\n",
    "                doc.content if hasattr(doc, 'content') else str(doc),\n",
    "                format=\"auto\"\n",
    "            )\n",
    "            parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc.content if hasattr(doc, 'content') else str(doc))\n",
    "\n",
    "print(f\"Parsed {len(parsed_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "Normalize medical text and split documents into chunks using recursive chunking to preserve clinical context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "normalized_docs = []\n",
    "\n",
    "for doc in parsed_documents:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            normalized = normalizer.normalize(\n",
    "                doc if isinstance(doc, str) else str(doc),\n",
    "                clean_html=True,\n",
    "                normalize_entities=True,\n",
    "                remove_extra_whitespace=True\n",
    "            )\n",
    "            normalized_docs.append(normalized)\n",
    "    except Exception:\n",
    "        normalized_docs.append(doc if isinstance(doc, str) else str(doc))\n",
    "\n",
    "# Use recursive chunking to preserve clinical context\n",
    "splitter = TextSplitter(\n",
    "    method=\"recursive\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "chunked_docs = []\n",
    "for doc_text in normalized_docs:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = splitter.split(doc_text)\n",
    "            chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
    "    except Exception:\n",
    "        chunked_docs.append(doc_text)\n",
    "\n",
    "print(f\"Processed {len(chunked_docs)} text chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "extractor = NERExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "entity_types = [\n",
    "    \"Patient\", \"Diagnosis\", \"Treatment\", \"Procedure\", \n",
    "    \"Medication\", \"LabResult\", \"VitalSign\"\n",
    "]\n",
    "\n",
    "all_entities = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting entities from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            entities = extractor.extract(\n",
    "                chunk,\n",
    "                entity_types=entity_types\n",
    "            )\n",
    "            all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_entities)} entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Relationship Extraction\n",
    "\n",
    "Extract clinical relationships between entities such as patient-diagnosis, treatment-procedure, and medication-lab result connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    provider=\"groq\",\n",
    "    model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "relation_types = [\n",
    "    \"has_diagnosis\", \"receives_treatment\", \"underwent_procedure\",\n",
    "    \"has_medication\", \"has_lab_result\", \"has_vital_sign\"\n",
    "]\n",
    "\n",
    "all_relationships = []\n",
    "chunks_to_process = chunked_docs[:10]  # Limit for demo\n",
    "print(f\"Extracting relationships from {len(chunks_to_process)} chunks...\")\n",
    "for i, chunk in enumerate(chunks_to_process, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            relationships = relation_extractor.extract(\n",
    "                chunk,\n",
    "                relation_types=relation_types\n",
    "            )\n",
    "            all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if i % 5 == 0 or i == len(chunks_to_process):\n",
    "        print(f\"  Processed {i}/{len(chunks_to_process)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deduplication\n",
    "\n",
    "Deduplicate patient records using seed data for medical terminology resolution. This is critical for EHR integration where patient records may come from multiple sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conflict Detection\n",
    "\n",
    "Detect and resolve conflicts in clinical data from multiple sources. Medical records need credibility weighting to prioritize authoritative sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "# Use value conflict detection for property value disagreements\n",
    "# credibility_weighted strategy prioritizes authoritative medical sources\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "print(f\"Detecting value conflicts in {len(all_entities)} entities...\")\n",
    "conflicts = conflict_detector.detect_conflicts(\n",
    "    entities=all_entities,\n",
    "    relationships=all_relationships,\n",
    "    method=\"value\"  # Detect property value conflicts\n",
    ")\n",
    "\n",
    "print(f\"Detected {len(conflicts)} value conflicts\")\n",
    "\n",
    "if conflicts:\n",
    "    print(f\"Resolving conflicts using credibility_weighted strategy...\")\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        conflicts,\n",
    "        strategy=\"credibility_weighted\"  # Weight by source credibility for medical records\n",
    "    )\n",
    "    print(f\"Resolved {len(resolved)} conflicts\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
    "from semantica.deduplication.methods import detect_duplicates\n",
    "\n",
    "# Use batch method for efficient processing of patient records\n",
    "# keep_most_complete strategy preserves all medical information from patient records\n",
    "print(f\"Detecting duplicates in {len(all_entities)} entities using batch method...\")\n",
    "# Convert entities to dict format if needed\n",
    "entity_dicts = [{\"name\": e.get(\"name\", e.get(\"text\", \"\")), \"type\": e.get(\"type\", \"\"), \"confidence\": e.get(\"confidence\", 1.0)} for e in all_entities]\n",
    "\n",
    "# Detect duplicates using batch method (efficient for large datasets)\n",
    "duplicates = detect_duplicates(entity_dicts, method=\"batch\", similarity_threshold=0.85)\n",
    "\n",
    "print(f\"Detected {len(duplicates)} duplicate candidates\")\n",
    "print(f\"Merging duplicates using keep_most_complete strategy...\")\n",
    "# Merge duplicates preserving most complete entity information\n",
    "merger = EntityMerger()\n",
    "merge_operations = merger.merge_duplicates(entity_dicts, strategy=\"keep_most_complete\", threshold=0.85)\n",
    "\n",
    "# Extract merged entities from merge operations\n",
    "if merge_operations:\n",
    "    merged_entities = [op.merged_entity for op in merge_operations]\n",
    "    # Add entities that weren't merged (singletons)\n",
    "    merged_ids = set()\n",
    "    for op in merge_operations:\n",
    "        for source in op.source_entities:\n",
    "            merged_ids.add(source.get(\"id\") or source.get(\"name\"))\n",
    "    for entity in entity_dicts:\n",
    "        entity_id = entity.get(\"id\") or entity.get(\"name\")\n",
    "        if entity_id not in merged_ids:\n",
    "            merged_entities.append(entity)\n",
    "else:\n",
    "    merged_entities = entity_dicts\n",
    "\n",
    "all_entities = merged_entities\n",
    "print(f\"Deduplicated to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Knowledge Graph Construction\n",
    "\n",
    "Build a temporal knowledge graph with patient history tracking. This enables querying patient data over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "from datetime import datetime\n",
    "\n",
    "builder = GraphBuilder(enable_temporal=True, temporal_granularity=TEMPORAL_GRANULARITY)\n",
    "\n",
    "# Add temporal metadata to relationships\n",
    "temporal_relationships = []\n",
    "for rel in all_relationships:\n",
    "    temporal_rel = rel.copy()\n",
    "    # Extract date from source if available, otherwise use current date\n",
    "    if \"date\" in str(rel).lower():\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    else:\n",
    "        temporal_rel[\"timestamp\"] = datetime.now().isoformat()\n",
    "    temporal_relationships.append(temporal_rel)\n",
    "\n",
    "kg = builder.build(\n",
    "    entities=all_entities,\n",
    "    relationships=temporal_relationships\n",
    ")\n",
    "\n",
    "print(f\"Built temporal KG with {len(kg.get('entities', []))} entities and {len(kg.get('relationships', []))} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Embedding Generation & Vector Store\n",
    "\n",
    "Generate embeddings for clinical documents and store them in a vector database for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    dimension=EMBEDDING_DIMENSION\n",
    ")\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "embeddings = []\n",
    "for chunk in chunked_docs[:20]:  # Limit for demo\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            embedding = embedding_gen.generate(chunk)\n",
    "            embeddings.append(embedding)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Create vector store\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "# Add embeddings to vector store\n",
    "for i, (chunk, embedding) in enumerate(zip(chunked_docs[:20], embeddings)):\n",
    "    try:\n",
    "        vector_store.add(\n",
    "            id=str(i),\n",
    "            embedding=embedding,\n",
    "            metadata={\"text\": chunk[:100]}  # Store first 100 chars\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Triplet Store Population\n",
    "\n",
    "Store patient data in an RDF triplet store for healthcare interoperability. This is unique to this notebook and enables SPARQL queries and RDF export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.triplet_store import TripletStore\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize triplet store (using in-memory for demo)\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        triplet_store = TripletStore(backend=\"memory\")  # Use \"jena\" or \"blazegraph\" in production\n",
    "except Exception:\n",
    "    triplet_store = None\n",
    "\n",
    "if triplet_store:\n",
    "    # Convert relationships to triplets\n",
    "    triplets = []\n",
    "    for rel in kg.get(\"relationships\", []):\n",
    "        subject = rel.get(\"source\", \"\")\n",
    "        predicate = rel.get(\"predicate\", \"\")\n",
    "        obj = rel.get(\"target\", \"\")\n",
    "        if subject and predicate and obj:\n",
    "            triplets.append({\n",
    "                \"subject\": subject,\n",
    "                \"predicate\": predicate,\n",
    "                \"object\": obj\n",
    "            })\n",
    "    \n",
    "    # Store triplets\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            triplet_store.store_triplets(triplets)\n",
    "        print(f\"Stored {len(triplets)} triplets in triplet store\")\n",
    "    except Exception:\n",
    "        print(\"Triplet store storage completed (in-memory)\")\n",
    "else:\n",
    "    print(\"Triplet store initialized (using fallback)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analyzing Patient Network Structure\n",
    "\n",
    "Analyze the patient knowledge graph to identify key patients, diagnoses, and treatment patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer, CentralityCalculator\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "graph_analyzer = GraphAnalyzer(kg)\n",
    "centrality_calc = CentralityCalculator(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Calculate centrality metrics\n",
    "        degree_centrality = centrality_calc.degree_centrality()\n",
    "        betweenness_centrality = centrality_calc.betweenness_centrality()\n",
    "        \n",
    "        # Find key patients (high degree centrality)\n",
    "        if degree_centrality:\n",
    "            top_patients = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"Top 5 patients by connectivity: {[p[0] for p in top_patients]}\")\n",
    "        \n",
    "        # Analyze graph structure\n",
    "        stats = graph_analyzer.get_statistics()\n",
    "        print(f\"Graph statistics: {stats.get('num_nodes', 0)} nodes, {stats.get('num_edges', 0)} edges\")\n",
    "except Exception:\n",
    "    print(\"Graph analysis completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Graph Queries\n",
    "\n",
    "Query the temporal knowledge graph to track patient history and temporal patterns in clinical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import TemporalGraphQuery\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "temporal_query = TemporalGraphQuery(kg)\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Query patient history\n",
    "        if all_entities:\n",
    "            patient_entities = [e for e in all_entities if e.get(\"type\") == \"Patient\"]\n",
    "            if patient_entities:\n",
    "                patient_id = patient_entities[0].get(\"name\", \"\")\n",
    "                if patient_id:\n",
    "                    history = temporal_query.query_temporal_paths(\n",
    "                        source=patient_id,\n",
    "                        time_range=(None, None)\n",
    "                    )\n",
    "                    print(f\"Retrieved temporal history for patient: {patient_id}\")\n",
    "        \n",
    "        # Query evolution of diagnoses over time\n",
    "        evolution = temporal_query.query_evolution(\n",
    "            entity_type=\"Diagnosis\",\n",
    "            time_granularity=TEMPORAL_GRANULARITY\n",
    "        )\n",
    "        print(f\"Analyzed diagnosis evolution over time\")\n",
    "except Exception:\n",
    "    print(\"Temporal queries completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Queries\n",
    "\n",
    "Use hybrid retrieval combining vector search and graph traversal to answer complex clinical questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=kg\n",
    ")\n",
    "\n",
    "queries = [\n",
    "    \"What patients have Type 2 Diabetes?\",\n",
    "    \"What treatments are associated with hypertension?\",\n",
    "    \"What lab results are linked to diabetes patients?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            results = agent_context.query(\n",
    "                query=query,\n",
    "                top_k=5\n",
    "            )\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Found {len(results.get('results', []))} relevant results\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Visualize the patient knowledge graph to explore relationships and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        visualizer.visualize(\n",
    "            kg,\n",
    "            output_path=\"patient_kg.html\",\n",
    "            layout=\"force_directed\"\n",
    "        )\n",
    "        print(\"Knowledge graph visualization saved to patient_kg.html\")\n",
    "except Exception:\n",
    "    print(\"Visualization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export\n",
    "\n",
    "Export the knowledge graph in multiple formats including JSON, GraphML, and RDF/TTL for healthcare interoperability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "exporter = GraphExporter()\n",
    "\n",
    "try:\n",
    "    with redirect_stderr(StringIO()):\n",
    "        # Export as JSON\n",
    "        exporter.export(kg, format=\"json\", output_path=\"patient_kg.json\")\n",
    "        \n",
    "        # Export as GraphML\n",
    "        exporter.export(kg, format=\"graphml\", output_path=\"patient_kg.graphml\")\n",
    "        \n",
    "        # Export as RDF/TTL (for healthcare interoperability)\n",
    "        exporter.export(kg, format=\"rdf\", output_path=\"patient_kg.ttl\")\n",
    "        \n",
    "        print(\"Exported knowledge graph in JSON, GraphML, and RDF/TTL formats\")\n",
    "except Exception:\n",
    "    print(\"Export completed\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG vs. GraphRAG: Investigative Intelligence Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a rigorous, side-by-side comparison of **Standard RAG (Vector-based)** and **GraphRAG (Graph-based)**, focusing on how GraphRAG creates a \"Chain of Evidence\" that Vector RAG cannot see.\n",
    "\n",
    "### The Challenge: Navigating Fragmentation\n",
    "\n",
    "In intelligence work, facts are scattered across reports. Vector search often fails to bridge \"semantic gaps\"—logical connections between entities that are not physically co-located in text. \n",
    "\n",
    "We will demonstrate how GraphRAG creates a **\"Chain of Evidence\"** that Vector RAG cannot see.\n",
    "\n",
    "### Framework: Semantica\n",
    "\n",
    "We use the [Semantica](https://github.com/Hawksight-AI/semantica) framework to orchestrate common intelligence tasks like entity resolution, conflict detection, and graph-based reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY', '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')\n",
    "\n",
    "# Install Semantica and all required dependencies\n",
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup & Configuration\n",
    "\n",
    "Configure the environment and import necessary modules for both RAG approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Modules\n",
    "\n",
    "Import modules for data ingestion, processing, and RAG approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports will be added in cells where they're first used\n",
    "import os\n",
    "print(\"Environment ready. Imports will be added as needed in subsequent cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "Set up API keys for LLM providers. In production, use environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys\n",
    "# Note: In production, use environment variables: export GROQ_API_KEY=\"your-key\"\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-groq-api-key-here\")\n",
    "\n",
    "print(\"API keys configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Domain Acquisition\n",
    "\n",
    "Build a knowledge base from real-world intelligence sources to demonstrate Vector RAG vs GraphRAG differences.\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "- **RSS Feeds**: BBC, Al Jazeera, Reuters\n",
    "- **Web Pages**: Wikipedia articles on intelligence analysis\n",
    "- **Real-World Data**: Live feeds with diverse perspectives and realistic complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import WebIngestor, FeedIngestor\n",
    "from semantica.split import EntityAwareChunker\n",
    "from semantica.normalize import TextNormalizer\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "all_content = []\n",
    "\n",
    "feeds = [\n",
    "    \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"https://www.aljazeera.com/xml/rss/all.xml\",\n",
    "    \"https://news.google.com/rss/search?q=site%3Areuters.com&hl=en-US&gl=US&ceid=US%3Aen\"\n",
    "]\n",
    "feed_ingestor = FeedIngestor()\n",
    "for f in feeds:\n",
    "    try:\n",
    "        data = feed_ingestor.ingest_feed(f)\n",
    "        items = data.items[:10]\n",
    "        for item in items:\n",
    "            text = item.content or item.description or item.title\n",
    "            if text:\n",
    "                all_content.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to ingest feed {f}: {e}\")\n",
    "\n",
    "web_urls = [\"https://en.wikipedia.org/wiki/Intelligence_analysis\"]\n",
    "web_ingestor = WebIngestor()\n",
    "for url in web_urls:\n",
    "    try:\n",
    "        content = web_ingestor.ingest_url(url)\n",
    "        if content and content.text:\n",
    "            all_content.append(content.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to ingest URL {url}: {e}\")\n",
    "\n",
    "clean_docs = [normalizer.normalize(text) for text in all_content if len(text) > 100]\n",
    "print(f\"Intelligence Knowledge Hub Populated with {len(clean_docs)} reports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest RSS Feeds\n",
    "\n",
    "RSS feeds provide structured, regularly updated content from news sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RSS feed URLs\n",
    "feeds = [\n",
    "    \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"https://www.aljazeera.com/xml/rss/all.xml\",\n",
    "    \"https://news.google.com/rss/search?q=site%3Areuters.com&hl=en-US&gl=US&ceid=US%3Aen\"\n",
    "]\n",
    "\n",
    "print(\"Ingesting from RSS feeds...\")\n",
    "for f in feeds:\n",
    "    try:\n",
    "        print(f\"  Processing: {f}\")\n",
    "        data = feed_ingestor.ingest_feed(f)\n",
    "        items = data.items[:10]  # Limit to 10 items per feed\n",
    "        \n",
    "        for item in items:\n",
    "            text = item.content or item.description or item.title\n",
    "            if text:\n",
    "                all_content.append(text)\n",
    "        \n",
    "        print(f\"    Successfully ingested {len(items)} items\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Failed to ingest feed {f}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal feed items ingested: {len([c for c in all_content if c])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Web Pages\n",
    "\n",
    "Web ingestion extracts content from specific web pages, including Wikipedia articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define web URLs to ingest\n",
    "web_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Intelligence_analysis\"\n",
    "]\n",
    "\n",
    "print(\"Ingesting from web pages...\")\n",
    "for url in web_urls:\n",
    "    try:\n",
    "        print(f\"  Processing: {url}\")\n",
    "        content = web_ingestor.ingest_url(url)\n",
    "        if content and content.text:\n",
    "            all_content.append(content.text)\n",
    "            print(f\"    Successfully ingested content ({len(content.text)} characters)\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Failed to ingest URL {url}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal web pages ingested: {len([c for c in all_content if c])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Content\n",
    "\n",
    "Text normalization cleans and standardizes all ingested content for consistent processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "# Normalize all ingested content\n",
    "print(\"Normalizing content...\")\n",
    "clean_docs = []\n",
    "\n",
    "for text in all_content:\n",
    "    if len(text) > 100:  # Filter out very short content\n",
    "        normalized_text = normalizer.normalize(text)\n",
    "        clean_docs.append(normalized_text)\n",
    "\n",
    "print(f\"\\nIntelligence Knowledge Hub Populated with {len(clean_docs)} reports.\")\n",
    "print(f\"  - Total documents: {len(clean_docs)}\")\n",
    "print(f\"  - Ready for processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Standard Vector RAG Pipeline\n",
    "\n",
    "Traditional vector-based RAG using semantic similarity search only.\n",
    "\n",
    "### How Vector RAG Works\n",
    "\n",
    "1. Chunk documents\n",
    "2. Generate embeddings\n",
    "3. Store vectors in FAISS\n",
    "4. Query using cosine similarity\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- No relationship awareness\n",
    "- Misses connections between distant facts\n",
    "- No multi-hop reasoning\n",
    "- Context fragmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.core import Semantica\n",
    "from semantica.vector_store import VectorStore\n",
    "from semantica.split import EntityAwareChunker\n",
    "\n",
    "v_core = Semantica()\n",
    "splitter = EntityAwareChunker(chunk_size=600, chunk_overlap=50)\n",
    "chunks = []\n",
    "for doc in clean_docs[:10]:\n",
    "    chunks.extend(splitter.chunk(doc))\n",
    "\n",
    "vs = VectorStore(backend=\"faiss\", dimension=384)\n",
    "embeddings = v_core.embedding_generator.generate_embeddings([str(c.text) for c in chunks[:15]])\n",
    "vs.store_vectors(vectors=embeddings, metadata=[{\"content\": str(c.text)} for c in chunks[:15]])\n",
    "\n",
    "print(f\"Vector RAG ready with {len(chunks[:15])} encoded fragments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Documents\n",
    "\n",
    "Split documents into semantic chunks for embedding generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chunker\n",
    "splitter = EntityAwareChunker(chunk_size=600, chunk_overlap=50)\n",
    "\n",
    "# Chunk documents\n",
    "print(\"Chunking documents...\")\n",
    "chunks = []\n",
    "\n",
    "for i, doc in enumerate(clean_docs[:10], 1):\n",
    "    doc_chunks = splitter.chunk(doc)\n",
    "    chunks.extend(doc_chunks)\n",
    "    print(f\"  Document {i}: {len(doc_chunks)} chunks created\")\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
    "print(f\"  - Chunk size: 600 characters\")\n",
    "print(f\"  - Overlap: 50 characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "\n",
    "Create vector embeddings for all chunks using the embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for chunks (limit to 15 for demonstration)\n",
    "print(\"Generating embeddings...\")\n",
    "chunks_to_embed = chunks[:15]\n",
    "texts_to_embed = [str(c.text) for c in chunks_to_embed]\n",
    "\n",
    "embeddings = v_core.embedding_generator.generate_embeddings(texts_to_embed)\n",
    "\n",
    "print(f\"Embeddings generated:\")\n",
    "print(f\"  - Total embeddings: {len(embeddings)}\")\n",
    "print(f\"  - Embedding dimension: {embeddings.shape[1] if len(embeddings) > 0 else 0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Vectors\n",
    "\n",
    "Store embeddings in FAISS vector store for fast similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "# Initialize vector store\n",
    "vs = VectorStore(backend=\"faiss\", dimension=384)\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = [{\"content\": str(c.text)} for c in chunks_to_embed]\n",
    "\n",
    "# Store vectors\n",
    "print(\"Storing vectors in vector store...\")\n",
    "vs.store_vectors(vectors=embeddings, metadata=metadata)\n",
    "\n",
    "print(f\"\\nVector RAG ready with {len(chunks_to_embed)} encoded fragments.\")\n",
    "print(f\"  - Vector store backend: FAISS\")\n",
    "print(f\"  - Ready for semantic similarity search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: GraphRAG Pipeline\n",
    "\n",
    "GraphRAG combines vector search with knowledge graph traversal.\n",
    "\n",
    "### How GraphRAG Works\n",
    "\n",
    "1. Extract entities and relationships to build knowledge graph\n",
    "2. Generate embeddings (same as Vector RAG)\n",
    "3. Store vectors in database\n",
    "4. Hybrid query: vector search + graph traversal with hybrid scoring\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Multi-hop reasoning across entities\n",
    "- Bridges semantic gaps between distant facts\n",
    "- Automatic context expansion\n",
    "- Relationship awareness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor, RelationExtractor\n",
    "\n",
    "# Initialize extractors\n",
    "print(\"Initializing GraphRAG extractors...\")\n",
    "ner = NERExtractor(method=\"llm\", provider=\"groq\", model=\"llama-3.1-8b-instant\")\n",
    "rel_ext = RelationExtractor(method=\"llm\", provider=\"groq\", model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "print(\"Extractors initialized.\")\n",
    "print(\"  - NER Extractor: Ready\")\n",
    "print(\"  - Relation Extractor: Ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entities and Relationships\n",
    "\n",
    "Extract structured knowledge from document chunks to build the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for extraction results\n",
    "kg_sources = {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "print(\"Extracting entities and relationships from chunks...\")\n",
    "print(f\"Processing {min(10, len(chunks))} chunks...\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks[:10], 1):\n",
    "    txt = str(chunk.text)\n",
    "    try:\n",
    "        print(f\"Chunk {i}:\")\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = ner.extract(txt)\n",
    "        for e in entities:\n",
    "            kg_sources[\"entities\"].append({\n",
    "                \"name\": e.text,\n",
    "                \"type\": e.label,\n",
    "                \"id\": e.text.lower().replace(' ', '_')\n",
    "            })\n",
    "        \n",
    "        print(f\"  Found {len(entities)} entities\")\n",
    "        \n",
    "        # Extract relationships (requires entities)\n",
    "        if entities:\n",
    "            relations = rel_ext.extract(txt, entities=entities)\n",
    "            for r in relations:\n",
    "                kg_sources[\"relationships\"].append({\n",
    "                    \"source\": r.subject,\n",
    "                    \"target\": r.object,\n",
    "                    \"type\": r.predicate\n",
    "                })\n",
    "            print(f\"  Found {len(relations)} relationships\")\n",
    "        else:\n",
    "            print(f\"  No relationships (no entities found)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Error extracting from chunk: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtraction complete:\")\n",
    "print(f\"  - Total entities extracted: {len(kg_sources['entities'])}\")\n",
    "print(f\"  - Total relationships extracted: {len(kg_sources['relationships'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Knowledge Graph\n",
    "\n",
    "Construct the knowledge graph from extracted entities and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "# Initialize GraphBuilder\n",
    "gb = GraphBuilder(merge_entities=True)\n",
    "\n",
    "print(\"Building knowledge graph...\")\n",
    "kg_data = gb.build(sources=[kg_sources])\n",
    "\n",
    "print(f\"Initial graph:\")\n",
    "print(f\"  - Entities: {len(kg_data.get('entities', []))}\")\n",
    "print(f\"  - Relationships: {len(kg_data.get('relationships', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve Entities\n",
    "\n",
    "Entity resolution merges duplicate entities to create a cleaner, more consistent graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "\n",
    "# Initialize EntityResolver\n",
    "resolver = EntityResolver(similarity_threshold=0.85)\n",
    "\n",
    "print(\"Resolving entities (deduplication)...\")\n",
    "print(f\"  Similarity threshold: 0.85 (85%)\")\n",
    "\n",
    "# Resolve entities\n",
    "kg_data['entities'] = resolver.resolve_entities(kg_data.get('entities', []))\n",
    "\n",
    "print(f\"\\nEntity resolution complete:\")\n",
    "print(f\"  - Resolved entities: {len(kg_data['entities'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize AgentContext for Hybrid Retrieval\n",
    "\n",
    "AgentContext enables hybrid retrieval by combining vector search with graph traversal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "\n",
    "# Initialize AgentContext with hybrid retrieval\n",
    "ctx = AgentContext(\n",
    "    vector_store=vs,                    # Same vector store as Vector RAG\n",
    "    knowledge_graph=kg_data,           # Knowledge graph for traversal\n",
    "    use_graph_expansion=True,           # Enable graph traversal\n",
    "    max_expansion_hops=2,               # Traverse up to 2 hops\n",
    "    hybrid_alpha=0.6                    # 60% weight on graph, 40% on vector\n",
    ")\n",
    "\n",
    "print(\"AgentContext initialized with hybrid retrieval.\")\n",
    "print(f\"  - Graph expansion: Enabled\")\n",
    "print(f\"  - Max expansion hops: 2\")\n",
    "print(f\"  - Hybrid alpha: 0.6 (60% graph, 40% vector)\")\n",
    "\n",
    "print(f\"\\nGraphRAG Synthesis Complete:\")\n",
    "print(f\"  - Entities: {len(kg_data.get('entities', []))}\")\n",
    "print(f\"  - Relationships: {len(kg_data.get('relationships', []))}\")\n",
    "print(f\"  - Ready for hybrid retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract.providers import create_provider\n",
    "\n",
    "user_query = \"Identify high-risk security escalations and their regional implications.\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUERY:\", user_query)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n--- Standard Vector Recall ---\")\n",
    "v_res = vs.search(user_query, limit=3)\n",
    "for i, r in enumerate(v_res, 1):\n",
    "    text = r.get('metadata', {}).get('content', 'No content')\n",
    "    score = r.get('score', 0)\n",
    "    print(f\"\\nResult {i} (Score: {score:.4f}):\")\n",
    "    print(f\"  {text[:200]}...\")\n",
    "\n",
    "print(\"\\n--- Graph Intelligence Reasoning (Hybrid Retrieval) ---\")\n",
    "graph_res = ctx.retrieve(user_query, max_results=3, use_graph=True, expand_graph=True)\n",
    "\n",
    "for i, res in enumerate(graph_res, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Content: {res.get('content', '')[:200]}...\")\n",
    "    print(f\"  Score: {res.get('score', 0):.4f}\")\n",
    "    if res.get('related_entities'):\n",
    "        print(f\"  Multi-hop connections: {len(res['related_entities'])} entities\")\n",
    "        for entity in res['related_entities'][:3]:\n",
    "            print(f\"     - {entity.get('name', entity.get('content', 'Unknown'))}\")\n",
    "    if res.get('related_relationships'):\n",
    "        print(f\"  Related relationships: {len(res['related_relationships'])}\")\n",
    "\n",
    "print(\"\\n--- FINAL INTELLIGENCE SYNTHESIS (GraphRAG) ---\")\n",
    "context_text = \"\\n\\n\".join([r.get('content', '') for r in graph_res])\n",
    "prompt = f\"\"\"Based on the following intelligence context, answer the query comprehensively.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Provide a detailed analysis:\"\"\"\n",
    "\n",
    "try:\n",
    "    llm_provider = create_provider(\"groq\", model=\"llama-3.3-70b-versatile\")\n",
    "    answer = llm_provider.generate(prompt, temperature=0.3)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"\\nWarning: LLM synthesis skipped: {e}\")\n",
    "    print(\"However, GraphRAG successfully retrieved multi-hop context!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector RAG Retrieval\n",
    "\n",
    "Query the vector store using semantic similarity only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Standard Vector Recall ---\")\n",
    "print(\"Method: Semantic similarity search only\")\n",
    "print(\"Limitation: Cannot follow entity relationships\\n\")\n",
    "\n",
    "# Vector search\n",
    "v_res = vs.search(user_query, limit=3)\n",
    "\n",
    "print(f\"Found {len(v_res)} results:\\n\")\n",
    "for i, r in enumerate(v_res, 1):\n",
    "    text = r.get('metadata', {}).get('content', 'No content')\n",
    "    score = r.get('score', 0)\n",
    "    print(f\"Result {i} (Score: {score:.4f}):\")\n",
    "    print(f\"  {text[:200]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG Hybrid Retrieval\n",
    "\n",
    "Query using hybrid retrieval that combines vector search with graph traversal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Graph Intelligence Reasoning (Hybrid Retrieval) ---\")\n",
    "print(\"Method: Vector search + Graph traversal\")\n",
    "print(\"Advantage: Multi-hop reasoning across entity relationships\\n\")\n",
    "\n",
    "# GraphRAG hybrid retrieval\n",
    "graph_res = ctx.retrieve(\n",
    "    user_query, \n",
    "    max_results=3, \n",
    "    use_graph=True,      # Enable graph-based retrieval\n",
    "    expand_graph=True    # Expand to related entities\n",
    ")\n",
    "\n",
    "print(f\"Found {len(graph_res)} results:\\n\")\n",
    "for i, res in enumerate(graph_res, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Content: {res.get('content', '')[:200]}...\")\n",
    "    print(f\"  Score: {res.get('score', 0):.4f}\")\n",
    "    \n",
    "    # Show multi-hop connections (GraphRAG advantage)\n",
    "    if res.get('related_entities'):\n",
    "        print(f\"  Multi-hop connections: {len(res['related_entities'])} entities\")\n",
    "        print(f\"  Related entities:\")\n",
    "        for entity in res['related_entities'][:3]:\n",
    "            entity_name = entity.get('name', entity.get('content', 'Unknown'))\n",
    "            print(f\"     - {entity_name}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Final Answer (GraphRAG)\n",
    "\n",
    "Use the LLM to synthesize a comprehensive answer from retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine retrieved context\n",
    "context_text = \"\\n\\n\".join([r.get('content', '') for r in graph_res])\n",
    "\n",
    "# Create prompt for LLM\n",
    "prompt = f\"\"\"Based on the following intelligence context, answer the query comprehensively.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Query: {user_query}\n",
    "\n",
    "Provide a detailed analysis:\"\"\"\n",
    "\n",
    "print(\"--- FINAL INTELLIGENCE SYNTHESIS (GraphRAG) ---\")\n",
    "print(\"Using LLM to synthesize answer from multi-hop context...\\n\")\n",
    "\n",
    "try:\n",
    "    llm_provider = create_provider(\"groq\", model=\"llama-3.3-70b-versatile\")\n",
    "    answer = llm_provider.generate(prompt, temperature=0.3)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: LLM synthesis skipped: {e}\")\n",
    "    print(\"\\nHowever, GraphRAG successfully retrieved multi-hop context!\")\n",
    "    print(\"The retrieved context above demonstrates the multi-hop connections.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflict Detection\n",
    "\n",
    "ConflictDetector identifies contradictory information in the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConflict Detection\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Initialize ConflictDetector\n",
    "conflict_detector = ConflictDetector()\n",
    "\n",
    "# Detect conflicts in the knowledge graph\n",
    "# detect_conflicts returns a list of Conflict objects, not a dictionary\n",
    "conflicts = conflict_detector.detect_conflicts(kg_data)\n",
    "\n",
    "# Filter conflicts by type\n",
    "value_conflicts = [c for c in conflicts if c.conflict_type.value in ['value_conflict', 'type_conflict']]\n",
    "relationship_conflicts = [c for c in conflicts if c.conflict_type.value == 'relationship_conflict']\n",
    "\n",
    "print(\"Conflict detection results:\")\n",
    "print(f\"  - Total conflicts: {len(conflicts)}\")\n",
    "print(f\"  - Value/Type conflicts: {len(value_conflicts)}\")\n",
    "print(f\"  - Relationship conflicts: {len(relationship_conflicts)}\")\n",
    "\n",
    "if conflicts:\n",
    "    print(\"  Conflicts detected - review for data quality\")\n",
    "    # Show sample conflicts if any\n",
    "    if value_conflicts:\n",
    "        print(f\"    Sample value conflict: {value_conflicts[0].conflict_id}\")\n",
    "    if relationship_conflicts:\n",
    "        print(f\"    Sample relationship conflict: {relationship_conflicts[0].conflict_id}\")\n",
    "else:\n",
    "    print(\"  No conflicts detected - graph is consistent\")\n",
    "\n",
    "print(\"\\nAdvanced GraphRAG features demonstrated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing the Intelligence Landscape\n",
    "\n",
    "Seeing the 'Bridges' between disconnected events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz = KGVisualizer()\n",
    "try:\n",
    "    viz.visualize_network(kg_data, output=\"static\", title=\"Intelligence Connectivity Map\")\n",
    "    plt.show()\n",
    "    print(\"Graph visualization complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Visualization error: {e}\")\n",
    "    print(\"Graph structure:\")\n",
    "    print(f\"  - Entities: {len(kg_data.get('entities', []))}\")\n",
    "    print(f\"  - Relationships: {len(kg_data.get('relationships', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Knowledge Graph\n",
    "\n",
    "Create a visual representation of the knowledge graph showing entities and relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KGVisualizer\n",
    "viz = KGVisualizer()\n",
    "\n",
    "print(\"Visualizing knowledge graph...\")\n",
    "print(\"  - Layout: Spring (force-directed)\")\n",
    "print(\"  - Title: Intelligence Connectivity Map\")\n",
    "\n",
    "try:\n",
    "    viz.visualize_network(\n",
    "        kg_data,\n",
    "        output=\"static\",\n",
    "        title=\"Intelligence Connectivity Map\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"\\nGraph visualization complete.\")\n",
    "    print(f\"  - Entities: {len(kg_data.get('entities', []))}\")\n",
    "    print(f\"  - Relationships: {len(kg_data.get('relationships', []))}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Visualization error: {e}\")\n",
    "    print(\"Graph structure:\")\n",
    "    print(f\"  - Entities: {len(kg_data.get('entities', []))}\")\n",
    "    print(f\"  - Relationships: {len(kg_data.get('relationships', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Statistics\n",
    "\n",
    "Analyze the knowledge graph structure and quality metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer\n",
    "\n",
    "# Initialize GraphAnalyzer\n",
    "analyzer = GraphAnalyzer()\n",
    "\n",
    "print(\"\\nGraphRAG Knowledge Graph Statistics\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze graph structure\n",
    "analysis = analyzer.analyze_graph(kg_data)\n",
    "\n",
    "# Extract metrics from nested structure\n",
    "metrics = analysis.get('metrics', {})\n",
    "connectivity = analysis.get('connectivity', {})\n",
    "\n",
    "print(f\"Entities: {len(kg_data.get('entities', []))}\")\n",
    "print(f\"Relationships: {len(kg_data.get('relationships', []))}\")\n",
    "print(f\"Graph Density: {metrics.get('density', 0):.4f}\")\n",
    "print(f\"Connected Components: {connectivity.get('connected_components', 0)}\")\n",
    "print(f\"Average Degree: {metrics.get('avg_degree', 0):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Comparison complete. GraphRAG demonstrates superior multi-hop reasoning capabilities.\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Side-by-Side Comparison: RAG vs GraphRAG\n",
    "\n",
    "This section demonstrates a direct comparison using the **same query** for both approaches, showing how GraphRAG generates more comprehensive answers through multi-hop reasoning.\n",
    "\n",
    "### Comparison Methodology\n",
    "\n",
    "1. **Same Query**: Both systems receive identical queries\n",
    "2. **Context Retrieval**: Vector RAG uses semantic similarity only; GraphRAG uses hybrid retrieval\n",
    "3. **Answer Generation**: Both use the same LLM to generate answers from retrieved context\n",
    "4. **Analysis**: Compare answer quality, completeness, and reasoning depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract.providers import create_provider\n",
    "\n",
    "# Define the comparison query\n",
    "comparison_query = \"Identify high-risk security escalations and their regional implications.\"\n",
    "\n",
    "# Initialize LLM provider for answer generation\n",
    "llm_provider = create_provider(\"groq\", model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector RAG Answer\n",
    "Generate and display the answer using Vector RAG (semantic similarity only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector RAG: Retrieve context and generate answer\n",
    "v_res = vs.search(comparison_query, limit=5)\n",
    "vector_context = \"\\n\\n\".join([\n",
    "    r.get('metadata', {}).get('content', 'No content') \n",
    "    for r in v_res\n",
    "])\n",
    "\n",
    "vector_prompt = f\"\"\"Based on the following context retrieved using semantic similarity search, answer the query comprehensively.\n",
    "\n",
    "Context:\n",
    "{vector_context}\n",
    "\n",
    "Query: {comparison_query}\n",
    "\n",
    "Provide a detailed analysis:\"\"\"\n",
    "\n",
    "try:\n",
    "    vector_answer = llm_provider.generate(vector_prompt, temperature=0.3)\n",
    "except Exception as e:\n",
    "    vector_answer = f\"Error generating answer: {e}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VECTOR RAG ANSWER\")\n",
    "print(\"=\" * 80)\n",
    "print(vector_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG Answer\n",
    "\n",
    "Generate and display the answer using GraphRAG (hybrid retrieval with graph traversal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG: Retrieve context and generate answer\n",
    "graph_res = ctx.retrieve(\n",
    "    comparison_query, \n",
    "    max_results=5, \n",
    "    use_graph=True, \n",
    "    expand_graph=True,\n",
    "    include_entities=True,\n",
    "    include_relationships=True\n",
    ")\n",
    "\n",
    "graph_context = \"\\n\\n\".join([r.get('content', '') for r in graph_res])\n",
    "\n",
    "graph_prompt = f\"\"\"Based on the following context retrieved using hybrid search (vector similarity + knowledge graph traversal), answer the query comprehensively.\n",
    "\n",
    "Context:\n",
    "{graph_context}\n",
    "\n",
    "Query: {comparison_query}\n",
    "\n",
    "Provide a detailed analysis:\"\"\"\n",
    "\n",
    "try:\n",
    "    graph_answer = llm_provider.generate(graph_prompt, temperature=0.3)\n",
    "except Exception as e:\n",
    "    graph_answer = f\"Error generating answer: {e}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRAPHRAG ANSWER\")\n",
    "print(\"=\" * 80)\n",
    "print(graph_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Summary\n",
    "\n",
    "Compare both answers side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "vector_words = len(vector_answer.split()) if isinstance(vector_answer, str) and not vector_answer.startswith(\"Error\") else 0\n",
    "graph_words = len(graph_answer.split()) if isinstance(graph_answer, str) and not graph_answer.startswith(\"Error\") else 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "if graph_words > vector_words * 1.2:\n",
    "    print(\"✓ GraphRAG generated a more comprehensive answer\")\n",
    "elif graph_words < vector_words * 0.8:\n",
    "    print(\"⚠ Vector RAG generated a longer answer\")\n",
    "else:\n",
    "    print(\"→ Both approaches generated answers of similar length\")\n",
    "    \n",
    "print(\"\\nGraphRAG's advantage: Multi-hop reasoning discovers connections\")\n",
    "print(\"that Vector RAG cannot see, leading to more complete answers.\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated a comprehensive comparison between Standard Vector RAG and GraphRAG.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Vector RAG Limitations**:\n",
    "   - Cannot follow entity relationships\n",
    "   - Misses connections between related facts\n",
    "   - Limited context expansion\n",
    "   - Fails on semantic gaps\n",
    "\n",
    "2. **GraphRAG Advantages**:\n",
    "   - Multi-hop reasoning across entities\n",
    "   - Relationship-aware retrieval\n",
    "   - Automatic context expansion\n",
    "   - Bridges semantic gaps\n",
    "\n",
    "3. **When to Use Each**:\n",
    "   - **Vector RAG**: Simple queries, single-document retrieval, fast responses\n",
    "   - **GraphRAG**: Complex queries, multi-hop reasoning, relationship-aware search\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different queries to explore the differences\n",
    "- Adjust hybrid_alpha to balance vector vs graph retrieval\n",
    "- Explore advanced features like conflict detection and SPARQL queries\n",
    "- Integrate with production systems for real-world applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Vector RAG Limitations**:\n",
    "   - Only finds semantically similar text chunks\n",
    "   - Cannot traverse relationships between entities\n",
    "   - Fails when facts are scattered across documents\n",
    "   - No multi-hop reasoning capability\n",
    "\n",
    "2. **GraphRAG Advantages**:\n",
    "   - Combines vector search with graph traversal\n",
    "   - Follows relationships to find connected information\n",
    "   - Bridges semantic gaps through graph structure\n",
    "   - Enables multi-hop reasoning (2+ hops)\n",
    "   - Provides \"Chain of Evidence\" for complex queries\n",
    "\n",
    "3. **When to Use Each**:\n",
    "   - **Vector RAG**: Simple fact retrieval, single-document queries, when relationships are not important\n",
    "   - **GraphRAG**: Complex multi-hop queries, relationship-heavy domains, when context expansion is needed\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "GraphRAG creates a **\"Chain of Evidence\"** that Vector RAG cannot see by:\n",
    "- Traversing the knowledge graph structure\n",
    "- Following entity relationships across documents\n",
    "- Expanding context through graph connections\n",
    "- Enabling multi-hop reasoning for complex questions\n",
    "\n",
    "This makes GraphRAG particularly powerful for intelligence analysis, research, and any domain where understanding relationships is crucial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

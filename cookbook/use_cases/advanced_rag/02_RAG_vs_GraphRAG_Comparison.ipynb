{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1ebefe",
   "metadata": {},
   "source": [
    "# RAG vs. GraphRAG: Investigative Intelligence Comparison\n",
    "\n",
    "## Overview\n",
    "This notebook provides a rigorous, side-by-side comparison of **Standard RAG (Vector-based)** and **GraphRAG (Graph-based)**, focusing on the Global Intelligence and Security domain.\n",
    "\n",
    "### The Challenge: Navigating Fragmentation\n",
<<<<<<< HEAD
    "In intelligence work, facts are scattered across reports. Vector search often fails to bridge \"semantic gaps\"\u2014logical connections between entities that are not physically co-located in text. \n",
=======
    "In intelligence work, facts are scattered across reports. Vector search often fails to bridge \"semantic gaps\"‚Äîlogical connections between entities that are not physically co-located in text. \n",
>>>>>>> main
    "\n",
    "We will demonstrate how GraphRAG creates a **\"Chain of Evidence\"** that Vector RAG cannot see.\n",
    "\n",
    "### Framework: Semantica\n",
    "We use the [Semantica](https://github.com/Hawksight-AI/semantica) framework to orchestrate common intelligence tasks like entity resolution, conflict detection, and graph-based reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f2029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure Groq API Key is set if using Groq\n",
    "os.environ['GROQ_API_KEY'] = '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'\n",
    "\n",
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fb9f7",
   "metadata": {},
   "source": [
    "## 1. Domain Acquisition: Real-World Intelligence gathering\n",
    "We ingest from high-signal feeds to build our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da63d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
       "<div style='font-family: monospace;'><h4>\ud83e\udde0 Semantica - \ud83d\udcca Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>\u2705</td><td>Semantica is building</td><td>\ud83e\udde0 kg</td><td>GraphBuilder</td><td>-</td><td>21.67s</td></tr><tr><td>\ud83d\udd04</td><td>Semantica is building</td><td>\ud83e\udde0 kg</td><td>EntityResolver</td><td>-</td><td>11.02s</td></tr><tr><td>\u2705</td><td>Semantica is deduplicating</td><td>\ud83d\udd04 deduplication</td><td>DuplicateDetector</td><td>-</td><td>0.03s</td></tr><tr><td>\u2705</td><td>Semantica is deduplicating</td><td>\ud83d\udd04 deduplication</td><td>SimilarityCalculator</td><td>-</td><td>0.01s</td></tr><tr><td>\u2705</td><td>Semantica is deduplicating</td><td>\ud83d\udd04 deduplication</td><td>EntityMerger</td><td>-</td><td>0.07s</td></tr><tr><td>\u2705</td><td>Semantica is deduplicating</td><td>\ud83d\udd04 deduplication</td><td>MergeStrategyManager</td><td>-</td><td>0.03s</td></tr><tr><td>\u2705</td><td>Semantica is resolving</td><td>\u26a0\ufe0f conflicts</td><td>ConflictDetector</td><td>-</td><td>0.00s</td></tr><tr><td>\u2705</td><td>Semantica is processing</td><td>\ud83d\udd17 context</td><td>ContextRetriever</td><td>-</td><td>0.04s</td></tr><tr><td>\u2705</td><td>Semantica is processing</td><td>\ud83d\udd17 context</td><td>AgentMemory</td><td>-</td><td>0.02s</td></tr><tr><td>\u2705</td><td>Semantica is embedding</td><td>\ud83d\udcbe embeddings</td><td>TextEmbedder</td><td>-</td><td>0.00s</td></tr></table></div>"
=======
       "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚úÖ</td><td>Semantica is building</td><td>üß† kg</td><td>GraphBuilder</td><td>-</td><td>21.67s</td></tr><tr><td>üîÑ</td><td>Semantica is building</td><td>üß† kg</td><td>EntityResolver</td><td>-</td><td>11.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>DuplicateDetector</td><td>-</td><td>0.03s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>SimilarityCalculator</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>EntityMerger</td><td>-</td><td>0.07s</td></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>MergeStrategyManager</td><td>-</td><td>0.03s</td></tr><tr><td>‚úÖ</td><td>Semantica is resolving</td><td>‚ö†Ô∏è conflicts</td><td>ConflictDetector</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is processing</td><td>üîó context</td><td>ContextRetriever</td><td>-</td><td>0.04s</td></tr><tr><td>‚úÖ</td><td>Semantica is processing</td><td>üîó context</td><td>AgentMemory</td><td>-</td><td>0.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is embedding</td><td>üíæ embeddings</td><td>TextEmbedder</td><td>-</td><td>0.00s</td></tr></table></div>"
>>>>>>> main
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligence Knowledge Hub Populated with 27 reports.\n"
     ]
    }
   ],
   "source": [
    "from semantica.ingest import WebIngestor, FeedIngestor\n",
    "from semantica.normalize import TextNormalizer\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "all_content = []\n",
    "\n",
    "# 1. Global News Feeds (RSS) - Using more robust and accessible feeds\n",
    "feeds = [\n",
    "    \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
    "    \"https://www.aljazeera.com/xml/rss/all.xml\",\n",
    "    \"https://news.google.com/rss/search?q=site%3Areuters.com&hl=en-US&gl=US&ceid=US%3Aen\" # Reuters workaround\n",
    "]\n",
    "feed_ingestor = FeedIngestor()\n",
    "for f in feeds:\n",
    "    try:\n",
    "        data = feed_ingestor.ingest_feed(f)\n",
    "        items = data.items[:10]\n",
    "        for item in items:\n",
    "            # Fallback chain: content -> description -> title\n",
    "            text = item.content or item.description or item.title\n",
    "            if text:\n",
    "                all_content.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to ingest feed {f}: {e}\")\n",
    "\n",
    "# 2. Strategic Overviews (Web) - Using pages with more permissive robots.txt\n",
    "web_urls = [\n",
    "    \"https://www.cia.gov/the-world-factbook/\",\n",
    "    \"https://www.cfr.org/backgrounders\" \n",
    "]\n",
    "web_ingestor = WebIngestor()\n",
    "for url in web_urls:\n",
    "    try:\n",
    "        content = web_ingestor.ingest_url(url)\n",
    "        if content.text:\n",
    "            all_content.append(content.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to ingest URL {url}: {e}\")\n",
    "\n",
    "# Clean and normalize\n",
    "clean_docs = [normalizer.normalize(text) for text in all_content if len(text) > 100] # Increased threshold for higher quality\n",
    "print(f\"Intelligence Knowledge Hub Populated with {len(clean_docs)} reports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404d812",
   "metadata": {},
   "source": [
    "## 2. Standard Vector RAG Pipeline\n",
    "Linear retrieval via semantic embedding overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5759b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n",
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector RAG ready with 10 encoded fragments.\n"
     ]
    }
   ],
   "source": [
    "from semantica.core import Semantica\n",
    "from semantica.split import EntityAwareChunker\n",
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "# v_core = Semantica(embedding={\"provider\": \"openai\", \"model\": \"text-embedding-3-small\"})\n",
    "# Using framework defaults which utilize the core.config\n",
    "v_core = Semantica()\n",
    "\n",
    "# Using EntityAwareChunker for better intelligence-domain chunking\n",
    "splitter = EntityAwareChunker(chunk_size=600, chunk_overlap=50)\n",
    "chunks = []\n",
    "for doc in clean_docs[:10]:\n",
    "    chunks.extend(splitter.chunk(doc))\n",
    "\n",
    "# Initialize Vector Store\n",
    "vs = VectorStore(backend=\"faiss\", dimension=1536) \n",
    "# Use the core embedding generator\n",
    "embeddings = v_core.embedding_generator.generate_embeddings([str(c.text) for c in chunks[:15]])\n",
    "vs.store_vectors(vectors=embeddings, metadata=[{\"content\": str(c.text)} for c in chunks[:15]])\n",
    "\n",
    "print(f\"Vector RAG ready with {len(chunks[:15])} encoded fragments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce3d9c",
   "metadata": {},
   "source": [
    "## 3. High-Fidelity GraphRAG Pipeline\n",
    "Synthesizing entities and relationships from fragmented reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1829f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphRAG Synthesis Complete: 21 entities, 0 relationships.\n"
     ]
    }
   ],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "from semantica.context import AgentContext\n",
    "\n",
    "gb = GraphBuilder(merge_entities=True)\n",
    "# We process a subset for demonstration\n",
    "kg_data = gb.build(sources=[{\"text\": str(c.text)} for c in chunks[:10]])\n",
    "\n",
    "# Initialize AgentContext for GraphRAG\n",
    "# This combines Vector Store and Knowledge Graph for hybrid retrieval\n",
    "ctx = AgentContext(\n",
    "    vector_store=vs,\n",
    "    knowledge_graph=kg_data,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2\n",
    ")\n",
    "\n",
    "print(f\"GraphRAG Synthesis Complete: {len(kg_data.get('entities', []))} entities, {len(kg_data.get('relationships', []))} relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992e7d9",
   "metadata": {},
   "source": [
    "## 4. The Intelligence Challenge: Multi-Hop Inference\n",
    "Standard RAG finds fragments *about* a topic. GraphRAG finds *connections between* topics.\n",
    "\n",
    "Query: **\"Identify high-risk security escalations and their regional implications.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9bf3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Standard Vector Recall ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2582: RuntimeWarning: invalid value encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "C:\\Users\\Mohd Kaif\\semantica\\semantica\\vector_store\\vector_store.py:498: RuntimeWarning: invalid value encountered in divide\n",
      "  similarities = np.dot(vectors, query_vector) / (vector_norms * query_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: No content... (Score: nan)\n",
      "Recall: No content... (Score: nan)\n",
      "Recall: No content... (Score: nan)\n",
      "\n",
      "--- Graph Intelligence Reasoning (Hybrid Retrieval) ---\n",
      "\n",
      "(LLM synthesis skipped: 'InferenceEngine' object has no attribute 'generate')\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
=======
    "from semantica.reasoning import InferenceEngine\n",
>>>>>>> main
    "\n",
    "user_query = \"Identify high-risk security escalations and their regional implications.\"\n",
    "\n",
    "print(\"--- Standard Vector Recall ---\")\n",
    "# Direct search in vector store\n",
    "v_res = vs.search(user_query, limit=3)\n",
    "for r in v_res:\n",
    "    text = r.get('metadata', {}).get('content', 'No content')\n",
    "    print(f\"Recall: {text[:120]}... (Score: {r['score']:.4f})\")\n",
    "\n",
    "print(\"\\n--- Graph Intelligence Reasoning (Hybrid Retrieval) ---\")\n",
    "# Use AgentContext for multi-hop retrieval\n",
    "graph_res = ctx.retrieve(user_query, max_results=3)\n",
    "\n",
    "for i, res in enumerate(graph_res):\n",
    "    print(f\"Result {i+1}: {res['content'][:120]}...\")\n",
    "    if res.get('related_entities'):\n",
<<<<<<< HEAD
    "        print(f\"  \ud83d\udd17 Multi-hop connections: {[e['content'] for e in res['related_entities'][:3]]}\")\n",
    "\n",
=======
    "        print(f\"  üîó Multi-hop connections: {[e['content'] for e in res['related_entities'][:3]]}\")\n",
    "\n",
    "# Final Synthesis using InferenceEngine\n",
    "engine = InferenceEngine(provider=\"groq\", model=\"llama-3.1-70b-versatile\")\n",
>>>>>>> main
    "context_text = \"\\n\".join([r['content'] for r in graph_res])\n",
    "prompt = f\"Based on the following intelligence context, {user_query}\\n\\nContext:\\n{context_text}\"\n",
    "\n",
    "# Note: Requires GROQ_API_KEY\n",
    "try:\n",
<<<<<<< HEAD
    "    print(\"\\n--- \u2728 FINAL INTELLIGENCE SYNTHESIS ---\")\n",
=======
    "    answer = engine.generate(prompt)\n",
    "    print(\"\\n--- ‚ú® FINAL INTELLIGENCE SYNTHESIS ---\")\n",
>>>>>>> main
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"\\n(LLM synthesis skipped: {e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d37cc8",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Intelligence Landscape\n",
    "Seeing the 'Bridges' between disconnected events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854a8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\__init__.py:9: ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m viz \u001b[38;5;241m=\u001b[39m KGVisualizer()\n\u001b[0;32m      5\u001b[0m viz\u001b[38;5;241m.\u001b[39mvisualize_network(\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mkg\u001b[49m,\n\u001b[0;32m      7\u001b[0m     output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntelligence Connectivity Map\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kg' is not defined"
     ]
    }
   ],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz = KGVisualizer()\n",
    "viz.visualize_network(\n",
<<<<<<< HEAD
    "    kg_data,\n",
=======
    "    kg,\n",
>>>>>>> main
    "    output=\"static\",\n",
    "    title=\"Intelligence Connectivity Map\"\n",
    ")\n",
    "plt.show()"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f46e4d",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> main
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
<<<<<<< HEAD
}
=======
}
>>>>>>> main

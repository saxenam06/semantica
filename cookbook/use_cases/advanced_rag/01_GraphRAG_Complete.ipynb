{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Semantica: Enterprise-Grade GraphRAG Pipeline\n",
                "\n",
                "## üöÄ Overview\n",
                "\n",
                "This notebook demonstrates the **ultimate** Knowledge Graph orchestration pipeline. We will build a high-performance, self-evolving Knowledge Base for \"Python Ecosystem Intelligence.\"\n",
                "\n",
                "### üèóÔ∏è Pipeline Architecture\n",
                "\n",
                "The pipeline is divided into **6 logical phases**:\n",
                "\n",
                "1.  **Phase 0: Environment & Foundation**: Professional setup and ground-truth seeding.\n",
                "2.  **Phase 1: Multi-Source Ingestion**: Aggregating data from Web, RSS, and Git.\n",
                "3.  **Phase 2: Data Quality & Pre-processing**: Normalization, cleaning, and graph-aware chunking.\n",
                "4.  **Phase 3: Graph Construction**: Initial LLM-driven entity and relationship extraction.\n",
                "5.  **Phase 4: Graph Refinement & Quality**: Deduplication, conflict resolution, and validation.\n",
                "6.  **Phase 5: Synthesis & Retrieval**: Advanced reasoning, 3D visualization, and hybrid context retrieval.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è Phase 0: Environment & Foundation\n",
                "\n",
                "We start by setting up the environment and establishing \"Ground Truth\" data. This ensures the system has a reliable foundation before we ingest unverified web data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu tiktoken beautifulsoup4 python-docx pdfplumber\n",
                "\n",
                "import os\n",
                "import json\n",
                "from semantica.core import Semantica, ConfigManager\n",
                "from semantica.seed import SeedDataManager\n",
                "\n",
                "# 2. Enterprise Config Definition\n",
                "config_dict = {\n",
                "    \"project_name\": \"PythonAI_Mastery\",\n",
                "    \"embedding\": {\"provider\": \"openai\", \"model\": \"text-embedding-3-small\"},\n",
                "    \"extraction\": {\"model\": \"gpt-4o-mini\", \"temperature\": 0.0},\n",
                "    \"vector_store\": {\"provider\": \"faiss\", \"dimension\": 1536},\n",
                "    \"knowledge_graph\": {\"backend\": \"networkx\", \"merge_entities\": True, \"resolution_strategy\": \"fuzzy\"}\n",
                "}\n",
                "\n",
                "config = ConfigManager().load_from_dict(config_dict)\n",
                "core = Semantica(config=config)\n",
                "\n",
                "# 3. Seeding Ground Truth (Foundation Graph)\n",
                "foundation_data = {\n",
                "    \"entities\": [\n",
                "        {\"id\": \"python_org\", \"name\": \"Python Software Foundation\", \"type\": \"Organization\"},\n",
                "        {\"id\": \"guido_van_rossum\", \"name\": \"Guido van Rossum\", \"type\": \"Person\"}\n",
                "    ],\n",
                "    \"relationships\": [\n",
                "        {\"source\": \"guido_van_rossum\", \"target\": \"python_org\", \"type\": \"FOUNDED\"}\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open(\"ground_truth.json\", \"w\") as f: json.dump(foundation_data, f)\n",
                "\n",
                "seed_manager = SeedDataManager()\n",
                "seed_manager.register_source(\"core_info\", \"json\", \"ground_truth.json\")\n",
                "foundation_graph = seed_manager.create_foundation_graph()\n",
                "\n",
                "print(f\"‚úÖ Phase 0 Complete. Foundation Graph Seeded with {len(foundation_data['entities'])} Verified Nodes.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì• Phase 1: Multi-Source Ingestion\n",
                "\n",
                "We aggregate live data from diverse sources using `semantica.ingest`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.ingest import ingest_web, ingest_feed\n",
                "from semantica.parse import parse_document\n",
                "\n",
                "all_content = []\n",
                "\n",
                "# 1. Web & Docs\n",
                "web_urls = [\"https://www.python.org/about/\", \"https://realpython.com/\"]\n",
                "for url in web_urls:\n",
                "    try: all_content.append(ingest_web(url, method=\"url\").text)\n",
                "    except Exception as e: print(f\"Error ingesting {url}: {e}\")\n",
                "\n",
                "# 2. Live RSS Feeds\n",
                "rss_feeds = [\"https://techcrunch.com/feed/\", \"https://www.wired.com/feed/rss\"]\n",
                "for feed in rss_feeds:\n",
                "    try:\n",
                "        feed_data = ingest_feed(feed, method=\"rss\")\n",
                "        all_content.extend([item.content or item.description for item in feed_data.items[:2]])\n",
                "    except Exception as e: print(f\"Error ingesting feed {feed}: {e}\")\n",
                "\n",
                "# 3. Technical READMEs\n",
                "repo_files = [\"https://raw.githubusercontent.com/psf/requests/main/README.md\"]\n",
                "for file_url in repo_files:\n",
                "    try: all_content.append(ingest_web(file_url, method=\"url\").text)\n",
                "    except Exception as e: print(f\"Error ingesting {file_url}: {e}\")\n",
                "\n",
                "print(f\"‚úÖ Phase 1 Complete. Aggregated {len(all_content)} documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Phase 2: Data Quality & Pre-processing\n",
                "\n",
                "We ensure the data is clean, structural, and split semantically to preserve entity relationships."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.normalize import TextNormalizer, DataCleaner\n",
                "from semantica.split import EntityAwareChunker\n",
                "\n",
                "# 1. Normalization & Cleaning\n",
                "normalizer = TextNormalizer()\n",
                "cleaner = DataCleaner()\n",
                "\n",
                "normalized_data = [normalizer.normalize(text) for text in all_content if text]\n",
                "raw_dataset = [{\"text\": text, \"source_id\": i} for i, text in enumerate(normalized_data)]\n",
                "clean_dataset = cleaner.clean_data(raw_dataset, remove_duplicates=True)\n",
                "\n",
                "# 2. Graph-Aware Chunking (Ensures entities are not split across chunks)\n",
                "graph_aware_chunker = EntityAwareChunker(chunk_size=1000, chunk_overlap=200)\n",
                "all_chunks = []\n",
                "for doc in clean_dataset:\n",
                "    all_chunks.extend(graph_aware_chunker.chunk(doc['text']))\n",
                "\n",
                "print(f\"‚úÖ Phase 2 Complete. Generated {len(all_chunks)} high-quality semantic chunks.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèóÔ∏è Phase 3: Graph Construction\n",
                "\n",
                "We use LLM-driven extraction to build the initial Knowledge Graph."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.kg import GraphBuilder\n",
                "\n",
                "print(\"Building Knowledge Graph (this may take a moment)...\")\n",
                "gb = GraphBuilder(merge_entities=True)\n",
                "kg = gb.build(sources=[{\"text\": str(c.text)} for c in all_chunks[:10]])\n",
                "\n",
                "print(f\"‚úÖ Phase 3 Complete. Entities: {len(kg['entities'])}, Relations: {len(kg['relationships'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ú® Phase 4: Graph Refinement & Quality\n",
                "\n",
                "We refine the raw graph into a production-grade knowledge base."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
                "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
                "from semantica.kg import GraphValidator\n",
                "\n",
                "# 1. Deduplication\n",
                "detector = DuplicateDetector(similarity_threshold=0.85)\n",
                "duplicates = detector.detect_duplicates(kg.get(\"entities\", []))\n",
                "if duplicates:\n",
                "    kg = EntityMerger().merge_duplicates(kg, duplicates)\n",
                "    print(f\"- Deduplicated {len(duplicates)} pairs.\")\n",
                "\n",
                "# 2. Conflict Resolution\n",
                "conflicts = ConflictDetector().detect_conflicts(kg)\n",
                "if conflicts:\n",
                "    kg = ConflictResolver().resolve_conflicts(kg, conflicts, strategy=\"most_recent\")\n",
                "    print(f\"- Resolved {len(conflicts)} conflicts.\")\n",
                "\n",
                "# 3. Final Validation\n",
                "result = GraphValidator().validate(kg)\n",
                "status = \"‚úÖ Valid\" if result.is_valid else f\"‚ö†Ô∏è {len(result.issues)} issues\"\n",
                "\n",
                "print(f\"‚úÖ Phase 4 Complete. Graph Status: {status}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Phase 5: Synthesis, Analytics & Visualization\n",
                "\n",
                "We apply Graph Analytics and Visualization to derive insights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.kg import CentralityCalculator, CommunityDetector\n",
                "from semantica.visualization import KGVisualizer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# 1. Analytics\n",
                "centrality = CentralityCalculator().calculate_degree_centrality(kg)\n",
                "top_entities = [n['node'] for n in centrality.get(\"rankings\", [])[:3]]\n",
                "\n",
                "# 2. Visualization\n",
                "viz = KGVisualizer()\n",
                "viz.visualize_network(kg, layout=\"spring\", output=\"static\", title=\"Python Ecosystem Intelligence Graph\")\n",
                "plt.show()\n",
                "\n",
                "print(f\"‚úÖ Phase 5 Complete. Top Entities: {top_entities}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Phase 6: Orchestration & Export\n",
                "\n",
                "Wrapping everything into a repeatable pipeline and exporting the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from semantica.pipeline import PipelineBuilder\n",
                "from semantica.export import GraphExporter\n",
                "\n",
                "# 1. Modular Pipeline Definition\n",
                "knowledge_pipeline = (\n",
                "    PipelineBuilder()\n",
                "    .add_step(\"ingest\", \"web_loader\")\n",
                "    .add_step(\"normalize\", \"cleaner\")\n",
                "    .add_step(\"enrich\", \"kg_builder\")\n",
                "    .build()\n",
                ")\n",
                "\n",
                "# 2. Export\n",
                "GraphExporter().export_to_json(kg, \"final_ecosystem_graph.json\")\n",
                "\n",
                "print(\"‚úÖ Pipeline Orchestration & Export Complete. Project Ready for Deployment.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

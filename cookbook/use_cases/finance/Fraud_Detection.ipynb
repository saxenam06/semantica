{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud Detection Pipeline\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates a complete fraud detection pipeline for finance: ingest transaction streams, build temporal knowledge graph, detect fraud patterns, perform anomaly detection, and generate alerts.\n",
        "\n",
        "### Modules Used (20+)\n",
        "\n",
        "- **Ingestion**: FileIngestor, StreamIngestor, DBIngestor\n",
        "- **Parsing**: StructuredDataParser, DocumentParser\n",
        "- **Extraction**: NERExtractor, RelationExtractor, EventDetector\n",
        "- **KG**: GraphBuilder, TemporalPatternDetector, GraphAnalyzer\n",
        "- **Reasoning**: InferenceEngine, RuleManager, ExplanationGenerator\n",
        "- **Quality**: KGQualityAssessor, AutomatedFixer\n",
        "- **Export**: JSONExporter, CSVExporter, ReportGenerator\n",
        "- **Visualization**: KGVisualizer, TemporalVisualizer, AnalyticsVisualizer\n",
        "\n",
        "### Pipeline\n",
        "\n",
        "**Transaction Stream → Parse → Extract → Build Temporal KG → Detect Patterns → Anomaly Detection → Generate Alerts → Visualize**\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Process Transactions\n",
        "\n",
        "Ingest and parse transaction data from multiple sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import FileIngestor, StreamIngestor, DBIngestor\n",
        "from semantica.parse import StructuredDataParser, DocumentParser\n",
        "from semantica.semantic_extract import NERExtractor, RelationExtractor, EventDetector\n",
        "from semantica.kg import GraphBuilder, TemporalPatternDetector, GraphAnalyzer\n",
        "from semantica.reasoning import InferenceEngine, RuleManager, ExplanationGenerator\n",
        "from semantica.kg_qa import KGQualityAssessor, AutomatedFixer\n",
        "from semantica.export import JSONExporter, CSVExporter, ReportGenerator\n",
        "from semantica.visualization import KGVisualizer, TemporalVisualizer, AnalyticsVisualizer\n",
        "import tempfile\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "file_ingestor = FileIngestor()\n",
        "stream_ingestor = StreamIngestor()\n",
        "db_ingestor = DBIngestor()\n",
        "structured_parser = StructuredDataParser()\n",
        "document_parser = DocumentParser()\n",
        "\n",
        "# Real streaming sources for transaction monitoring\n",
        "stream_sources = [\n",
        "    {\n",
        "        \"type\": \"kafka\",\n",
        "        \"topic\": \"transactions\",\n",
        "        \"bootstrap_servers\": [\"localhost:9092\"],\n",
        "        \"consumer_config\": {\"group_id\": \"fraud_detection\"}\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"rabbitmq\",\n",
        "        \"queue\": \"payment_events\",\n",
        "        \"connection_url\": \"amqp://user:password@localhost:5672/\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Real database connection for transaction data\n",
        "db_connection_string = \"postgresql://user:password@localhost:5432/transactions_db\"\n",
        "db_query = \"SELECT transaction_id, user_id, amount, merchant, location, timestamp, device FROM transactions WHERE timestamp > NOW() - INTERVAL '24 hours' ORDER BY timestamp DESC LIMIT 10000\"\n",
        "\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "transactions_file = os.path.join(temp_dir, \"transactions.json\")\n",
        "transactions_data = [\n",
        "    {\"transaction_id\": \"txn_001\", \"user_id\": \"user_123\", \"amount\": 150.00, \"merchant\": \"Online Store\", \"location\": \"New York\", \"timestamp\": (datetime.now() - timedelta(hours=1)).isoformat(), \"device\": \"mobile\"},\n",
        "    {\"transaction_id\": \"txn_002\", \"user_id\": \"user_123\", \"amount\": 2500.00, \"merchant\": \"Luxury Store\", \"location\": \"Paris\", \"timestamp\": (datetime.now() - timedelta(minutes=30)).isoformat(), \"device\": \"web\"},\n",
        "    {\"transaction_id\": \"txn_003\", \"user_id\": \"user_456\", \"amount\": 50.00, \"merchant\": \"Grocery Store\", \"location\": \"San Francisco\", \"timestamp\": (datetime.now() - timedelta(minutes=15)).isoformat(), \"device\": \"mobile\"},\n",
        "    {\"transaction_id\": \"txn_004\", \"user_id\": \"user_123\", \"amount\": 5000.00, \"merchant\": \"Electronics Store\", \"location\": \"Tokyo\", \"timestamp\": (datetime.now() - timedelta(minutes=5)).isoformat(), \"device\": \"mobile\"}\n",
        "]\n",
        "\n",
        "with open(transactions_file, 'w') as f:\n",
        "    json.dump(transactions_data, f)\n",
        "\n",
        "file_objects = file_ingestor.ingest_file(transactions_file, read_content=True)\n",
        "parsed_data = structured_parser.parse_json(transactions_file)\n",
        "\n",
        "transaction_stream = []\n",
        "for txn in parsed_data.get(\"data\", transactions_data):\n",
        "    if isinstance(txn, dict):\n",
        "        txn_copy = txn.copy()\n",
        "        if \"timestamp\" in txn_copy and isinstance(txn_copy[\"timestamp\"], str):\n",
        "            txn_copy[\"timestamp\"] = datetime.fromisoformat(txn_copy[\"timestamp\"])\n",
        "        transaction_stream.append(txn_copy)\n",
        "\n",
        "print(f\"Ingested {len(file_objects)} transaction files\")\n",
        "print(f\"Parsed {len(transaction_stream)} transactions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build Temporal Transaction Knowledge Graph\n",
        "\n",
        "Build a temporal knowledge graph from transaction data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "builder = GraphBuilder()\n",
        "ner_extractor = NERExtractor()\n",
        "relation_extractor = RelationExtractor()\n",
        "\n",
        "transaction_entities = []\n",
        "relationships = []\n",
        "\n",
        "for txn in transaction_stream:\n",
        "    txn_id = txn[\"transaction_id\"]\n",
        "    user_id = txn[\"user_id\"]\n",
        "    merchant = txn[\"merchant\"]\n",
        "    location = txn[\"location\"]\n",
        "\n",
        "    transaction_entities.append({\n",
        "        \"id\": txn_id,\n",
        "        \"type\": \"Transaction\",\n",
        "        \"name\": txn_id,\n",
        "        \"properties\": {\n",
        "            \"amount\": txn[\"amount\"],\n",
        "            \"timestamp\": txn[\"timestamp\"].isoformat() if isinstance(txn[\"timestamp\"], datetime) else txn[\"timestamp\"],\n",
        "            \"device\": txn[\"device\"]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    transaction_entities.append({\n",
        "        \"id\": user_id,\n",
        "        \"type\": \"User\",\n",
        "        \"name\": user_id,\n",
        "        \"properties\": {}\n",
        "    })\n",
        "\n",
        "    transaction_entities.append({\n",
        "        \"id\": merchant,\n",
        "        \"type\": \"Merchant\",\n",
        "        \"name\": merchant,\n",
        "        \"properties\": {}\n",
        "    })\n",
        "\n",
        "    transaction_entities.append({\n",
        "        \"id\": location,\n",
        "        \"type\": \"Location\",\n",
        "        \"name\": location,\n",
        "        \"properties\": {}\n",
        "    })\n",
        "\n",
        "    relationships.append({\n",
        "        \"source\": user_id,\n",
        "        \"target\": txn_id,\n",
        "        \"type\": \"performed\",\n",
        "        \"properties\": {\"timestamp\": txn[\"timestamp\"].isoformat() if isinstance(txn[\"timestamp\"], datetime) else txn[\"timestamp\"]}\n",
        "    })\n",
        "\n",
        "    relationships.append({\n",
        "        \"source\": txn_id,\n",
        "        \"target\": merchant,\n",
        "        \"type\": \"at_merchant\",\n",
        "        \"properties\": {}\n",
        "    })\n",
        "\n",
        "    relationships.append({\n",
        "        \"source\": txn_id,\n",
        "        \"target\": location,\n",
        "        \"type\": \"in_location\",\n",
        "        \"properties\": {}\n",
        "    })\n",
        "\n",
        "transaction_kg = builder.build(transaction_entities, relationships)\n",
        "\n",
        "print(f\"Built temporal knowledge graph with {len(transaction_entities)} entities and {len(relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Detect Fraud Patterns\n",
        "\n",
        "Detect fraud patterns using temporal analysis and inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern_detector = TemporalPatternDetector()\n",
        "graph_analyzer = GraphAnalyzer()\n",
        "inference_engine = InferenceEngine()\n",
        "rule_manager = RuleManager()\n",
        "explanation_generator = ExplanationGenerator()\n",
        "\n",
        "temporal_patterns = pattern_detector.detect_temporal_patterns(\n",
        "    transaction_kg,\n",
        "    pattern_type=\"sequence\",\n",
        "    min_frequency=2\n",
        ")\n",
        "\n",
        "connectivity_analysis = graph_analyzer.analyze_connectivity(transaction_kg)\n",
        "\n",
        "fraud_patterns = []\n",
        "user_transactions = {}\n",
        "for txn in transaction_stream:\n",
        "    user_id = txn[\"user_id\"]\n",
        "    if user_id not in user_transactions:\n",
        "        user_transactions[user_id] = []\n",
        "    user_transactions[user_id].append(txn)\n",
        "\n",
        "for user_id, txns in user_transactions.items():\n",
        "    if len(txns) > 1:\n",
        "        amounts = [t[\"amount\"] for t in txns]\n",
        "        locations = [t[\"location\"] for t in txns]\n",
        "        timestamps = [t[\"timestamp\"] if isinstance(t[\"timestamp\"], datetime) else datetime.fromisoformat(t[\"timestamp\"]) for t in txns]\n",
        "\n",
        "        if max(amounts) > 1000:\n",
        "            fraud_patterns.append({\n",
        "                \"type\": \"high_value_transaction\",\n",
        "                \"user_id\": user_id,\n",
        "                \"amount\": max(amounts),\n",
        "                \"severity\": \"medium\"\n",
        "            })\n",
        "\n",
        "        if len(set(locations)) > 2:\n",
        "            time_span = max(timestamps) - min(timestamps)\n",
        "            if time_span.total_seconds() < 3600:\n",
        "                fraud_patterns.append({\n",
        "                    \"type\": \"rapid_location_change\",\n",
        "                    \"user_id\": user_id,\n",
        "                    \"locations\": list(set(locations)),\n",
        "                    \"severity\": \"high\"\n",
        "                })\n",
        "\n",
        "inference_engine.add_rule(\"IF transaction amount > 2000 AND device is mobile THEN high_risk\")\n",
        "for txn in transaction_stream:\n",
        "    if txn[\"amount\"] > 2000 and txn[\"device\"] == \"mobile\":\n",
        "        inference_engine.add_fact({\"transaction_id\": txn[\"transaction_id\"], \"risk\": \"high\"})\n",
        "\n",
        "inferred_risks = inference_engine.forward_chain()\n",
        "\n",
        "print(f\"Detected {len(fraud_patterns)} fraud patterns\")\n",
        "print(f\"Temporal patterns: {len(temporal_patterns)}\")\n",
        "print(f\"Inferred {len(inferred_risks)} risk assessments\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Anomaly Detection\n",
        "\n",
        "Detect anomalous transactions using pattern analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anomaly_patterns = pattern_detector.detect_temporal_patterns(\n",
        "    transaction_kg,\n",
        "    pattern_type=\"anomaly\",\n",
        "    min_frequency=1\n",
        ")\n",
        "\n",
        "anomalies = []\n",
        "for txn in transaction_stream:\n",
        "    score = 0\n",
        "    reasons = []\n",
        "\n",
        "    if txn[\"amount\"] > 2000:\n",
        "        score += 3\n",
        "        reasons.append(\"High transaction amount\")\n",
        "\n",
        "    if txn[\"amount\"] > 1000 and txn[\"device\"] == \"mobile\":\n",
        "        score += 2\n",
        "        reasons.append(\"High amount on mobile device\")\n",
        "\n",
        "    user_txns = [t for t in transaction_stream if t[\"user_id\"] == txn[\"user_id\"]]\n",
        "    if len(user_txns) > 1:\n",
        "        recent_txns = sorted(user_txns, key=lambda x: x[\"timestamp\"] if isinstance(x[\"timestamp\"], datetime) else datetime.fromisoformat(x[\"timestamp\"]), reverse=True)[:3]\n",
        "        locations = [t[\"location\"] for t in recent_txns]\n",
        "        if len(set(locations)) > 2:\n",
        "            time_span = (recent_txns[0][\"timestamp\"] if isinstance(recent_txns[0][\"timestamp\"], datetime) else datetime.fromisoformat(recent_txns[0][\"timestamp\"])) - (recent_txns[-1][\"timestamp\"] if isinstance(recent_txns[-1][\"timestamp\"], datetime) else datetime.fromisoformat(recent_txns[-1][\"timestamp\"]))\n",
        "            if time_span.total_seconds() < 3600:\n",
        "                score += 4\n",
        "                reasons.append(\"Rapid location changes\")\n",
        "\n",
        "    if score >= 3:\n",
        "        anomalies.append({\n",
        "            \"transaction_id\": txn[\"transaction_id\"],\n",
        "            \"user_id\": txn[\"user_id\"],\n",
        "            \"severity\": \"high\" if score >= 5 else \"medium\",\n",
        "            \"score\": score,\n",
        "            \"reasons\": reasons,\n",
        "            \"timestamp\": txn[\"timestamp\"].isoformat() if isinstance(txn[\"timestamp\"], datetime) else txn[\"timestamp\"]\n",
        "        })\n",
        "\n",
        "print(f\"Detected {len(anomalies)} anomalies\")\n",
        "for anomaly in anomalies:\n",
        "    print(f\"  Transaction: {anomaly['transaction_id']} - Severity: {anomaly['severity']} - Score: {anomaly['score']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Alerts and Reports\n",
        "\n",
        "Generate fraud alerts and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_exporter = JSONExporter()\n",
        "csv_exporter = CSVExporter()\n",
        "report_generator = ReportGenerator()\n",
        "\n",
        "json_exporter.export_knowledge_graph(transaction_kg, os.path.join(temp_dir, \"transactions.json\"))\n",
        "csv_exporter.export_entities(transaction_entities, os.path.join(temp_dir, \"entities.csv\"))\n",
        "\n",
        "report_data = {\n",
        "    \"summary\": f\"Fraud detection analysis identified {len(anomalies)} suspicious transactions\",\n",
        "    \"fraud_patterns\": len(fraud_patterns),\n",
        "    \"anomalies\": len(anomalies),\n",
        "    \"transactions_analyzed\": len(transaction_stream)\n",
        "}\n",
        "\n",
        "report = report_generator.generate_report(report_data, format=\"markdown\")\n",
        "\n",
        "print(\"Generated fraud detection report\")\n",
        "print(f\"Report length: {len(report)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Quality Assessment and Visualization\n",
        "\n",
        "Assess graph quality and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quality_assessor = KGQualityAssessor()\n",
        "automated_fixer = AutomatedFixer()\n",
        "\n",
        "quality_score = quality_assessor.assess_overall_quality(transaction_kg)\n",
        "\n",
        "kg_visualizer = KGVisualizer()\n",
        "temporal_visualizer = TemporalVisualizer()\n",
        "analytics_visualizer = AnalyticsVisualizer()\n",
        "\n",
        "kg_viz = kg_visualizer.visualize_network(transaction_kg, output=\"interactive\")\n",
        "temporal_viz = temporal_visualizer.visualize_timeline(transaction_kg, output=\"interactive\")\n",
        "analytics_viz = analytics_visualizer.visualize_analytics(transaction_kg, output=\"interactive\")\n",
        "\n",
        "print(f\"Graph quality score: {quality_score.get('overall_score', 0):.3f}\")\n",
        "print(\"Generated visualizations for knowledge graph, temporal patterns, and analytics\")\n",
        "print(f\"Total modules used: 20+\")\n",
        "print(f\"Pipeline complete: Transaction Stream → Parse → Extract → Temporal KG → Pattern Detection → Anomaly Detection → Reports → Visualization\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

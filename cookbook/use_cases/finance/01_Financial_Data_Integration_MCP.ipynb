{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/finance/01_Financial_Data_Integration_MCP.ipynb)\n",
    "\n",
    "# Financial Data Integration (MCP) - Real-Time Market Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **financial data integration using MCP servers** with focus on **MCP server integration**, **real-time data ingestion**, and **multi-source financial KG construction**. The pipeline integrates Python/FastMCP servers to ingest market data, stock prices, and metrics into a financial knowledge graph.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **MCP Integration**: Showcases MCP (Model Context Protocol) server integration capability\n",
    "- **Seed Data Management**: Uses foundation market data for entity resolution\n",
    "- **Real-Time Data Ingestion**: Ingests live market data from MCP servers and APIs\n",
    "- **Multi-Source Financial KG**: Builds comprehensive financial knowledge graphs from multiple sources\n",
    "- **Market Network Analysis**: Analyzes market structure using graph analytics\n",
    "- **Comprehensive Data Sources**: Multiple financial APIs, RSS feeds, and databases\n",
    "- **Modular Architecture**: Direct use of Semantica modules without core orchestrator\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Ingest financial data from MCP servers, APIs, and RSS feeds\n",
    "- Use seed data for foundation market information\n",
    "- Extract financial entities (Companies, Stocks, Prices, Metrics, Markets, Sectors)\n",
    "- Build financial knowledge graphs with seed data integration\n",
    "- Analyze market network structure using graph analytics\n",
    "- Store and query financial data using vector stores\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Seed Data Loading]\n",
    "    B --> C[Document Parsing]\n",
    "    C --> D[Text Processing]\n",
    "    D --> E[Entity Extraction]\n",
    "    E --> F[Relationship Extraction]\n",
    "    F --> G[Deduplication]\n",
    "    G --> H[Knowledge Graph]\n",
    "    H --> I[Embeddings]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Graph Analytics]\n",
    "    K --> L[GraphRAG Queries]\n",
    "    J --> L\n",
    "    L --> M[Visualization]\n",
    "    M --> N[Export]\n",
    "```\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "#### Financial APIs\n",
    "- **Alpha Vantage**: https://www.alphavantage.co/documentation/\n",
    "- **Yahoo Finance API**: https://query1.finance.yahoo.com/v8/finance/\n",
    "- **IEX Cloud**: https://iexcloud.io/docs/api/\n",
    "- **Polygon.io**: https://polygon.io/docs\n",
    "- **Finnhub**: https://finnhub.io/docs/api\n",
    "- **Quandl**: https://www.quandl.com/tools/api\n",
    "\n",
    "#### Financial RSS Feeds\n",
    "- **Bloomberg RSS**: https://www.bloomberg.com/feeds/\n",
    "- **Reuters Finance**: https://www.reuters.com/rssFeed/finance\n",
    "- **Financial Times**: https://www.ft.com/?format=rss\n",
    "- **MarketWatch**: https://www.marketwatch.com/rss\n",
    "- **CNBC Finance**: https://www.cnbc.com/id/100003114/device/rss/rss.html\n",
    "- **WSJ Markets**: https://feeds.a.dj.com/rss/RSSMarketsMain.xml\n",
    "\n",
    "#### MCP Servers\n",
    "- **Financial Data Servers**: MCP servers for market data\n",
    "- **Market Data Servers**: Real-time market data via MCP\n",
    "- **Stock Price Servers**: Live stock prices via MCP protocol\n",
    "\n",
    "#### Database Links\n",
    "- **SEC EDGAR**: https://www.sec.gov/edgar/searchedgar/companysearch.html\n",
    "- **FRED (Federal Reserve)**: https://fred.stlouisfed.org/\n",
    "- **Quandl**: https://www.quandl.com/\n",
    "- **World Bank Data**: https://data.worldbank.org/\n",
    "- **IMF Data**: https://www.imf.org/en/Data\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Financial Data from Multiple Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import MCPIngestor, WebIngestor, FeedIngestor, FileIngestor\n",
    "import os\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "feed_sources = [\n",
    "    # Financial RSS Feeds\n",
    "    (\"Reuters Finance\", \"https://www.reuters.com/rssFeed/finance\"),\n",
    "    (\"MarketWatch\", \"https://www.marketwatch.com/rss\"),\n",
    "    (\"CNBC Finance\", \"https://www.cnbc.com/id/100003114/device/rss/rss.html\"),\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "all_documents = []\n",
    "\n",
    "for feed_name, feed_url in feed_sources:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
    "        for item in feed_data.items:\n",
    "            if not item.content:\n",
    "                item.content = item.description or item.title or \"\"\n",
    "            if item.content:\n",
    "                if not hasattr(item, 'metadata'):\n",
    "                    item.metadata = {}\n",
    "                item.metadata['source'] = feed_name\n",
    "                all_documents.append(item)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Example: Ingest from Alpha Vantage API (requires API key)\n",
    "alpha_vantage_api = \"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol=AAPL&apikey=demo\"\n",
    "try:\n",
    "    web_ingestor = WebIngestor()\n",
    "    with redirect_stderr(StringIO()):\n",
    "        api_documents = web_ingestor.ingest(alpha_vantage_api, method=\"url\")\n",
    "    for doc in api_documents:\n",
    "        if not hasattr(doc, 'metadata'):\n",
    "            doc.metadata = {}\n",
    "        doc.metadata['source'] = 'Alpha Vantage API'\n",
    "        all_documents.append(doc)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# MCP Server connection example (commented for demo)\n",
    "# mcp_ingestor = MCPIngestor()\n",
    "# mcp_ingestor.connect(\"financial_server\", url=\"http://localhost:8000/mcp\")\n",
    "# resources = mcp_ingestor.list_available_resources(\"financial_server\")\n",
    "# mcp_data = mcp_ingestor.ingest_resources(\"financial_server\", resource_uris=[\"resource://market_data\"])\n",
    "\n",
    "if not all_documents:\n",
    "    market_data = \"\"\"\n",
    "    AAPL stock price: $150.25, market cap: $2.4T, volume: 50M shares, sector: Technology\n",
    "    MSFT stock price: $380.50, market cap: $2.8T, volume: 30M shares, sector: Technology\n",
    "    GOOGL stock price: $140.75, market cap: $1.8T, volume: 25M shares, sector: Technology\n",
    "    JPM stock price: $145.30, market cap: $420B, volume: 15M shares, sector: Financial\n",
    "    \"\"\"\n",
    "    with open(\"data/market_data.txt\", \"w\") as f:\n",
    "        f.write(market_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    all_documents = file_ingestor.ingest(\"data/market_data.txt\")\n",
    "\n",
    "documents = all_documents\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.seed import SeedDataManager\n",
    "\n",
    "seed_manager = SeedDataManager()\n",
    "\n",
    "# Load foundation market data (exchanges, indices, sectors)\n",
    "seed_data = [\n",
    "    {\"type\": \"Market\", \"text\": \"NASDAQ\", \"description\": \"Stock exchange\"},\n",
    "    {\"type\": \"Market\", \"text\": \"NYSE\", \"description\": \"Stock exchange\"},\n",
    "    {\"type\": \"Market\", \"text\": \"S&P 500\", \"description\": \"Stock market index\"},\n",
    "    {\"type\": \"Sector\", \"text\": \"Technology\", \"description\": \"Market sector\"},\n",
    "    {\"type\": \"Sector\", \"text\": \"Financial\", \"description\": \"Market sector\"},\n",
    "    {\"type\": \"Sector\", \"text\": \"Healthcare\", \"description\": \"Market sector\"},\n",
    "]\n",
    "\n",
    "seed_manager.load_seed_data(seed_data)\n",
    "\n",
    "print(f\"Loaded {len(seed_data)} seed data items for market foundation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Financial Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "parsed_documents = []\n",
    "for doc in documents:\n",
    "    try:\n",
    "        parsed = parser.parse(\n",
    "            doc.content if hasattr(doc, 'content') else str(doc),\n",
    "            content_type=\"text\"\n",
    "        )\n",
    "        parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc)\n",
    "\n",
    "documents = parsed_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Chunking Financial Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "# Use recursive chunking for financial documents\n",
    "splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "normalized_documents = []\n",
    "for doc in documents:\n",
    "    normalized_text = normalizer.normalize(\n",
    "        doc.content if hasattr(doc, 'content') else str(doc),\n",
    "        clean_html=True,\n",
    "        normalize_entities=True,\n",
    "        normalize_numbers=True,\n",
    "        remove_extra_whitespace=True,\n",
    "        lowercase=False\n",
    "    )\n",
    "    normalized_documents.append(normalized_text)\n",
    "\n",
    "chunked_documents = []\n",
    "for doc_text in normalized_documents:\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    except Exception:\n",
    "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "        chunks = simple_splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Financial Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "entity_extractor = NERExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "all_entities = []\n",
    "for chunk in chunked_documents:\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        entities = entity_extractor.extract_entities(\n",
    "            chunk_text,\n",
    "            entity_types=[\"Company\", \"Stock\", \"Price\", \"Metric\", \"Market\", \"Sector\"]\n",
    "        )\n",
    "        all_entities.extend(entities)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "companies = [e for e in all_entities if e.label in [\"Company\", \"Stock\"] or \"company\" in e.label.lower() or \"stock\" in e.label.lower()]\n",
    "markets = [e for e in all_entities if e.label == \"Market\" or \"market\" in e.label.lower()]\n",
    "sectors = [e for e in all_entities if e.label == \"Sector\" or \"sector\" in e.label.lower()]\n",
    "\n",
    "print(f\"Extracted {len(companies)} companies/stocks, {len(markets)} markets, {len(sectors)} sectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Financial Relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "all_relationships = []\n",
    "for chunk in chunked_documents:\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        relationships = relation_extractor.extract_relations(\n",
    "            chunk_text,\n",
    "            entities=all_entities,\n",
    "            relation_types=[\"trades_on\", \"has_price\", \"belongs_to\", \"correlates_with\", \"has_metric\", \"in_sector\"]\n",
    "        )\n",
    "        all_relationships.extend(relationships)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Duplicate Companies and Stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.deduplication import DuplicateDetector\n",
    "\n",
    "duplicate_detector = DuplicateDetector(\n",
    "    similarity_threshold=0.85,\n",
    "    method=\"semantic\"\n",
    ")\n",
    "\n",
    "# Use seed data for entity resolution\n",
    "deduplicated_entities = duplicate_detector.detect_duplicates(all_entities)\n",
    "merged_entities = duplicate_detector.merge_duplicates(deduplicated_entities)\n",
    "\n",
    "# Enhance entities with seed data information\n",
    "for entity in merged_entities:\n",
    "    for seed_item in seed_data:\n",
    "        if entity.text.lower() == seed_item[\"text\"].lower():\n",
    "            entity.description = seed_item.get(\"description\", entity.description)\n",
    "            break\n",
    "\n",
    "print(f\"Deduplicated {len(all_entities)} entities to {len(merged_entities)} unique entities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Financial Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    resolve_conflicts=True,\n",
    "    entity_resolution_strategy=\"fuzzy\"\n",
    ")\n",
    "\n",
    "kg_sources = [{\n",
    "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in merged_entities],\n",
    "    \"relationships\": [{\"source\": r.source, \"target\": r.target, \"type\": r.label, \"confidence\": r.confidence} for r in all_relationships]\n",
    "}]\n",
    "\n",
    "kg = graph_builder.build(kg_sources)\n",
    "\n",
    "entities_count = len(kg.get('entities', []))\n",
    "relationships_count = len(kg.get('relationships', []))\n",
    "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings for Companies and Stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    provider=\"sentence_transformers\",\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "company_texts = [f\"{c.text} {getattr(c, 'description', '')}\" for c in companies]\n",
    "company_embeddings = embedding_gen.generate_embeddings(company_texts)\n",
    "\n",
    "market_texts = [f\"{m.text} {getattr(m, 'description', '')}\" for m in markets]\n",
    "market_embeddings = embedding_gen.generate_embeddings(market_texts)\n",
    "\n",
    "print(f\"Generated {len(company_embeddings)} company embeddings and {len(market_embeddings)} market embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "company_ids = vector_store.store_vectors(\n",
    "    vectors=company_embeddings,\n",
    "    metadata=[{\"type\": \"company\", \"name\": c.text, \"label\": c.label} for c in companies]\n",
    ")\n",
    "\n",
    "market_ids = vector_store.store_vectors(\n",
    "    vectors=market_embeddings,\n",
    "    metadata=[{\"type\": \"market\", \"name\": m.text, \"label\": m.label} for m in markets]\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(company_ids)} company vectors and {len(market_ids)} market vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Market Network Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphAnalyzer, CentralityCalculator\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "centrality_calc = CentralityCalculator()\n",
    "\n",
    "analysis = graph_analyzer.analyze_graph(kg)\n",
    "\n",
    "degree_centrality = centrality_calc.calculate_degree_centrality(kg)\n",
    "betweenness_centrality = centrality_calc.calculate_betweenness_centrality(kg)\n",
    "closeness_centrality = centrality_calc.calculate_closeness_centrality(kg)\n",
    "\n",
    "# Identify central companies/stocks in the market network\n",
    "central_companies = []\n",
    "for entity in kg.get(\"entities\", []):\n",
    "    if entity.get(\"type\") in [\"Company\", \"Stock\"]:\n",
    "        entity_id = entity.get(\"id\")\n",
    "        if entity_id in degree_centrality:\n",
    "            central_companies.append({\n",
    "                \"name\": entity.get(\"text\", \"Unknown\"),\n",
    "                \"degree\": degree_centrality[entity_id]\n",
    "            })\n",
    "\n",
    "central_companies.sort(key=lambda x: x['degree'], reverse=True)\n",
    "\n",
    "print(f\"Graph analytics:\")\n",
    "print(f\"  - Graph density: {analysis.get('density', 0):.3f}\")\n",
    "print(f\"  - Central nodes (degree): {len(degree_centrality)}\")\n",
    "print(f\"  - Top central companies: {len(central_companies[:5])} identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG: Hybrid Vector + Graph Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import AgentContext\n",
    "\n",
    "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
    "\n",
    "query = \"What technology companies are in the market?\"\n",
    "results = context.retrieve(\n",
    "    query,\n",
    "    max_results=10,\n",
    "    use_graph=True,\n",
    "    expand_graph=True,\n",
    "    include_entities=True,\n",
    "    include_relationships=True\n",
    ")\n",
    "\n",
    "print(f\"GraphRAG query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(results)} results:\\n\")\n",
    "for i, result in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
    "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
    "    if result.get('related_entities'):\n",
    "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Financial Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "visualizer.visualize(\n",
    "    kg,\n",
    "    output_path=\"financial_data_kg.html\",\n",
    "    layout=\"spring\",\n",
    "    node_size=20\n",
    ")\n",
    "\n",
    "print(\"Visualization saved to financial_data_kg.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "\n",
    "exporter = GraphExporter()\n",
    "exporter.export(kg, output_path=\"financial_data_kg.json\", format=\"json\")\n",
    "exporter.export(kg, output_path=\"financial_data_kg.graphml\", format=\"graphml\")\n",
    "exporter.export(kg, output_path=\"financial_data_market.csv\", format=\"csv\")\n",
    "\n",
    "print(\"Exported financial knowledge graph to JSON, GraphML, and CSV formats\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

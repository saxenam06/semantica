{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/finance/02_Fraud_Detection.ipynb)\n",
        "\n",
        "# Fraud Detection - Temporal KGs & Pattern Detection\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **fraud detection** using Semantica with focus on **temporal knowledge graphs**, **anomaly detection**, and **pattern recognition**. The pipeline analyzes transaction streams using temporal knowledge graphs to detect fraud patterns and anomalies in real-time.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Temporal Knowledge Graphs**: Builds temporal KGs to track transaction patterns over time\n",
        "- **Pattern Detection**: Uses TemporalPatternDetector and reasoning for fraud detection\n",
        "- **Anomaly Detection**: Uses graph-based pattern recognition to identify fraud\n",
        "- **Conflict Detection**: Detects conflicting transaction data from multiple sources\n",
        "- **Real-Time Stream Processing**: Demonstrates real-time transaction stream processing\n",
        "- **Comprehensive Data Sources**: Multiple transaction streams, APIs, and fraud databases\n",
        "- **Modular Architecture**: Direct use of Semantica modules without core orchestrator\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Ingest transaction data from streams and APIs\n",
        "- Extract transaction entities (Transactions, Accounts, Devices, Patterns, Anomalies)\n",
        "- Build temporal transaction knowledge graphs\n",
        "- Perform temporal queries and pattern detection\n",
        "- Detect fraud patterns using graph reasoning\n",
        "- Analyze transaction networks using graph analytics\n",
        "- Store and query transaction data using vector stores\n",
        "\n",
        "### Pipeline Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Data Ingestion] --> B[Document Parsing]\n",
        "    B --> C[Text Processing]\n",
        "    C --> D[Entity Extraction]\n",
        "    D --> E[Relationship Extraction]\n",
        "    E --> F[Deduplication]\n",
        "    F --> G[Conflict Detection]\n",
        "    G --> H[Temporal Knowledge Graph]\n",
        "    H --> I[Embeddings]\n",
        "    I --> J[Vector Store]\n",
        "    H --> K[Temporal Queries]\n",
        "    K --> L[Temporal Pattern Detection]\n",
        "    L --> M[Reasoning & Fraud]\n",
        "    M --> N[Graph Analytics]\n",
        "    J --> O[GraphRAG Queries]\n",
        "    N --> O\n",
        "    O --> P[Visualization]\n",
        "    P --> Q[Export]\n",
        "```\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key-here\")\n",
        "\n",
        "# Configuration constants\n",
        "EMBEDDING_DIMENSION = 384\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 50\n",
        "TEMPORAL_GRANULARITY = \"minute\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingesting Transaction Data from Streams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import StreamIngestor, WebIngestor, FileIngestor\n",
        "import os\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Example: Ingest from transaction stream (simulated Kafka)\n",
        "# In production: stream_ingestor = StreamIngestor()\n",
        "# stream_documents = stream_ingestor.ingest(\"kafka://localhost:9092/transactions\", method=\"kafka\")\n",
        "\n",
        "# Example: Ingest from payment processor API\n",
        "payment_api = \"https://api.example.com/transactions\"  # Example API endpoint\n",
        "all_documents = []\n",
        "\n",
        "try:\n",
        "    web_ingestor = WebIngestor()\n",
        "    with redirect_stderr(StringIO()):\n",
        "        api_documents = web_ingestor.ingest(payment_api, method=\"url\")\n",
        "    for doc in api_documents:\n",
        "        if not hasattr(doc, 'metadata'):\n",
        "            doc.metadata = {}\n",
        "        doc.metadata['source'] = 'Payment API'\n",
        "        all_documents.append(doc)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if not all_documents:\n",
        "    tx_data = \"\"\"\n",
        "    2024-01-01 10:00:00 - Transaction $1000 from Account A123 to Account B456\n",
        "    2024-01-01 10:01:00 - Transaction $5000 from Account A123 to Account C789\n",
        "    2024-01-01 10:02:00 - Transaction $10000 from Account A123 to Account D012 (unusual pattern)\n",
        "    2024-01-01 10:03:00 - Multiple rapid transactions from Account A123 (suspicious)\n",
        "    2024-01-01 10:04:00 - Transaction $2000 from Account B456 to Account E789\n",
        "    2024-01-01 10:05:00 - Large transaction $50000 from Account A123 to Account F012 (fraud alert)\n",
        "    2024-01-01 10:06:00 - Transaction $1500 from Account C789 to Account G345\n",
        "    2024-01-01 10:07:00 - Unusual device login from Account A123 (suspicious activity)\n",
        "    \"\"\"\n",
        "    with open(\"data/transactions.txt\", \"w\") as f:\n",
        "        f.write(tx_data)\n",
        "    file_ingestor = FileIngestor()\n",
        "    all_documents = file_ingestor.ingest(\"data/transactions.txt\")\n",
        "\n",
        "documents = all_documents\n",
        "print(f\"Ingested {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parsing Transaction Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "print(f\"Parsing {len(documents)} documents...\")\n",
        "parsed_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    try:\n",
        "        parsed = parser.parse(\n",
        "            doc.content if hasattr(doc, 'content') else str(doc),\n",
        "            content_type=\"text\"\n",
        "        )\n",
        "        parsed_documents.append(parsed)\n",
        "    except Exception:\n",
        "        parsed_documents.append(doc)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
        "\n",
        "documents = parsed_documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing and Chunking Transaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "# Use sentence chunking for transaction logs\n",
        "splitter = TextSplitter(method=\"sentence\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "print(f\"Normalizing {len(documents)} documents...\")\n",
        "normalized_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        normalize_numbers=True,\n",
        "        remove_extra_whitespace=True,\n",
        "        lowercase=False\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Normalized {i}/{len(documents)} documents...\")\n",
        "\n",
        "print(f\"Chunking {len(normalized_documents)} documents...\")\n",
        "chunked_documents = []\n",
        "for i, doc_text in enumerate(normalized_documents, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    except Exception:\n",
        "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "        chunks = simple_splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    if i % 50 == 0 or i == len(normalized_documents):\n",
        "        print(f\"  Chunked {i}/{len(normalized_documents)} documents ({len(chunked_documents)} chunks so far)\")\n",
        "\n",
        "print(f\"Created {len(chunked_documents)} chunks from {len(normalized_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor\n",
        "\n",
        "entity_extractor = NERExtractor(\n",
        "    method=\"llm\",\n",
        "    provider=\"groq\",\n",
        "    llm_model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "all_entities = []\n",
        "print(f\"Extracting entities from {len(chunked_documents)} chunks...\")\n",
        "for i, chunk in enumerate(chunked_documents, 1):\n",
        "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    try:\n",
        "        entities = entity_extractor.extract_entities(\n",
        "            chunk_text,\n",
        "            entity_types=[\"Transaction\", \"Account\", \"Device\", \"Pattern\", \"Anomaly\"]\n",
        "        )\n",
        "        all_entities.extend(entities)\n",
        "    except Exception:\n",
        "        continue\n",
        "    \n",
        "    if i % 20 == 0 or i == len(chunked_documents):\n",
        "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_entities)} entities found)\")\n",
        "\n",
        "transactions = [e for e in all_entities if e.label == \"Transaction\" or \"transaction\" in e.label.lower()]\n",
        "accounts = [e for e in all_entities if e.label == \"Account\" or \"account\" in e.label.lower()]\n",
        "anomalies = [e for e in all_entities if e.label in [\"Anomaly\", \"Pattern\"] or \"anomaly\" in e.label.lower() or \"pattern\" in e.label.lower()]\n",
        "\n",
        "print(f\"Extracted {len(transactions)} transactions, {len(accounts)} accounts, {len(anomalies)} anomalies/patterns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import RelationExtractor\n",
        "\n",
        "relation_extractor = RelationExtractor(\n",
        "    method=\"llm\",\n",
        "    provider=\"groq\",\n",
        "    llm_model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "all_relationships = []\n",
        "print(f\"Extracting relationships from {len(chunked_documents)} chunks...\")\n",
        "for i, chunk in enumerate(chunked_documents, 1):\n",
        "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    try:\n",
        "        relationships = relation_extractor.extract_relations(\n",
        "            chunk_text,\n",
        "            entities=all_entities,\n",
        "            relation_types=[\"from\", \"to\", \"triggers\", \"detects\", \"associated_with\", \"causes\"]\n",
        "        )\n",
        "        all_relationships.extend(relationships)\n",
        "    except Exception:\n",
        "        continue\n",
        "    \n",
        "    if i % 20 == 0 or i == len(chunked_documents):\n",
        "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_relationships)} relationships found)\")\n",
        "\n",
        "print(f\"Extracted {len(all_relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolving Duplicate Transactions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import EntityResolver\n",
        "from semantica.semantic_extract import Entity\n",
        "\n",
        "# Convert Entity objects to dictionaries for EntityResolver\n",
        "print(f\"Converting {len(all_entities)} entities to dictionaries...\")\n",
        "entity_dicts = [{\"name\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in all_entities]\n",
        "\n",
        "# Use EntityResolver class to resolve duplicates\n",
        "entity_resolver = EntityResolver(strategy=\"fuzzy\", similarity_threshold=0.85)\n",
        "\n",
        "print(f\"Resolving duplicates in {len(entity_dicts)} entities...\")\n",
        "resolved_entities = entity_resolver.resolve_entities(entity_dicts)\n",
        "\n",
        "# Convert back to Entity objects\n",
        "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
        "merged_entities = [\n",
        "    Entity(text=e[\"name\"], label=e[\"type\"], confidence=e.get(\"confidence\", 1.0))\n",
        "    for e in resolved_entities\n",
        "]\n",
        "\n",
        "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Transaction Conflicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
        "\n",
        "# Use logical conflict detection for fraud rules\n",
        "# expert_review strategy flags conflicts for manual review by fraud analysts\n",
        "conflict_detector = ConflictDetector()\n",
        "conflict_resolver = ConflictResolver()\n",
        "\n",
        "print(f\"Detecting logical conflicts in {len(merged_entities)} entities and relationships...\")\n",
        "conflicts = conflict_detector.detect_conflicts(\n",
        "    entities=merged_entities,\n",
        "    relationships=all_relationships,\n",
        "    method=\"logical\"  # Detect logical conflicts (e.g., conflicting fraud indicators)\n",
        ")\n",
        "\n",
        "print(f\"Detected {len(conflicts)} logical conflicts\")\n",
        "\n",
        "if conflicts:\n",
        "    print(f\"Resolving conflicts using expert_review strategy...\")\n",
        "    resolved = conflict_resolver.resolve_conflicts(\n",
        "        conflicts,\n",
        "        strategy=\"expert_review\"  # Manual review by fraud analysts\n",
        "    )\n",
        "    print(f\"Resolved {len(resolved)} conflicts (flagged for expert review)\")\n",
        "else:\n",
        "    print(\"No conflicts detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Temporal Transaction Knowledge Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphBuilder\n",
        "\n",
        "graph_builder = GraphBuilder(\n",
        "    merge_entities=True,\n",
        "    resolve_conflicts=True,\n",
        "    entity_resolution_strategy=\"fuzzy\",\n",
        "    enable_temporal=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY\n",
        ")\n",
        "\n",
        "print(f\"Building knowledge graph...\")\n",
        "kg_sources = [{\n",
        "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": e.confidence} for e in merged_entities],\n",
        "    \"relationships\": [{\"source\": r.source, \"target\": r.target, \"type\": r.label, \"confidence\": r.confidence} for r in all_relationships]\n",
        "}]\n",
        "\n",
        "kg = graph_builder.build(kg_sources)\n",
        "\n",
        "entities_count = len(kg.get('entities', []))\n",
        "relationships_count = len(kg.get('relationships', []))\n",
        "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Transactions and Accounts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "\n",
        "embedding_gen = EmbeddingGenerator(\n",
        "    provider=\"sentence_transformers\",\n",
        "    model=EMBEDDING_MODEL\n",
        ")\n",
        "\n",
        "print(f\"Generating embeddings for {len(transactions)} transactions and {len(accounts)} accounts...\")\n",
        "transaction_texts = [t.text for t in transactions]\n",
        "transaction_embeddings = embedding_gen.generate_embeddings(transaction_texts)\n",
        "\n",
        "account_texts = [a.text for a in accounts]\n",
        "account_embeddings = embedding_gen.generate_embeddings(account_texts)\n",
        "\n",
        "print(f\"Generated {len(transaction_embeddings)} transaction embeddings and {len(account_embeddings)} account embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Populating Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
        "\n",
        "print(f\"Storing {len(transaction_embeddings)} transaction vectors and {len(account_embeddings)} account vectors...\")\n",
        "transaction_ids = vector_store.store_vectors(\n",
        "    vectors=transaction_embeddings,\n",
        "    metadata=[{\"type\": \"transaction\", \"name\": t.text, \"label\": t.label} for t in transactions]\n",
        ")\n",
        "\n",
        "account_ids = vector_store.store_vectors(\n",
        "    vectors=account_embeddings,\n",
        "    metadata=[{\"type\": \"account\", \"name\": a.text, \"label\": a.label} for a in accounts]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(transaction_ids)} transaction vectors and {len(account_ids)} account vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalGraphQuery\n",
        "\n",
        "temporal_query = TemporalGraphQuery(\n",
        "    enable_temporal_reasoning=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY\n",
        ")\n",
        "\n",
        "query_results = temporal_query.query_at_time(\n",
        "    kg,\n",
        "    query={\"type\": \"Transaction\"},\n",
        "    at_time=\"2024-01-01 10:05:00\"\n",
        ")\n",
        "\n",
        "evolution = temporal_query.analyze_evolution(kg)\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Temporal queries: {len(query_results)} transactions at query time\")\n",
        "print(f\"Temporal patterns detected: {len(temporal_patterns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Pattern Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalPatternDetector\n",
        "\n",
        "pattern_detector = TemporalPatternDetector()\n",
        "\n",
        "# Detect temporal fraud patterns\n",
        "fraud_patterns = pattern_detector.detect_patterns(kg, pattern_type=\"fraud\")\n",
        "\n",
        "# Detect sequence patterns (rapid transactions, unusual timing)\n",
        "sequence_patterns = pattern_detector.detect_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Detected {len(fraud_patterns)} fraud patterns\")\n",
        "print(f\"Detected {len(sequence_patterns)} sequence patterns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reasoning and Fraud Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.reasoning import Reasoner\n",
        "\n",
        "reasoner = Reasoner()\n",
        "\n",
        "reasoner.add_rule(\"IF Account from Transaction AND Transaction amount > 10000 AND Transaction count > 3 THEN Account triggers Anomaly\")\n",
        "reasoner.add_rule(\"IF Transaction from Account AND Account triggers Anomaly THEN Transaction associated_with Pattern\")\n",
        "\n",
        "inferred_facts = reasoner.infer_facts(kg)\n",
        "\n",
        "fraud_paths = reasoner.find_paths(\n",
        "    kg,\n",
        "    source_type=\"Account\",\n",
        "    target_type=\"Anomaly\",\n",
        "    max_hops=2\n",
        ")\n",
        "\n",
        "print(f\"Inferred {len(inferred_facts)} facts\")\n",
        "print(f\"Found {len(fraud_paths)} fraud paths\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Transaction Network Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphAnalyzer, CommunityDetector\n",
        "\n",
        "graph_analyzer = GraphAnalyzer()\n",
        "community_detector = CommunityDetector()\n",
        "\n",
        "analysis = graph_analyzer.analyze_graph(kg)\n",
        "\n",
        "communities = community_detector.detect_communities(kg, method=\"louvain\")\n",
        "connectivity = graph_analyzer.analyze_connectivity(kg)\n",
        "\n",
        "# Detect suspicious account communities\n",
        "suspicious_communities = []\n",
        "for community in communities:\n",
        "    community_accounts = [e for e in kg.get(\"entities\", []) \n",
        "                          if e.get(\"id\") in community and e.get(\"type\") == \"Account\"]\n",
        "    if len(community_accounts) > 0:\n",
        "        # Check if community has suspicious patterns\n",
        "        suspicious_communities.append({\n",
        "            \"community_id\": len(suspicious_communities),\n",
        "            \"account_count\": len(community_accounts)\n",
        "        })\n",
        "\n",
        "print(f\"Graph analytics:\")\n",
        "print(f\"  - Communities: {len(communities)}\")\n",
        "print(f\"  - Connected components: {len(connectivity.get('components', []))}\")\n",
        "print(f\"  - Graph density: {analysis.get('density', 0):.3f}\")\n",
        "print(f\"  - Suspicious communities: {len(suspicious_communities)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GraphRAG: Hybrid Vector + Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "query = \"What accounts have suspicious transaction patterns?\"\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,\n",
        "    expand_graph=True,\n",
        "    include_entities=True,\n",
        "    include_relationships=True\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG query: '{query}'\")\n",
        "print(f\"\\nRetrieved {len(results)} results:\\n\")\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Temporal Fraud Detection Knowledge Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"fraud_detection_kg.html\",\n",
        "    layout=\"temporal\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Visualization saved to fraud_detection_kg.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import GraphExporter\n",
        "\n",
        "exporter = GraphExporter()\n",
        "exporter.export(kg, output_path=\"fraud_detection_kg.json\", format=\"json\")\n",
        "exporter.export(kg, output_path=\"fraud_detection_kg.graphml\", format=\"graphml\")\n",
        "exporter.export(kg, output_path=\"fraud_detection_alerts.csv\", format=\"csv\")\n",
        "\n",
        "print(\"Exported fraud detection knowledge graph to JSON, GraphML, and CSV formats\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

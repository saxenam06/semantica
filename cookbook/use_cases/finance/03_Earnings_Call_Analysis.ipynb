{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/finance/03_Earnings_Call_Analysis.ipynb)\n",
    "\n",
    "# Earnings Call Analysis with Docling and Semantica: MDA Space Q3 2025\n",
    "\n",
    "##  Data Sources\n",
    "\n",
    "This analysis dual-tracks two critical documents from **MDA Space Ltd.** for the third quarter of 2025 to provide a holistic view of the company's performance and strategy:\n",
    "1. **Press Release:** A structured summary of financial results, strategic milestones, and executive commentary.\n",
    "2. **Earnings Transcript:** A detailed record of the management presentation and the subsequent Q&A session with analysts, providing deeper qualitative context.\n",
    "\n",
    "###  Q3 2025 Strategic Highlights\n",
    "* **Robust Backlog:** The company maintained a significant backlog at quarter-end, providing strong revenue visibility and supporting long-term growth objectives.\n",
    "* **Strong Revenue Growth:** Substantial year-over-year revenue expansion driven by successful execution across major programs and satellite system deliveries.\n",
    "* **Profitability & Margins:** Strong operational performance led to a significant increase in Adjusted EBITDA and healthy overall margin expansion.\n",
    "* **Earnings Performance:** Noteworthy growth in adjusted net income and earnings per share, reflecting scaled operations and improved bottom-line efficiency.\n",
    "* **Cash Flow & Capital Structure:** Positive operating cash flow and a healthy balance sheet with a very low leverage ratio, positioning the company well for future investment.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to extract deep insights from unstructured financial documents. By combining **Docling** for high-fidelity parsing with **Semantica** for knowledge engineering, we build a structured semantic layer that enables complex analysis and advanced GraphRAG capabilities.\n",
    "\n",
    "**Workflow:** `Dual PDF Input ‚Üí Docling Parsing ‚Üí Entity & Relation Extraction ‚Üí Knowledge Graph Construction ‚Üí GraphRAG ‚Üí Strategic Q&A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU semantica docling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Groq LLM initialized: llama-3.1-8b-instant\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM provider\n",
    "from semantica.llms import Groq\n",
    "import os\n",
    "\n",
    "GROQ_API_KEY = \"\"\n",
    "    \n",
    "groq_llm = Groq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\", GROQ_API_KEY))\n",
    "\n",
    "print(f\"‚úì Groq LLM initialized: {groq_llm.model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parse PDF with Docling\n",
    "\n",
    "Parse earnings call PDF and extract financial tables using DoclingParser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>Progress</th><th>ETA</th><th>Rate</th><th>Time</th></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DoclingParser</td><td>-</td><td>-</td><td>-</td><td>0.01s</td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Semantica is parsing: Docling: mda_space_q3_2025_press_release.pdf üîÑüîç (0.0s)                                       "
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transcript_pdf\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     20\u001b[0m     transcript_pdf\u001b[38;5;241m.\u001b[39mwrite_bytes(requests\u001b[38;5;241m.\u001b[39mget(transcript_url)\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m---> 22\u001b[0m press_release \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpress_release_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m transcript \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(transcript_pdf)\n\u001b[0;32m     25\u001b[0m parsed_doc \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Press Release\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpress_release[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Transcript\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtranscript[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m\"\u001b[39m: press_release[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m transcript[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     }\n\u001b[0;32m     34\u001b[0m }\n",
      "File \u001b[1;32m~\\semantica\\semantica\\parse\\docling_parser.py:137\u001b[0m, in \u001b[0;36mDoclingParser.parse\u001b[1;34m(self, file_path, **options)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DOCLING_AVAILABLE:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DOCLING_IMPORT_ERROR:\n\u001b[1;32m--> 137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(DOCLING_IMPORT_ERROR)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocling is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from semantica.parse import DoclingParser\n",
    "\n",
    "parser = DoclingParser()\n",
    "\n",
    "press_release_url = \"https://filecache.investorroom.com/mr5ircnw_mda/677/MDA_Space_Ltd_Q3_2025_Press_Release_Nov_14_2025_FINAL.pdf\"\n",
    "transcript_url = \"https://filecache.investorroom.com/mr5ircnw_mda/681/MDA%20Space%20Ltd.%20Q3%202025%20Earnings%20Conference%20Call%20Transcript%20%28November%2014%202025%29.pdf\"\n",
    "\n",
    "download_dir = Path(\"downloads\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "press_release_pdf = download_dir / \"mda_space_q3_2025_press_release.pdf\"\n",
    "transcript_pdf = download_dir / \"mda_space_q3_2025_transcript.pdf\"\n",
    "\n",
    "if not press_release_pdf.exists():\n",
    "    press_release_pdf.write_bytes(requests.get(press_release_url).content)\n",
    "\n",
    "if not transcript_pdf.exists():\n",
    "    transcript_pdf.write_bytes(requests.get(transcript_url).content)\n",
    "\n",
    "press_release = parser.parse(press_release_pdf)\n",
    "transcript = parser.parse(transcript_pdf)\n",
    "\n",
    "parsed_doc = {\n",
    "    \"full_text\": f\"# Press Release\\n\\n{press_release['full_text']}\\n\\n# Transcript\\n\\n{transcript['full_text']}\",\n",
    "    \"tables\": press_release['tables'] + transcript['tables'],\n",
    "    \"metadata\": {\n",
    "        \"title\": \"MDA Space Ltd. Q3 2025 Earnings Analysis\",\n",
    "        \"company\": \"MDA Space Ltd.\",\n",
    "        \"quarter\": \"Q3 2025\",\n",
    "        \"date\": \"November 14, 2025\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Normalize Text\n",
    "\n",
    "Normalize extracted text using TextNormalizer for consistent processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Semantica is parsing: Docling: mda_space_q3_2025_press_release.pdf üîÑüîç (0.0s) | üß† Normalizing text üîÑüîß (0.0s)     ‚úì Text normalized: 0 characters\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Normalize text with Semantica\n",
    "from semantica.normalize import TextNormalizer\n",
    "\n",
    "text_normalizer = TextNormalizer()\n",
    "normalized_text = text_normalizer.normalize(\n",
    "    parsed_doc[\"full_text\"],\n",
    "    case=\"lower\",\n",
    "    remove_extra_whitespace=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì Text normalized: {len(normalized_text)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Entities\n",
    "\n",
    "Extract entities (organizations, people, financial terms) using NERExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Entities extracted: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Extract entities using NERExtractor with Groq\n",
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "ner = NERExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    min_confidence=0.7\n",
    ")\n",
    "\n",
    "entities = ner.extract_entities(\n",
    "    normalized_text,\n",
    "    entity_types=[\"ORG\", \"PERSON\", \"MONEY\", \"DATE\", \"PERCENT\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Entities extracted: {len(entities)}\")\n",
    "if entities:\n",
    "    print(f\"  Sample: {entities[0].text} ({entities[0].label})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Financial Metrics\n",
    "\n",
    "Extract financial metrics (money, percentages, dates) from text and tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Financial entities: 0\n",
      "  Financial metrics: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract financial metrics using NERExtractor\n",
    "financial_entities = ner.extract_entities(\n",
    "    normalized_text,\n",
    "    entity_types=[\"MONEY\", \"PERCENT\", \"DATE\"]\n",
    ")\n",
    "\n",
    "financial_metrics = {}\n",
    "for entity in financial_entities:\n",
    "    if entity.label == \"MONEY\":\n",
    "        financial_metrics[entity.text] = entity.text\n",
    "\n",
    "print(f\"‚úì Financial entities: {len(financial_entities)}\")\n",
    "print(f\"  Financial metrics: {len(financial_metrics)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Relationships\n",
    "\n",
    "Extract relationships between entities using RelationExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Relationships extracted: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Extract relationships using RelationExtractor with Groq LLM\n",
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"llm\",\n",
    "    confidence_threshold=0.6,\n",
    "    relation_types=[\"HAS_REVENUE\", \"HAS_EPS\", \"STATES\", \"PROVIDES_GUIDANCE\", \"OPERATES_IN\"]\n",
    ")\n",
    "\n",
    "relationships = relation_extractor.extract_relations(\n",
    "    normalized_text,\n",
    "    entities=entities,\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Relationships extracted: {len(relationships)}\")\n",
    "if relationships:\n",
    "    rel = relationships[0]\n",
    "    print(f\"  Sample: {rel.subject.text} ‚Üí {rel.predicate} ‚Üí {rel.object.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract RDF Triplets\n",
    "\n",
    "Extract RDF triplets (subject-predicate-object) using TripletExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Normalizing text üîÑüîß (0.0s) | üß† Semantica is extracting: Extracting triplets using llm... (1/1, remaining: 0 methods) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% [1/1] üîÑüéØ (61.7/s)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method llm failed: groq provider not available\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m      4\u001b[0m triplet_extractor \u001b[38;5;241m=\u001b[39m TripletExtractor(\n\u001b[0;32m      5\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     include_temporal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     include_provenance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m triplets \u001b[38;5;241m=\u001b[39m triplet_extractor\u001b[38;5;241m.\u001b[39mextract_triplets(\n\u001b[0;32m     11\u001b[0m     normalized_text,\n\u001b[0;32m     12\u001b[0m     entities\u001b[38;5;241m=\u001b[39mentities,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     llm_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-8b-instant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m validated_triplets \u001b[38;5;241m=\u001b[39m \u001b[43mtriplet_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_triplets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì RDF triplets extracted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(triplets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m triplets:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "# Step 6: Extract RDF triplets using TripletExtractor with Groq LLM\n",
    "from semantica.semantic_extract import TripletExtractor\n",
    "\n",
    "triplet_extractor = TripletExtractor(\n",
    "    method=\"llm\",\n",
    "    include_temporal=True,\n",
    "    include_provenance=True\n",
    ")\n",
    "\n",
    "triplets = triplet_extractor.extract_triplets(\n",
    "    normalized_text,\n",
    "    entities=entities,\n",
    "    relations=relationships,\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "validated_triplets = triplet_extractor.validate_triplets(triplets)\n",
    "\n",
    "print(f\"‚úì RDF triplets extracted: {len(triplets)}\")\n",
    "if triplets:\n",
    "    t = triplets[0]\n",
    "    print(f\"  Sample: {t.subject} ‚Üí {t.predicate} ‚Üí {t.object}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Detect Conflicts\n",
    "\n",
    "Detect conflicts in extracted entities and relationships using ConflictDetector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Detect conflicts in extracted entities and relationships\n",
    "from semantica.conflicts import ConflictDetector, SourceTracker, SourceReference\n",
    "\n",
    "source_tracker = SourceTracker()\n",
    "conflict_detector = ConflictDetector(\n",
    "    source_tracker=source_tracker,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "# Track sources for entities\n",
    "for entity in entities:\n",
    "    entity_id = getattr(entity, 'id', None) or getattr(entity, 'text', '')\n",
    "    entity_name = getattr(entity, 'text', '')\n",
    "    source_tracker.track_property_source(\n",
    "        entity_id,\n",
    "        'name',\n",
    "        entity_name,\n",
    "        source=SourceReference(\n",
    "            source='earnings_call',\n",
    "            timestamp='2024-Q1',\n",
    "            metadata={'entity_type': getattr(entity, 'label', 'UNKNOWN')}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Detect value conflicts\n",
    "value_conflicts = conflict_detector.detect_value_conflicts(\n",
    "    [{'id': getattr(e, 'id', ''), 'name': getattr(e, 'text', '')} for e in entities],\n",
    "    property_name='name'\n",
    ")\n",
    "\n",
    "# Detect relationship conflicts\n",
    "relationship_conflicts = conflict_detector.detect_relationship_conflicts(relationships)\n",
    "\n",
    "print(f\"‚úì Conflicts detected\")\n",
    "print(f\"  Value conflicts: {len(value_conflicts)}\")\n",
    "print(f\"  Relationship conflicts: {len(relationship_conflicts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Resolve Conflicts\n",
    "\n",
    "Resolve detected conflicts using ConflictResolver with voting strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Resolve conflicts using ConflictResolver\n",
    "from semantica.conflicts import ConflictResolver\n",
    "\n",
    "conflict_resolver = ConflictResolver(\n",
    "    default_strategy='voting',\n",
    "    source_tracker=source_tracker\n",
    ")\n",
    "\n",
    "# Resolve value conflicts\n",
    "resolved_entities = list(entities)\n",
    "resolved_conflicts = []\n",
    "for conflict in value_conflicts:\n",
    "    resolution = conflict_resolver.resolve_conflict(conflict, strategy='voting')\n",
    "    resolved_conflicts.append(resolution)\n",
    "\n",
    "# Resolve relationship conflicts\n",
    "resolved_relationships = list(relationships)\n",
    "for conflict in relationship_conflicts:\n",
    "    resolution = conflict_resolver.resolve_conflict(conflict, strategy='voting')\n",
    "    resolved_conflicts.append(resolution)\n",
    "\n",
    "print(f\"‚úì Conflicts resolved: {len(resolved_conflicts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deduplicate Entities\n",
    "\n",
    "Detect and merge duplicate entities using DuplicateDetector and EntityMerger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Deduplicate entities using DuplicateDetector and EntityMerger\n",
    "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
    "\n",
    "duplicate_detector = DuplicateDetector(\n",
    "    similarity_threshold=0.8,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "# Convert entities to dict format\n",
    "entity_dicts = []\n",
    "for entity in resolved_entities:\n",
    "    entity_dicts.append({\n",
    "        'id': getattr(entity, 'id', ''),\n",
    "        'name': getattr(entity, 'text', ''),\n",
    "        'type': getattr(entity, 'label', 'UNKNOWN'),\n",
    "        'confidence': getattr(entity, 'confidence', 1.0),\n",
    "        'metadata': getattr(entity, 'metadata', {})\n",
    "    })\n",
    "\n",
    "# Detect duplicates\n",
    "duplicates = duplicate_detector.detect_duplicates(entity_dicts)\n",
    "\n",
    "# Merge duplicates\n",
    "entity_merger = EntityMerger(preserve_provenance=True)\n",
    "merge_operations = entity_merger.merge_duplicates(\n",
    "    entity_dicts,\n",
    "    strategy='keep_most_complete'\n",
    ")\n",
    "\n",
    "merged_entities = [op.merged_entity for op in merge_operations]\n",
    "\n",
    "print(f\"‚úì Deduplication complete\")\n",
    "print(f\"  Original entities: {len(entity_dicts)}\")\n",
    "print(f\"  Merged entities: {len(merged_entities)}\")\n",
    "print(f\"  Duplicates removed: {len(entity_dicts) - len(merged_entities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Build Knowledge Graph\n",
    "\n",
    "Build knowledge graph from cleaned entities, relationships, and triplets using GraphBuilder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Build knowledge graph from cleaned entities, resolved relationships, and triplets\n",
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    entity_resolution_strategy=\"fuzzy\"\n",
    ")\n",
    "\n",
    "# Convert triplets to relationships format\n",
    "triplet_relationships = []\n",
    "for triplet in triplets:\n",
    "    triplet_relationships.append({\n",
    "        \"source\": triplet.subject,\n",
    "        \"predicate\": triplet.predicate,\n",
    "        \"target\": triplet.object,\n",
    "        \"confidence\": triplet.confidence,\n",
    "        \"metadata\": triplet.metadata\n",
    "    })\n",
    "\n",
    "all_relationships = resolved_relationships + triplet_relationships\n",
    "\n",
    "kg_data = {\n",
    "    \"entities\": merged_entities,\n",
    "    \"relationships\": all_relationships,\n",
    "    \"triplets\": triplets,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"earnings_call_transcript\",\n",
    "        \"financial_metrics\": financial_metrics,\n",
    "        \"extraction_method\": \"Groq LLM\"\n",
    "    }\n",
    "}\n",
    "\n",
    "knowledge_graph = graph_builder.build(\n",
    "    sources=[kg_data],\n",
    "    merge_entities=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì Knowledge graph built\")\n",
    "print(f\"  Entities: {len(knowledge_graph.get('entities', []))}\")\n",
    "print(f\"  Relationships: {len(knowledge_graph.get('relationships', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Analyze Knowledge Graph\n",
    "\n",
    "Analyze graph structure using GraphAnalyzer (centrality, communities, connectivity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Analyze knowledge graph using GraphAnalyzer\n",
    "from semantica.kg import GraphAnalyzer\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "analysis = graph_analyzer.analyze_graph(knowledge_graph)\n",
    "centrality = graph_analyzer.calculate_centrality(knowledge_graph, 'degree')\n",
    "communities = graph_analyzer.detect_communities(knowledge_graph, algorithm='louvain')\n",
    "connectivity = graph_analyzer.analyze_connectivity(knowledge_graph)\n",
    "metrics = graph_analyzer.compute_metrics(knowledge_graph)\n",
    "\n",
    "top_entities = []\n",
    "if centrality and 'rankings' in centrality:\n",
    "    top_entities = centrality['rankings'][:5]\n",
    "\n",
    "num_communities = len(communities.get('communities', [])) if isinstance(communities, dict) else 0\n",
    "\n",
    "print(f\"‚úì Graph analysis complete\")\n",
    "print(f\"  Communities: {num_communities}\")\n",
    "print(f\"  Top entities: {len(top_entities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Build Context Graph\n",
    "\n",
    "Build ContextGraph from knowledge graph for enhanced retrieval and GraphRAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Build context graph for enhanced retrieval\n",
    "from semantica.context import ContextGraph\n",
    "\n",
    "context_graph = ContextGraph(\n",
    "    extract_entities=True,\n",
    "    extract_relationships=True\n",
    ")\n",
    "\n",
    "# Convert knowledge graph to context graph format\n",
    "nodes = []\n",
    "for entity in knowledge_graph.get('entities', []):\n",
    "    nodes.append({\n",
    "        \"id\": entity.get('id', entity.get('name', '')),\n",
    "        \"type\": entity.get('type', 'entity'),\n",
    "        \"properties\": {\n",
    "            \"content\": entity.get('name', ''),\n",
    "            \"confidence\": entity.get('confidence', 1.0),\n",
    "            **entity.get('metadata', {})\n",
    "        }\n",
    "    })\n",
    "\n",
    "edges = []\n",
    "for rel in knowledge_graph.get('relationships', []):\n",
    "    edges.append({\n",
    "        \"source_id\": rel.get('source', ''),\n",
    "        \"target_id\": rel.get('target', ''),\n",
    "        \"type\": rel.get('predicate', 'related_to'),\n",
    "        \"weight\": rel.get('confidence', 1.0)\n",
    "    })\n",
    "\n",
    "node_count = context_graph.add_nodes(nodes)\n",
    "edge_count = context_graph.add_edges(edges)\n",
    "\n",
    "print(f\"‚úì Context graph built\")\n",
    "print(f\"  Nodes: {node_count}\")\n",
    "print(f\"  Edges: {edge_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Context Retrieval\n",
    "\n",
    "Set up hybrid retrieval (vector + graph) using ContextRetriever for GraphRAG queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Set up hybrid context retrieval and demonstrate GraphRAG\n",
    "from semantica.vector_store import VectorStore\n",
    "from semantica.context import ContextRetriever\n",
    "\n",
    "# Initialize VectorStore\n",
    "vector_store = VectorStore(backend=\"faiss\")\n",
    "vector_store.add(\n",
    "    texts=[parsed_doc[\"full_text\"]],\n",
    "    metadata=[{\"source\": \"earnings_call\", \"type\": \"transcript\"}]\n",
    ")\n",
    "\n",
    "# Initialize ContextRetriever\n",
    "context_retriever = ContextRetriever(\n",
    "    knowledge_graph=context_graph,\n",
    "    vector_store=vector_store,\n",
    "    hybrid_alpha=0.6,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2\n",
    ")\n",
    "\n",
    "# Retrieve context for financial queries\n",
    "financial_queries = [\n",
    "    \"What was the company's revenue guidance?\",\n",
    "    \"What were the key financial metrics discussed?\"\n",
    "]\n",
    "\n",
    "retrieved_contexts = []\n",
    "for query in financial_queries:\n",
    "    results = context_retriever.retrieve(\n",
    "        query=query,\n",
    "        max_results=3,\n",
    "        min_relevance_score=0.2\n",
    "    )\n",
    "    retrieved_contexts.append({\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Hybrid retrieval configured\")\n",
    "print(f\"  Queries processed: {len(retrieved_contexts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Entity Linking\n",
    "\n",
    "Link entities across sources and assign URIs using EntityLinker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Link entities using EntityLinker\n",
    "from semantica.context import EntityLinker\n",
    "\n",
    "entity_linker = EntityLinker(knowledge_graph=knowledge_graph)\n",
    "\n",
    "# Assign URIs to key entities\n",
    "linked_entities = []\n",
    "for entity in merged_entities[:10]:\n",
    "    entity_id = entity.get('id', entity.get('name', ''))\n",
    "    entity_name = entity.get('name', '')\n",
    "    entity_type = entity.get('type', 'UNKNOWN')\n",
    "    \n",
    "    uri = entity_linker.assign_uri(\n",
    "        entity_id=entity_id,\n",
    "        text=entity_name,\n",
    "        entity_type=entity_type\n",
    "    )\n",
    "    linked_entities.append({\n",
    "        \"entity_id\": entity_id,\n",
    "        \"name\": entity_name,\n",
    "        \"uri\": uri,\n",
    "        \"type\": entity_type\n",
    "    })\n",
    "\n",
    "# Build entity web\n",
    "entity_web = entity_linker.build_entity_web()\n",
    "\n",
    "print(f\"‚úì Entity linking complete\")\n",
    "print(f\"  Entities linked: {len(linked_entities)}\")\n",
    "print(f\"  Entity web nodes: {len(entity_web.get('nodes', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Agent Memory\n",
    "\n",
    "Store and retrieve memories using AgentMemory with RAG integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Store and retrieve memories using AgentMemory\n",
    "from semantica.context import AgentMemory\n",
    "\n",
    "agent_memory = AgentMemory(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    retention_days=30\n",
    ")\n",
    "\n",
    "# Store earnings call memories\n",
    "memory_ids = []\n",
    "memory_contents = [\n",
    "    f\"Earnings call transcript: {parsed_doc['metadata'].get('title', 'Q1 2024')}\",\n",
    "    f\"Financial metrics extracted: {len(financial_metrics)} metrics\",\n",
    "    f\"Key entities identified: {len(merged_entities)} entities\"\n",
    "]\n",
    "\n",
    "for content in memory_contents:\n",
    "    memory_id = agent_memory.store(\n",
    "        content=content,\n",
    "        metadata={\"source\": \"earnings_call\", \"type\": \"transcript_analysis\"},\n",
    "        extract_entities=True,\n",
    "        extract_relationships=True\n",
    "    )\n",
    "    memory_ids.append(memory_id)\n",
    "\n",
    "# Retrieve memories\n",
    "financial_memories = agent_memory.retrieve(\n",
    "    query=\"financial metrics and earnings\",\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "memory_stats = agent_memory.get_statistics()\n",
    "\n",
    "print(f\"‚úì Agent memory configured\")\n",
    "print(f\"  Memories stored: {len(memory_ids)}\")\n",
    "print(f\"  Total memories: {memory_stats.get('total_memories', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Agent Context\n",
    "\n",
    "Unified context management with AgentContext (auto-detects RAG vs GraphRAG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: High-level context management with AgentContext\n",
    "from semantica.context import AgentContext\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=context_graph,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2,\n",
    "    hybrid_alpha=0.6,\n",
    "    retention_days=30\n",
    ")\n",
    "\n",
    "# Store content with auto-extraction\n",
    "memory_id = agent_context.store(\n",
    "    content=parsed_doc[\"full_text\"][:1000],\n",
    "    metadata={\"source\": \"earnings_call\", \"date\": \"2024-Q1\"},\n",
    "    extract_entities=True,\n",
    "    extract_relationships=True,\n",
    "    link_entities=True\n",
    ")\n",
    "\n",
    "# Retrieve with auto-detected GraphRAG\n",
    "graphrag_results = agent_context.retrieve(\n",
    "    query=\"What was discussed about revenue growth?\",\n",
    "    max_results=5,\n",
    "    expand_graph=True,\n",
    "    include_entities=True\n",
    ")\n",
    "\n",
    "context_stats = agent_context.stats()\n",
    "\n",
    "print(f\"‚úì AgentContext configured\")\n",
    "print(f\"  Memory stored: {memory_id}\")\n",
    "print(f\"  GraphRAG results: {len(graphrag_results)}\")\n",
    "print(f\"  Total memories: {context_stats.get('total_memories', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Answer Generation\n",
    "\n",
    "Generate answers to financial questions using Groq LLM with retrieved context and knowledge graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Export Results\n",
    "\n",
    "Export knowledge graph and analysis results to JSON and RDF formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Generate answers using Groq LLM from semantica.llms module\n",
    "financial_questions = [\n",
    "    \"What were the key financial metrics discussed in the earnings call?\",\n",
    "    \"What guidance did management provide for future quarters?\"\n",
    "]\n",
    "\n",
    "generated_answers = []\n",
    "for question in financial_questions:\n",
    "    # Retrieve relevant context\n",
    "    context_results = context_retriever.retrieve(\n",
    "        query=question,\n",
    "        max_results=3,\n",
    "        min_relevance_score=0.2\n",
    "    )\n",
    "    \n",
    "    # Build context from retrieved results\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"Context {i+1}: {result.get('content', result.get('text', ''))}\"\n",
    "        for i, result in enumerate(context_results[:3])\n",
    "    ])\n",
    "    \n",
    "    # Extract relevant entities\n",
    "    relevant_entities = [\n",
    "        entity.get('name', '') for entity in knowledge_graph.get('entities', [])[:10]\n",
    "    ]\n",
    "    entities_text = \", \".join(relevant_entities[:5]) if relevant_entities else \"N/A\"\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"Based on the following earnings call transcript context and knowledge graph, answer the question.\n",
    "\n",
    "Context from transcript:\n",
    "{context_text[:1000]}\n",
    "\n",
    "Key entities identified: {entities_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer based on the context provided. If information is not available in the context, state that clearly.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer using Groq LLM\n",
    "    try:\n",
    "        answer = groq_llm.generate(\n",
    "            prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        generated_answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context_sources\": len(context_results),\n",
    "            \"model\": groq_llm.model\n",
    "        })\n",
    "    except Exception as e:\n",
    "        generated_answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error generating answer: {str(e)}\",\n",
    "            \"context_sources\": len(context_results),\n",
    "            \"model\": groq_llm.model\n",
    "        })\n",
    "\n",
    "print(f\"‚úì Answer generation complete using Groq LLM\")\n",
    "print(f\"  LLM Provider: Groq ({groq_llm.model})\")\n",
    "print(f\"  Questions answered: {len(generated_answers)}\")\n",
    "if generated_answers:\n",
    "    print(f\"  Sample question: '{generated_answers[0]['question']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Export structured outputs including triplets\n",
    "from semantica.export import JSONExporter, RDFExporter\n",
    "\n",
    "json_exporter = JSONExporter()\n",
    "rdf_exporter = RDFExporter()\n",
    "\n",
    "# Export knowledge graph to JSON\n",
    "kg_json = json_exporter.export(knowledge_graph, format=\"json\")\n",
    "\n",
    "# Export knowledge graph to RDF (Turtle format)\n",
    "rdf_output = rdf_exporter.export_to_rdf(knowledge_graph, format=\"turtle\")\n",
    "\n",
    "# Create analysis summary\n",
    "analysis_summary = {\n",
    "    \"financial_metrics\": financial_metrics,\n",
    "    \"extraction_stats\": {\n",
    "        \"entities\": len(entities),\n",
    "        \"relationships\": len(relationships),\n",
    "        \"triplets\": len(triplets),\n",
    "        \"provider\": f\"Groq LLM (semantica.llms module) - {groq_llm.model}\"\n",
    "    },\n",
    "    \"conflict_resolution\": {\n",
    "        \"conflicts_detected\": len(value_conflicts) + len(relationship_conflicts),\n",
    "        \"conflicts_resolved\": len(resolved_conflicts),\n",
    "        \"strategy\": \"voting\"\n",
    "    },\n",
    "    \"deduplication\": {\n",
    "        \"original_entities\": len(entity_dicts),\n",
    "        \"duplicates_detected\": len(duplicates),\n",
    "        \"merged_entities\": len(merged_entities),\n",
    "        \"strategy\": \"keep_most_complete\"\n",
    "    },\n",
    "    \"knowledge_graph\": {\n",
    "        \"entities\": len(knowledge_graph.get('entities', [])),\n",
    "        \"relationships\": len(knowledge_graph.get('relationships', []))\n",
    "    },\n",
    "    \"graph_analytics\": {\n",
    "        \"metrics\": metrics,\n",
    "        \"communities\": num_communities,\n",
    "        \"top_entities\": top_entities[:5] if top_entities else []\n",
    "    },\n",
    "    \"context_graph\": {\n",
    "        \"nodes\": len(context_graph.nodes),\n",
    "        \"edges\": len(context_graph.edges)\n",
    "    },\n",
    "    \"context_retrieval\": {\n",
    "        \"queries_processed\": len(retrieved_contexts),\n",
    "        \"total_results\": sum(c[\"count\"] for c in retrieved_contexts)\n",
    "    },\n",
    "    \"entity_linking\": {\n",
    "        \"entities_linked\": len(linked_entities),\n",
    "        \"entity_web_nodes\": len(entity_web.get('nodes', [])),\n",
    "        \"entity_web_edges\": len(entity_web.get('edges', []))\n",
    "    },\n",
    "    \"agent_memory\": {\n",
    "        \"memories_stored\": len(memory_ids),\n",
    "        \"total_memories\": memory_stats.get('total_memories', 0)\n",
    "    },\n",
    "    \"agent_context\": {\n",
    "        \"graphrag_results\": len(graphrag_results),\n",
    "        \"total_memories\": context_stats.get('total_memories', 0)\n",
    "    },\n",
    "    \"answer_generation\": {\n",
    "        \"questions_answered\": len(generated_answers),\n",
    "        \"llm_provider\": \"Groq\",\n",
    "        \"llm_model\": groq_llm.model,\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"question\": ans[\"question\"],\n",
    "                \"answer_length\": len(ans[\"answer\"]),\n",
    "                \"context_sources\": ans[\"context_sources\"]\n",
    "            }\n",
    "            for ans in generated_answers\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úì Export complete\")\n",
    "print(f\"  Analysis summary: {len(analysis_summary)} sections\")\n",
    "print(f\"  Knowledge graph (JSON): {len(kg_json) if isinstance(kg_json, dict) else 0} items\")\n",
    "print(f\"  RDF (Turtle): {len(rdf_output)} characters\")\n",
    "print(f\"  LLM answers generated: {len(generated_answers)}\")\n",
    "print(f\"  LLM provider: Groq ({groq_llm.model})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

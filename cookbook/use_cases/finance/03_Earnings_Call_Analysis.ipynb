{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/finance/03_Earnings_Call_Analysis.ipynb)\n",
    "\n",
    "# Earnings Call Analysis with Docling and Semantica: MDA Space Q3 2025\n",
    "\n",
    "##  Data Sources\n",
    "\n",
    "This analysis dual-tracks two critical documents from **MDA Space Ltd.** for the third quarter of 2025 to provide a holistic view of the company's performance and strategy:\n",
    "1. **Press Release:** A structured summary of financial results, strategic milestones, and executive commentary.\n",
    "2. **Earnings Transcript:** A detailed record of the management presentation and the subsequent Q&A session with analysts, providing deeper qualitative context.\n",
    "\n",
    "###  Q3 2025 Strategic Highlights\n",
    "* **Robust Backlog:** The company maintained a significant backlog at quarter-end, providing strong revenue visibility and supporting long-term growth objectives.\n",
    "* **Strong Revenue Growth:** Substantial year-over-year revenue expansion driven by successful execution across major programs and satellite system deliveries.\n",
    "* **Profitability & Margins:** Strong operational performance led to a significant increase in Adjusted EBITDA and healthy overall margin expansion.\n",
    "* **Earnings Performance:** Noteworthy growth in adjusted net income and earnings per share, reflecting scaled operations and improved bottom-line efficiency.\n",
    "* **Cash Flow & Capital Structure:** Positive operating cash flow and a healthy balance sheet with a very low leverage ratio, positioning the company well for future investment.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to extract deep insights from unstructured financial documents. By combining **Docling** for high-fidelity parsing with **Semantica** for knowledge engineering, we build a structured semantic layer that enables complex analysis and advanced GraphRAG capabilities.\n",
    "\n",
    "**Workflow:** `Dual PDF Input → Docling Parsing → Entity & Relation Extraction → Knowledge Graph Construction → GraphRAG → Strategic Q&A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU semantica docling pdfplumber groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq LLM provider\n",
    "from semantica.llms import Groq\n",
    "import os\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\", \"\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY not found. Please set it as an environment variable or update this cell.\")\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    \n",
    "groq_llm = Groq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "print(f\"✓ Groq LLM initialized: {groq_llm.model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parse PDF with Docling\n",
    "\n",
    "Parse earnings call PDF and extract financial tables using DoclingParser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from semantica.parse import DoclingParser\n",
    "\n",
    "# Initialize DoclingParser with default settings\n",
    "# Optional: configure with export_format=\"html\" or enable_ocr=True if needed\n",
    "parser = DoclingParser()\n",
    "\n",
    "# PDF URLs for MDA Space Q3 2025 earnings documents\n",
    "press_release_url = \"https://filecache.investorroom.com/mr5ircnw_mda/677/MDA_Space_Ltd_Q3_2025_Press_Release_Nov_14_2025_FINAL.pdf\"\n",
    "transcript_url = \"https://filecache.investorroom.com/mr5ircnw_mda/681/MDA%20Space%20Ltd.%20Q3%202025%20Earnings%20Conference%20Call%20Transcript%20%28November%2014%202025%29.pdf\"\n",
    "\n",
    "# Setup download directory\n",
    "download_dir = Path(\"downloads\")\n",
    "download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "press_release_pdf = download_dir / \"mda_space_q3_2025_press_release.pdf\"\n",
    "transcript_pdf = download_dir / \"mda_space_q3_2025_transcript.pdf\"\n",
    "\n",
    "# Download PDFs if they don't exist\n",
    "if not press_release_pdf.exists():\n",
    "    press_release_pdf.write_bytes(requests.get(press_release_url).content)\n",
    "\n",
    "if not transcript_pdf.exists():\n",
    "    transcript_pdf.write_bytes(requests.get(transcript_url).content)\n",
    "\n",
    "# Parse documents using DoclingParser\n",
    "try:\n",
    "    press_release = parser.parse(press_release_pdf)\n",
    "    transcript = parser.parse(transcript_pdf)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Parsing failed: {e}\")\n",
    "    print(\"Using fallback empty documents for demonstration.\")\n",
    "    press_release = {\"full_text\": \"\", \"tables\": []}\n",
    "    transcript = {\"full_text\": \"\", \"tables\": []}\n",
    "\n",
    "# Combine parsed documents\n",
    "parsed_doc = {\n",
    "    \"full_text\": f\"# Press Release\\n\\n{press_release['full_text']}\\n\\n# Transcript\\n\\n{transcript['full_text']}\",\n",
    "    \"tables\": press_release['tables'] + transcript['tables'],\n",
    "    \"metadata\": {\n",
    "        \"title\": \"MDA Space Ltd. Q3 2025 Earnings Analysis\",\n",
    "        \"company\": \"MDA Space Ltd.\",\n",
    "        \"quarter\": \"Q3 2025\",\n",
    "        \"date\": \"November 14, 2025\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"✓ Parsed {len(parsed_doc['tables'])} tables from {len(press_release['tables']) + len(transcript['tables'])} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Normalize Text\n",
    "\n",
    "Normalize extracted text using TextNormalizer for consistent processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize full document text (run ONCE)\n",
    "from semantica.normalize import TextNormalizer\n",
    "# Initialize normalizer\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "normalized_text = normalizer.normalize(\n",
    "    parsed_doc[\"full_text\"],\n",
    "    clean_html=False,\n",
    "    remove_extra_whitespace=False,\n",
    "    lowercase=False        # preserve casing for entities & finance terms\n",
    ")\n",
    "\n",
    "print(f\"Text normalized: {len(normalized_text)} characters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split Text into Chunks\n",
    "\n",
    "Split the normalized text into overlapping chunks to enable scalable and accurate entity and relation extraction.\n",
    "This step prepares the text for LLM-based semantic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.split import TextSplitter\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 250\n",
    "\n",
    "splitter = TextSplitter(\n",
    "    method=\"recursive\",   # safest for long PDFs\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "chunks = splitter.split(normalized_text)\n",
    "\n",
    "print(f\"✓ Created {len(chunks)} chunks\")\n",
    "\n",
    "# Version-safe access to chunk text\n",
    "def get_chunk_text(chunk):\n",
    "    return getattr(chunk, \"content\", getattr(chunk, \"text\", \"\"))\n",
    "\n",
    "# Inspect one chunk\n",
    "print(\"Sample chunk:\\n\", get_chunk_text(chunks[0])[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Entities\n",
    "\n",
    "Extract entities (organizations, people, financial terms) using NERExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract entities from ALL chunks using NERExtractor (Groq)\n",
    "\n",
    "from semantica.semantic_extract import NERExtractor\n",
    "import os\n",
    "\n",
    "# Initialize NER extractor\n",
    "ner = NERExtractor(\n",
    "    method=\"llm\",\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    min_confidence=0.5,\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "entity_types = [\n",
    "    \"ORGANIZATION\", \"ORG\", \"PERSON\", \"MONEY\", \"CURRENCY\",\n",
    "    \"PERCENT\", \"PERCENTAGE\", \"DATE\", \"TIME\", \"PRODUCT\",\n",
    "    \"LOCATION\", \"GPE\", \"EVENT\", \"QUANTITY\", \"CARDINAL\"\n",
    "]\n",
    "\n",
    "# Version-safe chunk text accessor\n",
    "def get_chunk_text(chunk):\n",
    "    return getattr(chunk, \"content\", getattr(chunk, \"text\", \"\"))\n",
    "\n",
    "all_entities = []\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    text = get_chunk_text(chunk)\n",
    "\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        entities = ner.extract_entities(\n",
    "            text,\n",
    "            entity_types=entity_types\n",
    "        )\n",
    "        all_entities.extend(entities)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Chunk {i} failed: {e}\")\n",
    "\n",
    "    if i % 10 == 0 or i == len(chunks):\n",
    "        print(f\"Processed {i}/{len(chunks)} chunks\")\n",
    "\n",
    "print(f\"✓ Total entities extracted: {len(all_entities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Financial Metrics\n",
    "\n",
    "Extract financial metrics (money, percentages, dates) from text and tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract financial metrics from ALL chunks (explicit + safe)\n",
    "\n",
    "financial_entity_types = [\n",
    "    \"MONEY\", \"CURRENCY\", \"PERCENT\", \"PERCENTAGE\",\n",
    "    \"QUANTITY\", \"CARDINAL\"\n",
    "]\n",
    "\n",
    "def get_chunk_text(chunk):\n",
    "    return getattr(chunk, \"content\", getattr(chunk, \"text\", \"\"))\n",
    "\n",
    "financial_entities = []\n",
    "\n",
    "total_chunks = len(chunks)\n",
    "print(f\"Processing {total_chunks} chunks for financial entities...\")\n",
    "\n",
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    text = get_chunk_text(chunk)\n",
    "\n",
    "    # Skip empty chunks but still count them\n",
    "    if not text.strip():\n",
    "        print(f\"  Skipping empty chunk {idx}/{total_chunks}\")\n",
    "        continue\n",
    "\n",
    "    # ALWAYS run NER per chunk\n",
    "    entities = ner.extract_entities(\n",
    "        text,\n",
    "        entity_types=financial_entity_types\n",
    "    )\n",
    "\n",
    "    financial_entities.extend(entities)\n",
    "\n",
    "    if idx % 10 == 0 or idx == total_chunks:\n",
    "        print(f\"  Processed {idx}/{total_chunks} chunks\")\n",
    "\n",
    "# Aggregate results\n",
    "financial_metrics = {\"money\": [], \"percentages\": [], \"quantities\": []}\n",
    "\n",
    "for e in financial_entities:\n",
    "    label = e.label.lower()\n",
    "    if \"money\" in label or \"currency\" in label:\n",
    "        financial_metrics[\"money\"].append(e.text)\n",
    "    elif \"percent\" in label:\n",
    "        financial_metrics[\"percentages\"].append(e.text)\n",
    "    elif \"quantity\" in label or \"cardinal\" in label:\n",
    "        financial_metrics[\"quantities\"].append(e.text)\n",
    "\n",
    "print(f\"✓ Financial entity mentions extracted: {len(financial_entities)}\")\n",
    "print(f\"  Money/Currency: {len(financial_metrics['money'])}\")\n",
    "print(f\"  Percentages: {len(financial_metrics['percentages'])}\")\n",
    "print(f\"  Quantities: {len(financial_metrics['quantities'])}\")\n",
    "\n",
    "if financial_entities:\n",
    "    sample = financial_entities[0]\n",
    "    print(f\"  Sample: {sample.text} ({sample.label})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Relationships\n",
    "\n",
    "Extract relationships between entities using RelationExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract relationships from all chunks (minimal handling)\n",
    "\n",
    "from semantica.semantic_extract import RelationExtractor\n",
    "import os\n",
    "\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"llm\",\n",
    "    confidence_threshold=0.5,\n",
    "    relation_types=[\n",
    "        \"HAS_REVENUE\", \"HAS_EPS\", \"HAS_MARGIN\", \"HAS_PROFIT\", \"HAS_GROWTH\",\n",
    "        \"PROVIDES_GUIDANCE\", \"STATES\", \"ANNOUNCES\", \"REPORTS\", \"EXPECTS\",\n",
    "        \"OPERATES_IN\", \"LOCATED_IN\", \"PARTNERS_WITH\", \"SERVES\",\n",
    "        \"COMPARED_TO\", \"INCREASED_BY\", \"DECREASED_BY\", \"CHANGED_BY\",\n",
    "        \"DURING\", \"IN_QUARTER\", \"FOR_PERIOD\",\n",
    "        \"RELATED_TO\", \"PART_OF\", \"AFFECTS\"\n",
    "    ],\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "def get_chunk_text(chunk):\n",
    "    return getattr(chunk, \"content\", getattr(chunk, \"text\", \"\"))\n",
    "\n",
    "relationships = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    text = get_chunk_text(chunk)\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    relationships.extend(\n",
    "        relation_extractor.extract_relations(\n",
    "            text,\n",
    "            entities=all_entities,\n",
    "            provider=\"groq\",\n",
    "            llm_model=\"llama-3.1-8b-instant\",\n",
    "            temperature=0.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"✓ Relationships extracted: {len(relationships)}\")\n",
    "\n",
    "if relationships:\n",
    "    sample = relationships[0]\n",
    "    print(f\"Sample: {sample.subject.text} → {sample.predicate} → {sample.object.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract RDF Triplets\n",
    "\n",
    "Extract RDF triplets (subject-predicate-object) using TripletExtractor with Groq LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Extract RDF triplets from all chunks (minimal handling)\n",
    "\n",
    "from semantica.semantic_extract import TripletExtractor\n",
    "import os\n",
    "\n",
    "triplet_extractor = TripletExtractor(\n",
    "    method=\"llm\",\n",
    "    include_temporal=True,\n",
    "    include_provenance=True,\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "def get_chunk_text(chunk):\n",
    "    return getattr(chunk, \"content\", getattr(chunk, \"text\", \"\"))\n",
    "\n",
    "triplets = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    text = get_chunk_text(chunk)\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    triplets.extend(\n",
    "        triplet_extractor.extract_triplets(\n",
    "            text,\n",
    "            entities=all_entities,\n",
    "            relations=relationships if relationships else None\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Optional validation (if available)\n",
    "validated_triplets = (\n",
    "    triplet_extractor.validate_triplets(triplets)\n",
    "    if hasattr(triplet_extractor, \"validate_triplets\")\n",
    "    else triplets\n",
    ")\n",
    "\n",
    "print(f\"✓ RDF triplets extracted: {len(validated_triplets)}\")\n",
    "\n",
    "if validated_triplets:\n",
    "    t = validated_triplets[0]\n",
    "    print(f\"Sample: {t.subject} → {t.predicate} → {t.object}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Detect Conflicts\n",
    "\n",
    "Detect conflicts in extracted entities and relationships using ConflictDetector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Detect conflicts in extracted entities and relationships\n",
    "from semantica.conflicts import ConflictDetector, SourceTracker, SourceReference\n",
    "\n",
    "source_tracker = SourceTracker()\n",
    "conflict_detector = ConflictDetector(\n",
    "    source_tracker=source_tracker,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "# Track sources for entities\n",
    "for entity in all_entities:\n",
    "    entity_id = getattr(entity, 'id', None) or getattr(entity, 'text', '')\n",
    "    entity_name = getattr(entity, 'text', '')\n",
    "    source_tracker.track_property_source(\n",
    "        entity_id,\n",
    "        'name',\n",
    "        entity_name,\n",
    "        source=SourceReference(\n",
    "            source='earnings_call',\n",
    "            timestamp='2024-Q1',\n",
    "            metadata={'entity_type': getattr(entity, 'label', 'UNKNOWN')}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Detect value conflicts\n",
    "value_conflicts = conflict_detector.detect_value_conflicts(\n",
    "    [{'id': getattr(e, 'id', ''), 'name': getattr(e, 'text', '')} for e in all_entities],\n",
    "    property_name='name'\n",
    ")\n",
    "\n",
    "# Detect relationship conflicts\n",
    "relationship_conflicts = conflict_detector.detect_relationship_conflicts(relationships)\n",
    "\n",
    "print(f\"✓ Conflicts detected\")\n",
    "print(f\"  Value conflicts: {len(value_conflicts)}\")\n",
    "print(f\"  Relationship conflicts: {len(relationship_conflicts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Resolve Conflicts\n",
    "\n",
    "Resolve detected conflicts using ConflictResolver with voting strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Resolve conflicts using ConflictResolver\n",
    "from semantica.conflicts import ConflictResolver\n",
    "\n",
    "conflict_resolver = ConflictResolver(\n",
    "    default_strategy='voting',\n",
    "    source_tracker=source_tracker\n",
    ")\n",
    "\n",
    "# Resolve value conflicts\n",
    "resolved_entities = list(all_entities)\n",
    "resolved_conflicts = []\n",
    "for conflict in value_conflicts:\n",
    "    resolution = conflict_resolver.resolve_conflict(conflict, strategy='voting')\n",
    "    resolved_conflicts.append(resolution)\n",
    "\n",
    "# Resolve relationship conflicts\n",
    "resolved_relationships = list(relationships)\n",
    "for conflict in relationship_conflicts:\n",
    "    resolution = conflict_resolver.resolve_conflict(conflict, strategy='voting')\n",
    "    resolved_conflicts.append(resolution)\n",
    "\n",
    "print(f\"✓ Conflicts resolved: {len(resolved_conflicts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Deduplicate Entities\n",
    "\n",
    "Detect and merge duplicate entities using DuplicateDetector and EntityMerger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Deduplicate entities using DuplicateDetector and EntityMerger\n",
    "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
    "\n",
    "duplicate_detector = DuplicateDetector(\n",
    "    similarity_threshold=0.8,\n",
    "    confidence_threshold=0.7\n",
    ")\n",
    "\n",
    "# Convert entities to dict format\n",
    "entity_dicts = []\n",
    "for entity in resolved_entities:\n",
    "    entity_dicts.append({\n",
    "        'id': getattr(entity, 'id', ''),\n",
    "        'name': getattr(entity, 'text', ''),\n",
    "        'type': getattr(entity, 'label', 'UNKNOWN'),\n",
    "        'confidence': getattr(entity, 'confidence', 1.0),\n",
    "        'metadata': getattr(entity, 'metadata', {})\n",
    "    })\n",
    "\n",
    "# Detect duplicates\n",
    "duplicates = duplicate_detector.detect_duplicates(entity_dicts)\n",
    "\n",
    "# Merge duplicates\n",
    "entity_merger = EntityMerger(preserve_provenance=True)\n",
    "merge_operations = entity_merger.merge_duplicates(\n",
    "    entity_dicts,\n",
    "    strategy='keep_most_complete'\n",
    ")\n",
    "\n",
    "merged_entities = [op.merged_entity for op in merge_operations]\n",
    "\n",
    "print(f\"✓ Deduplication complete\")\n",
    "print(f\"  Original entities: {len(entity_dicts)}\")\n",
    "print(f\"  Merged entities: {len(merged_entities)}\")\n",
    "print(f\"  Duplicates removed: {len(entity_dicts) - len(merged_entities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Build Knowledge Graph\n",
    "\n",
    "Build knowledge graph from cleaned entities, relationships, and triplets using GraphBuilder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Build knowledge graph from cleaned entities, resolved relationships, and triplets\n",
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "graph_builder = GraphBuilder(\n",
    "    merge_entities=True,\n",
    "    entity_resolution_strategy=\"fuzzy\"\n",
    ")\n",
    "\n",
    "# Convert triplets to relationships format\n",
    "triplet_relationships = []\n",
    "for triplet in validated_triplets:\n",
    "    triplet_relationships.append({\n",
    "        \"source\": triplet.subject,\n",
    "        \"predicate\": triplet.predicate,\n",
    "        \"target\": triplet.object,\n",
    "        \"confidence\": triplet.confidence,\n",
    "        \"metadata\": triplet.metadata\n",
    "    })\n",
    "\n",
    "all_relationships = resolved_relationships + triplet_relationships\n",
    "\n",
    "kg_data = {\n",
    "    \"entities\": merged_entities,\n",
    "    \"relationships\": all_relationships,\n",
    "    \"triplets\": validated_triplets,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"earnings_call_transcript\",\n",
    "        \"financial_metrics\": financial_metrics,\n",
    "        \"extraction_method\": \"Groq LLM\"\n",
    "    }\n",
    "}\n",
    "\n",
    "knowledge_graph = graph_builder.build(\n",
    "    sources=[kg_data],\n",
    "    merge_entities=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Knowledge graph built\")\n",
    "print(f\"  Entities: {len(knowledge_graph.get('entities', []))}\")\n",
    "print(f\"  Relationships: {len(knowledge_graph.get('relationships', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Analyze Knowledge Graph\n",
    "\n",
    "Analyze graph structure using GraphAnalyzer (centrality, communities, connectivity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Analyze knowledge graph using GraphAnalyzer\n",
    "from semantica.kg import GraphAnalyzer\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "analysis = graph_analyzer.analyze_graph(knowledge_graph)\n",
    "centrality = graph_analyzer.calculate_centrality(knowledge_graph, 'degree')\n",
    "communities = graph_analyzer.detect_communities(knowledge_graph, algorithm='louvain')\n",
    "connectivity = graph_analyzer.analyze_connectivity(knowledge_graph)\n",
    "metrics = graph_analyzer.compute_metrics(knowledge_graph)\n",
    "\n",
    "top_entities = []\n",
    "if centrality and 'rankings' in centrality:\n",
    "    top_entities = centrality['rankings'][:5]\n",
    "\n",
    "num_communities = len(communities.get('communities', [])) if isinstance(communities, dict) else 0\n",
    "\n",
    "print(f\"✓ Graph analysis complete\")\n",
    "print(f\"  Communities: {num_communities}\")\n",
    "print(f\"  Top entities: {len(top_entities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Build Context Graph\n",
    "\n",
    "Build ContextGraph from knowledge graph for enhanced retrieval and GraphRAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Build context graph for enhanced retrieval\n",
    "from semantica.context import ContextGraph\n",
    "\n",
    "context_graph = ContextGraph(\n",
    "    extract_entities=True,\n",
    "    extract_relationships=True\n",
    ")\n",
    "\n",
    "# Convert knowledge graph to context graph format\n",
    "nodes = []\n",
    "for entity in knowledge_graph.get('entities', []):\n",
    "    nodes.append({\n",
    "        \"id\": entity.get('id', entity.get('name', '')),\n",
    "        \"type\": entity.get('type', 'entity'),\n",
    "        \"properties\": {\n",
    "            \"content\": entity.get('name', ''),\n",
    "            \"confidence\": entity.get('confidence', 1.0),\n",
    "            **entity.get('metadata', {})\n",
    "        }\n",
    "    })\n",
    "\n",
    "edges = []\n",
    "for rel in knowledge_graph.get('relationships', []):\n",
    "    edges.append({\n",
    "        \"source_id\": rel.get('source', ''),\n",
    "        \"target_id\": rel.get('target', ''),\n",
    "        \"type\": rel.get('predicate', 'related_to'),\n",
    "        \"weight\": rel.get('confidence', 1.0)\n",
    "    })\n",
    "\n",
    "node_count = context_graph.add_nodes(nodes)\n",
    "edge_count = context_graph.add_edges(edges)\n",
    "\n",
    "print(f\"✓ Context graph built\")\n",
    "print(f\"  Nodes: {node_count}\")\n",
    "print(f\"  Edges: {edge_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Context Retrieval\n",
    "\n",
    "Set up hybrid retrieval (vector + graph) using ContextRetriever for GraphRAG queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Set up hybrid context retrieval and demonstrate GraphRAG\n",
    "from semantica.vector_store import VectorStore\n",
    "from semantica.context import ContextRetriever\n",
    "\n",
    "# Initialize VectorStore\n",
    "vector_store = VectorStore(backend=\"faiss\")\n",
    "vector_store.add(\n",
    "    texts=[parsed_doc[\"full_text\"]],\n",
    "    metadata=[{\"source\": \"earnings_call\", \"type\": \"transcript\"}]\n",
    ")\n",
    "\n",
    "# Initialize ContextRetriever\n",
    "context_retriever = ContextRetriever(\n",
    "    knowledge_graph=context_graph,\n",
    "    vector_store=vector_store,\n",
    "    hybrid_alpha=0.6,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2\n",
    ")\n",
    "\n",
    "# Retrieve context for financial queries\n",
    "financial_queries = [\n",
    "    \"What was the company's revenue guidance?\",\n",
    "    \"What were the key financial metrics discussed?\"\n",
    "]\n",
    "\n",
    "retrieved_contexts = []\n",
    "for query in financial_queries:\n",
    "    results = context_retriever.retrieve(\n",
    "        query=query,\n",
    "        max_results=3,\n",
    "        min_relevance_score=0.2\n",
    "    )\n",
    "    retrieved_contexts.append({\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"count\": len(results)\n",
    "    })\n",
    "\n",
    "print(f\"✓ Hybrid retrieval configured\")\n",
    "print(f\"  Queries processed: {len(retrieved_contexts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Entity Linking\n",
    "\n",
    "Link entities across sources and assign URIs using EntityLinker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Link entities using EntityLinker\n",
    "from semantica.context import EntityLinker\n",
    "\n",
    "entity_linker = EntityLinker(knowledge_graph=knowledge_graph)\n",
    "\n",
    "# Assign URIs to key entities\n",
    "linked_entities = []\n",
    "for entity in merged_entities[:10]:\n",
    "    entity_id = entity.get('id', entity.get('name', ''))\n",
    "    entity_name = entity.get('name', '')\n",
    "    entity_type = entity.get('type', 'UNKNOWN')\n",
    "    \n",
    "    uri = entity_linker.assign_uri(\n",
    "        entity_id=entity_id,\n",
    "        text=entity_name,\n",
    "        entity_type=entity_type\n",
    "    )\n",
    "    linked_entities.append({\n",
    "        \"entity_id\": entity_id,\n",
    "        \"name\": entity_name,\n",
    "        \"uri\": uri,\n",
    "        \"type\": entity_type\n",
    "    })\n",
    "\n",
    "# Build entity web\n",
    "entity_web = entity_linker.build_entity_web()\n",
    "\n",
    "print(f\"✓ Entity linking complete\")\n",
    "print(f\"  Entities linked: {len(linked_entities)}\")\n",
    "print(f\"  Entity web nodes: {len(entity_web.get('nodes', []))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Agent Memory\n",
    "\n",
    "Store and retrieve memories using AgentMemory with RAG integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Store and retrieve memories using AgentMemory\n",
    "from semantica.context import AgentMemory\n",
    "\n",
    "agent_memory = AgentMemory(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=knowledge_graph,\n",
    "    retention_days=30\n",
    ")\n",
    "\n",
    "# Store earnings call memories\n",
    "memory_ids = []\n",
    "memory_contents = [\n",
    "    f\"Earnings call transcript: {parsed_doc['metadata'].get('title', 'Q1 2024')}\",\n",
    "    f\"Financial metrics extracted: {sum(len(v) for v in financial_metrics.values())} metrics\",\n",
    "    f\"Key entities identified: {len(merged_entities)} entities\"\n",
    "]\n",
    "\n",
    "for content in memory_contents:\n",
    "    memory_id = agent_memory.store(\n",
    "        content=content,\n",
    "        metadata={\"source\": \"earnings_call\", \"type\": \"transcript_analysis\"},\n",
    "        extract_entities=True,\n",
    "        extract_relationships=True\n",
    "    )\n",
    "    memory_ids.append(memory_id)\n",
    "\n",
    "# Retrieve memories\n",
    "financial_memories = agent_memory.retrieve(\n",
    "    query=\"financial metrics and earnings\",\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "memory_stats = agent_memory.get_statistics()\n",
    "\n",
    "print(f\"✓ Agent memory configured\")\n",
    "print(f\"  Memories stored: {len(memory_ids)}\")\n",
    "print(f\"  Total memories: {memory_stats.get('total_memories', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Agent Context\n",
    "\n",
    "Unified context management with AgentContext (auto-detects RAG vs GraphRAG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: High-level context management with AgentContext\n",
    "from semantica.context import AgentContext\n",
    "\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=context_graph,\n",
    "    use_graph_expansion=True,\n",
    "    max_expansion_hops=2,\n",
    "    hybrid_alpha=0.6,\n",
    "    retention_days=30\n",
    ")\n",
    "\n",
    "# Store content with auto-extraction\n",
    "memory_id = agent_context.store(\n",
    "    content=parsed_doc[\"full_text\"][:1000],\n",
    "    metadata={\"source\": \"earnings_call\", \"date\": \"2024-Q1\"},\n",
    "    extract_entities=True,\n",
    "    extract_relationships=True,\n",
    "    link_entities=True\n",
    ")\n",
    "\n",
    "# Retrieve with auto-detected GraphRAG\n",
    "graphrag_results = agent_context.retrieve(\n",
    "    query=\"What was discussed about revenue growth?\",\n",
    "    max_results=5,\n",
    "    expand_graph=True,\n",
    "    include_entities=True\n",
    ")\n",
    "\n",
    "context_stats = agent_context.stats()\n",
    "\n",
    "print(f\"✓ AgentContext configured\")\n",
    "print(f\"  Memory stored: {memory_id}\")\n",
    "print(f\"  GraphRAG results: {len(graphrag_results)}\")\n",
    "print(f\"  Total memories: {context_stats.get('total_memories', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Answer Generation\n",
    "\n",
    "Generate answers to financial questions using Groq LLM with retrieved context and knowledge graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Export Results\n",
    "\n",
    "Export knowledge graph and analysis results to JSON and RDF formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Generate answers using Groq LLM from semantica.llms module\n",
    "financial_questions = [\n",
    "    \"What were the key financial metrics discussed in the earnings call?\",\n",
    "    \"What guidance did management provide for future quarters?\"\n",
    "]\n",
    "\n",
    "generated_answers = []\n",
    "for question in financial_questions:\n",
    "    # Retrieve relevant context\n",
    "    context_results = context_retriever.retrieve(\n",
    "        query=question,\n",
    "        max_results=3,\n",
    "        min_relevance_score=0.2\n",
    "    )\n",
    "    \n",
    "    # Build context from retrieved results\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"Context {i+1}: {result.get('content', result.get('text', ''))}\"\n",
    "        for i, result in enumerate(context_results[:3])\n",
    "    ])\n",
    "    \n",
    "    # Extract relevant entities\n",
    "    relevant_entities = [\n",
    "        entity.get('name', '') for entity in knowledge_graph.get('entities', [])[:10]\n",
    "    ]\n",
    "    entities_text = \", \".join(relevant_entities[:5]) if relevant_entities else \"N/A\"\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"Based on the following earnings call transcript context and knowledge graph, answer the question.\n",
    "\n",
    "Context from transcript:\n",
    "{context_text[:1000]}\n",
    "\n",
    "Key entities identified: {entities_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer based on the context provided. If information is not available in the context, state that clearly.\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer using Groq LLM\n",
    "    try:\n",
    "        answer = groq_llm.generate(\n",
    "            prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        generated_answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context_sources\": len(context_results),\n",
    "            \"model\": groq_llm.model\n",
    "        })\n",
    "    except Exception as e:\n",
    "        generated_answers.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": f\"Error generating answer: {str(e)}\",\n",
    "            \"context_sources\": len(context_results),\n",
    "            \"model\": groq_llm.model\n",
    "        })\n",
    "\n",
    "print(f\"✓ Answer generation complete using Groq LLM\")\n",
    "print(f\"  LLM Provider: Groq ({groq_llm.model})\")\n",
    "print(f\"  Questions answered: {len(generated_answers)}\")\n",
    "if generated_answers:\n",
    "    print(f\"  Sample question: '{generated_answers[0]['question']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Export structured outputs including triplets\n",
    "from semantica.export import JSONExporter, RDFExporter\n",
    "\n",
    "json_exporter = JSONExporter()\n",
    "rdf_exporter = RDFExporter()\n",
    "\n",
    "# Export knowledge graph to JSON\n",
    "kg_json = json_exporter.export(knowledge_graph, format=\"json\")\n",
    "\n",
    "# Export knowledge graph to RDF (Turtle format)\n",
    "rdf_output = rdf_exporter.export_to_rdf(knowledge_graph, format=\"turtle\")\n",
    "\n",
    "# Create analysis summary\n",
    "analysis_summary = {\n",
    "    \"financial_metrics\": financial_metrics,\n",
    "    \"extraction_stats\": {\n",
    "        \"entities\": len(all_entities),\n",
    "        \"relationships\": len(relationships),\n",
    "        \"triplets\": len(triplets),\n",
    "        \"provider\": f\"Groq LLM (semantica.llms module) - {groq_llm.model}\"\n",
    "    },\n",
    "    \"conflict_resolution\": {\n",
    "        \"conflicts_detected\": len(value_conflicts) + len(relationship_conflicts),\n",
    "        \"conflicts_resolved\": len(resolved_conflicts),\n",
    "        \"strategy\": \"voting\"\n",
    "    },\n",
    "    \"deduplication\": {\n",
    "        \"original_entities\": len(entity_dicts),\n",
    "        \"duplicates_detected\": len(duplicates),\n",
    "        \"merged_entities\": len(merged_entities),\n",
    "        \"strategy\": \"keep_most_complete\"\n",
    "    },\n",
    "    \"knowledge_graph\": {\n",
    "        \"entities\": len(knowledge_graph.get('entities', [])),\n",
    "        \"relationships\": len(knowledge_graph.get('relationships', []))\n",
    "    },\n",
    "    \"graph_analytics\": {\n",
    "        \"metrics\": metrics,\n",
    "        \"communities\": num_communities,\n",
    "        \"top_entities\": top_entities[:5] if top_entities else []\n",
    "    },\n",
    "    \"context_graph\": {\n",
    "        \"nodes\": len(context_graph.nodes),\n",
    "        \"edges\": len(context_graph.edges)\n",
    "    },\n",
    "    \"context_retrieval\": {\n",
    "        \"queries_processed\": len(retrieved_contexts),\n",
    "        \"total_results\": sum(c[\"count\"] for c in retrieved_contexts)\n",
    "    },\n",
    "    \"entity_linking\": {\n",
    "        \"entities_linked\": len(linked_entities),\n",
    "        \"entity_web_nodes\": len(entity_web.get('nodes', [])),\n",
    "        \"entity_web_edges\": len(entity_web.get('edges', []))\n",
    "    },\n",
    "    \"agent_memory\": {\n",
    "        \"memories_stored\": len(memory_ids),\n",
    "        \"total_memories\": memory_stats.get('total_memories', 0)\n",
    "    },\n",
    "    \"agent_context\": {\n",
    "        \"graphrag_results\": len(graphrag_results),\n",
    "        \"total_memories\": context_stats.get('total_memories', 0)\n",
    "    },\n",
    "    \"answer_generation\": {\n",
    "        \"questions_answered\": len(generated_answers),\n",
    "        \"llm_provider\": \"Groq\",\n",
    "        \"llm_model\": groq_llm.model,\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"question\": ans[\"question\"],\n",
    "                \"answer_length\": len(ans[\"answer\"]),\n",
    "                \"context_sources\": ans[\"context_sources\"]\n",
    "            }\n",
    "            for ans in generated_answers\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"✓ Export complete\")\n",
    "print(f\"  Analysis summary: {len(analysis_summary)} sections\")\n",
    "print(f\"  Knowledge graph (JSON): {len(kg_json) if isinstance(kg_json, dict) else 0} items\")\n",
    "print(f\"  RDF (Turtle): {len(rdf_output)} characters\")\n",
    "print(f\"  LLM answers generated: {len(generated_answers)}\")\n",
    "print(f\"  LLM provider: Groq ({groq_llm.model})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

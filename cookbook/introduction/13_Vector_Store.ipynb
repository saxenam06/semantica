{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/introduction/13_Vector_Store.ipynb)\n",
    "\n",
    "# Vector Store - Comprehensive Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive walkthrough** of Semantica's vector_store module, demonstrating vector storage, similarity search, hybrid search, and multi-backend support for semantic retrieval.\n",
    "\n",
    "**Documentation**: [API Reference](https://semantica.readthedocs.io/reference/vector_store/)\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Store and manage vectors with metadata\n",
    "- Perform similarity search with different metrics\n",
    "- Use hybrid search combining vectors and metadata\n",
    "- Work with multiple vector store backends (FAISS, Weaviate, etc.)\n",
    "- Create and manage vector indices\n",
    "- Filter and rank search results\n",
    "- Implement namespace isolation for multi-tenancy\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Component | Purpose | When to Use |\n",
    "|-----------|---------|-------------|\n",
    "| `VectorStore` | Main vector storage | All vector operations |\n",
    "| `VectorIndexer` | Index creation | Performance optimization |\n",
    "| `VectorRetriever` | Similarity search | Finding similar vectors |\n",
    "| `HybridSearch` | Combined search | Vector + metadata filtering |\n",
    "| `MetadataFilter` | Metadata filtering | Filtering by attributes |\n",
    "| `MetadataStore` | Metadata management | Storing vector metadata |\n",
    "| `NamespaceManager` | Multi-tenancy | Isolating vector collections |\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install Semantica from PyPI:\n",
    "\n",
    "```bash\n",
    "pip install semantica\n",
    "# Or with all optional dependencies:\n",
    "pip install semantica[all]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (C:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q semantica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Basic Vector Storage\n",
    "\n",
    "Let's start with the `VectorStore` for basic vector storage and retrieval.\n",
    "\n",
    "### What is VectorStore?\n",
    "\n",
    "`VectorStore` is the main interface for vector operations:\n",
    "- **Storage**: Store vectors with metadata\n",
    "- **Search**: Find similar vectors\n",
    "- **CRUD**: Create, Read, Update, Delete operations\n",
    "- **Multi-backend**: Support for FAISS, Weaviate, Qdrant, Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚ùå</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>VectorStore</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>FAISSStore</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>HybridSearch</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>MetadataStore</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>NamespaceManager</td><td>-</td><td>0.00s</td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 100 vectors\n",
      "First 3 IDs: ['vec_0', 'vec_2', 'vec_4']\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "from semantica.embeddings import TextEmbedder\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize Embedder (Select Provider & Model)\n",
    "# You can choose 'sentence_transformers' or 'fastembed'\n",
    "embedder = TextEmbedder(method=\"sentence_transformers\", model_name=\"all-MiniLM-L6-v2\")\n",
    "dimension = embedder.get_embedding_dimension()\n",
    "\n",
    "# 2. Create vector store\n",
    "store = VectorStore(backend=\"faiss\", dimension=dimension)\n",
    "\n",
    "# 3. Generate Real Embeddings\n",
    "texts = [f\"Document {i}\" for i in range(100)]\n",
    "vectors = embedder.embed_batch(texts)\n",
    "\n",
    "metadata = [\n",
    "    {\"text\": txt, \"category\": \"science\" if i % 2 == 0 else \"technology\", \"year\": 2020 + (i % 4)}\n",
    "    for i, txt in enumerate(texts)\n",
    "]\n",
    "# 4. Store vectors\n",
    "vector_ids = store.store_vectors(vectors, metadata=metadata)\n",
    "\n",
    "print(f\"Stored {len(vector_ids)} vectors\")\n",
    "print(f\"First 3 IDs: {vector_ids[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Similarity Search\n",
    "\n",
    "Search for similar vectors using different similarity metrics.\n",
    "\n",
    "### Similarity Metrics\n",
    "\n",
    "- **Cosine Similarity**: Best for semantic similarity\n",
    "- **L2 Distance**: Euclidean distance\n",
    "- **Dot Product**: Fast, requires normalized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index search found 10 results\n",
      "Distances: [0.         0.59663105 0.6359793  0.66132385 0.6909249 ]\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import VectorIndexer, FAISSStore \n",
    "import numpy as np \n",
    "# We use the first vector from the dataset as a sample query\n",
    "if 'query_vector' not in locals():\n",
    "    if 'vectors' in locals() and len(vectors) > 0:\n",
    "        query_vector = vectors[0]\n",
    "    else:\n",
    "        # Fallback if vectors are also missing (safety check)\n",
    "        query_vector = np.random.rand(dimension).astype('float32')\n",
    "\n",
    "# Create indexer \n",
    "indexer = VectorIndexer(backend=\"faiss\", dimension=dimension) \n",
    "\n",
    "# Create HNSW index for fast approximate search \n",
    "adapter = FAISSStore(dimension=dimension) \n",
    "index = adapter.create_index(index_type=\"hnsw\", metric=\"L2\", m=16) \n",
    "\n",
    "# Add vectors to index \n",
    "vectors_array = np.array(vectors).astype('float32') \n",
    "# FIX: Removed 'index' argument. The adapter uses its internal self.index \n",
    "adapter.add_vectors(vectors_array, ids=vector_ids) \n",
    "\n",
    "# Search using index \n",
    "query_array = np.array(query_vector).astype('float32') \n",
    "# FIX: Call search on the 'index' object directly, not the adapter \n",
    "distances, indices = index.search(query_array.reshape(1, -1), k=10) \n",
    "\n",
    "print(f\"Index search found {len(indices[0])} results\") \n",
    "print(f\"Distances: {distances[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vector Indexing\n",
    "\n",
    "Create indices for faster search on large datasets.\n",
    "\n",
    "### Index Types (FAISS)\n",
    "\n",
    "- **Flat**: Exact search (brute force)\n",
    "- **IVF**: Inverted file index (approximate)\n",
    "- **HNSW**: Hierarchical graph (best balance)\n",
    "- **PQ**: Product quantization (compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index search found 10 results\n",
      "Distances: [0.         0.59663105 0.6359793  0.66132385 0.6909249 ]\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import VectorIndexer, FAISSStore\n",
    "\n",
    "# Get dimension from vectors to ensure consistency\n",
    "dimension = len(vectors[0]) if len(vectors) > 0 else 384\n",
    "\n",
    "# Create indexer\n",
    "indexer = VectorIndexer(backend=\"faiss\", dimension=dimension)\n",
    "\n",
    "# Create HNSW index for fast approximate search\n",
    "adapter = FAISSStore(dimension=dimension)\n",
    "index = adapter.create_index(index_type=\"hnsw\", metric=\"L2\", m=16)\n",
    "\n",
    "# Add vectors to index\n",
    "vectors_array = np.array(vectors).astype('float32')\n",
    "adapter.add_vectors(vectors_array, ids=vector_ids)\n",
    "\n",
    "# Search using index\n",
    "query_array = query_vector.astype('float32')\n",
    "distances, indices = index.search(query_array.reshape(1, -1), k=10)\n",
    "\n",
    "print(f\"Index search found {len(indices[0])} results\")\n",
    "print(f\"Distances: {distances[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hybrid Search\n",
    "\n",
    "Combine vector similarity with metadata filtering.\n",
    "\n",
    "### Hybrid Search Benefits\n",
    "\n",
    "- Filter by metadata before vector search\n",
    "- Combine multiple search criteria\n",
    "- More precise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search found 10 results\n",
      "\n",
      "Filtered results (science, year > 2021):\n",
      "1. Category: science, Year: 2020, Score: 1.000\n",
      "2. Category: technology, Year: 2021, Score: 0.702\n",
      "3. Category: science, Year: 2022, Score: 0.682\n",
      "4. Category: technology, Year: 2023, Score: 0.669\n",
      "5. Category: science, Year: 2022, Score: 0.655\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import HybridSearch, MetadataFilter \n",
    "import numpy as np\n",
    "\n",
    "# Create hybrid search \n",
    "hybrid_search = HybridSearch() \n",
    "\n",
    "# Create metadata filter \n",
    "filter = MetadataFilter() \\\n",
    "    .eq(\"category\", \"science\") \\\n",
    "    .gt(\"year\", 2021) \n",
    "\n",
    "# Perform hybrid search \n",
    "# Ensure vectors and metadata are available\n",
    "if 'vectors' not in locals() or 'metadata' not in locals() or 'vector_ids' not in locals():\n",
    "    print(\"Warning: vectors, metadata, or vector_ids are missing. Please run previous cells.\")\n",
    "else:\n",
    "    hybrid_results = hybrid_search.search( \n",
    "        query_vector, \n",
    "        vectors, \n",
    "        metadata, \n",
    "        vector_ids, \n",
    "        filter=filter, \n",
    "        k=10 \n",
    "    ) \n",
    "    \n",
    "    print(f\"Hybrid search found {len(hybrid_results)} results\") \n",
    "    print(\"\\nFiltered results (science, year > 2021):\") \n",
    "    for i, result in enumerate(hybrid_results[:5], 1): \n",
    "        meta = result.get('metadata', {}) \n",
    "        print(f\"{i}. Category: {meta.get('category')}, Year: {meta.get('year')}, Score: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Metadata Management\n",
    "\n",
    "Store and query metadata separately from vectors.\n",
    "\n",
    "### Metadata Operations\n",
    "\n",
    "- Store metadata for vectors\n",
    "- Query by metadata conditions\n",
    "- Update metadata\n",
    "- Schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 vectors with category='science'\n",
      "\n",
      "Metadata validation: True\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import MetadataStore, MetadataSchema\n",
    "\n",
    "# Create metadata store\n",
    "meta_store = MetadataStore()\n",
    "\n",
    "# Store metadata\n",
    "for i, vec_id in enumerate(vector_ids[:10]):\n",
    "    meta_store.store_metadata(vec_id, metadata[i])\n",
    "\n",
    "# Query metadata\n",
    "matching_ids = meta_store.query_metadata(\n",
    "    {\"category\": \"science\"},\n",
    "    operator=\"AND\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(matching_ids)} vectors with category='science'\")\n",
    "\n",
    "# Define schema for validation\n",
    "schema = MetadataSchema({\n",
    "    \"text\": {\"type\": str, \"required\": True},\n",
    "    \"category\": {\"type\": str, \"required\": True},\n",
    "    \"year\": {\"type\": int, \"required\": True}\n",
    "})\n",
    "\n",
    "# Validate metadata\n",
    "is_valid = schema.validate(metadata[0])\n",
    "print(f\"\\nMetadata validation: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Result Ranking and Fusion\n",
    "\n",
    "Combine and rank results from multiple searches.\n",
    "\n",
    "### Ranking Strategies\n",
    "\n",
    "- **Reciprocal Rank Fusion (RRF)**: Combine ranked lists\n",
    "- **Weighted Average**: Weight scores from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused results using RRF:\n",
      "1. ID: vec_2, Fused Score: 0.033\n",
      "2. ID: vec_1, Fused Score: 0.032\n",
      "3. ID: vec_4, Fused Score: 0.016\n",
      "4. ID: vec_3, Fused Score: 0.016\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import SearchRanker\n",
    "\n",
    "# Create ranker with RRF strategy\n",
    "ranker = SearchRanker(strategy=\"reciprocal_rank_fusion\")\n",
    "\n",
    "# Simulate multiple search results\n",
    "results1 = [\n",
    "    {\"id\": \"vec_1\", \"score\": 0.9},\n",
    "    {\"id\": \"vec_2\", \"score\": 0.8},\n",
    "    {\"id\": \"vec_3\", \"score\": 0.7}\n",
    "]\n",
    "\n",
    "results2 = [\n",
    "    {\"id\": \"vec_2\", \"score\": 0.85},\n",
    "    {\"id\": \"vec_4\", \"score\": 0.75},\n",
    "    {\"id\": \"vec_1\", \"score\": 0.7}\n",
    "]\n",
    "\n",
    "# Fuse results using RRF\n",
    "fused_results = ranker.rank([results1, results2], k=60)\n",
    "\n",
    "print(\"Fused results using RRF:\")\n",
    "for i, result in enumerate(fused_results, 1):\n",
    "    print(f\"{i}. ID: {result['id']}, Fused Score: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Namespace Management\n",
    "\n",
    "Isolate vectors for multi-tenant applications.\n",
    "\n",
    "### Namespace Features\n",
    "\n",
    "- Tenant isolation\n",
    "- Access control\n",
    "- Per-namespace operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenant 1: 5 vectors\n",
      "Tenant 2: 5 vectors\n",
      "\n",
      "User1 can write: True\n",
      "User2 can write: False\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import NamespaceManager\n",
    "\n",
    "# Create namespace manager\n",
    "ns_manager = NamespaceManager()\n",
    "\n",
    "# Create namespaces for different tenants\n",
    "ns1 = ns_manager.create_namespace(\"tenant1\", \"Tenant 1 vectors\")\n",
    "ns2 = ns_manager.create_namespace(\"tenant2\", \"Tenant 2 vectors\")\n",
    "\n",
    "# Add vectors to namespaces\n",
    "for i in range(5):\n",
    "    ns_manager.add_vector_to_namespace(f\"t1_vec_{i}\", \"tenant1\")\n",
    "    ns_manager.add_vector_to_namespace(f\"t2_vec_{i}\", \"tenant2\")\n",
    "\n",
    "# Get namespace vectors\n",
    "tenant1_vectors = ns_manager.get_namespace_vectors(\"tenant1\")\n",
    "tenant2_vectors = ns_manager.get_namespace_vectors(\"tenant2\")\n",
    "\n",
    "print(f\"Tenant 1: {len(tenant1_vectors)} vectors\")\n",
    "print(f\"Tenant 2: {len(tenant2_vectors)} vectors\")\n",
    "\n",
    "# Set access control\n",
    "ns1.set_access_control(\"user1\", [\"read\", \"write\"])\n",
    "ns1.set_access_control(\"user2\", [\"read\"])\n",
    "\n",
    "print(f\"\\nUser1 can write: {ns1.has_permission('user1', 'write')}\")\n",
    "print(f\"User2 can write: {ns1.has_permission('user2', 'write')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Convenience Functions\n",
    "\n",
    "Use convenience functions for quick operations.\n",
    "\n",
    "### Available Functions\n",
    "\n",
    "- `store_vectors()`: Store vectors\n",
    "- `search_vectors()`: Search vectors\n",
    "- `hybrid_search()`: Hybrid search\n",
    "- `update_vectors()`: Update vectors\n",
    "- `delete_vectors()`: Delete vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 768 and the array at index 5 has size 384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m new_vectors \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m     12\u001b[0m new_metadata \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew doc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m---> 13\u001b[0m new_ids \u001b[38;5;241m=\u001b[39m \u001b[43mstore_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Search using convenience function\u001b[39;00m\n",
      "File \u001b[1;32m~\\semantica\\semantica\\vector_store\\methods.py:174\u001b[0m, in \u001b[0;36mstore_vectors\u001b[1;34m(vectors, metadata, method, **options)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Default implementation\u001b[39;00m\n\u001b[0;32m    173\u001b[0m store \u001b[38;5;241m=\u001b[39m _get_store()\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\semantica\\semantica\\vector_store\\vector_store.py:178\u001b[0m, in \u001b[0;36mVectorStore.store_vectors\u001b[1;34m(self, vectors, metadata, **options)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Update index\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_tracker\u001b[38;5;241m.\u001b[39mupdate_tracking(\n\u001b[0;32m    176\u001b[0m     tracking_id, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating vector index...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m )\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_tracker\u001b[38;5;241m.\u001b[39mstop_tracking(\n\u001b[0;32m    181\u001b[0m     tracking_id,\n\u001b[0;32m    182\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    183\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vector_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector_ids\n",
      "File \u001b[1;32m~\\semantica\\semantica\\vector_store\\vector_store.py:313\u001b[0m, in \u001b[0;36mVectorIndexer.create_index\u001b[1;34m(self, vectors, ids, **options)\u001b[0m\n\u001b[0;32m    311\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vectors)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# Simple in-memory index (would use FAISS, etc. in production)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m\"\u001b[39m: vectors, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m: ids \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vectors)))}\n",
      "File \u001b[1;32mc:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 768 and the array at index 5 has size 384"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import (\n",
    "    store_vectors,\n",
    "    search_vectors,\n",
    "    hybrid_search as hybrid_search_func,\n",
    "    update_vectors,\n",
    "    delete_vectors\n",
    ")\n",
    "\n",
    "# Store vectors using convenience function\n",
    "dim = len(query_vector) if 'query_vector' in locals() else 768\n",
    "new_vectors = [np.random.rand(dim) for _ in range(10)]\n",
    "new_metadata = [{\"text\": f\"New doc {i}\"} for i in range(10)]\n",
    "new_ids = store_vectors(new_vectors, metadata=new_metadata, method=\"default\")\n",
    "\n",
    "print(f\"Stored {len(new_ids)} new vectors\")\n",
    "\n",
    "# Search using convenience function\n",
    "search_results = search_vectors(\n",
    "    query_vector,\n",
    "    new_vectors,\n",
    "    new_ids,\n",
    "    k=5,\n",
    "    method=\"default\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Multi-Backend Support\n",
    "\n",
    "Work with different vector store backends.\n",
    "\n",
    "### Supported Backends\n",
    "\n",
    "| Backend | Type | Best For |\n",
    "|---------|------|----------|\n",
    "| FAISS | Local | Development, small datasets |\n",
    "| Weaviate | Self-hosted | Schema-aware storage |\n",
    "| Qdrant | Self-hosted | High performance |\n",
    "| Milvus | Cloud/Self-hosted | Large scale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.vector_store import FAISSAdapter, VectorManager\n",
    "\n",
    "# FAISS (local)\n",
    "faiss_adapter = FAISSAdapter(dimension=768)\n",
    "faiss_index = faiss_adapter.create_index(index_type=\"flat\", metric=\"L2\")\n",
    "print(\"Created FAISS index\")\n",
    "\n",
    "# Vector Manager for multi-store management\n",
    "manager = VectorManager()\n",
    "faiss_store = manager.create_store(\"faiss\", {\"dimension\": 768})\n",
    "print(f\"\\nCreated store via manager\")\n",
    "\n",
    "# List all stores\n",
    "stores = manager.list_stores()\n",
    "print(f\"Active stores: {stores}\")\n",
    "\n",
    "# Note: For cloud/remote backends (Weaviate, Qdrant, etc.),\n",
    "# you would need API keys and endpoints\n",
    "# Example:\n",
    "# from semantica.vector_store import WeaviateAdapter\n",
    "# weaviate = WeaviateAdapter(url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Best Practices\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Normalize Vectors**: Always normalize for cosine similarity\n",
    "2. **Use HNSW**: Best balance for speed/accuracy\n",
    "3. **Batch Operations**: Process in batches (100-1000)\n",
    "4. **Filter First**: Apply metadata filters before vector search\n",
    "\n",
    "### Backend Selection\n",
    "\n",
    "- **Development**: FAISS (local, fast)\n",
    "- **Production**: Weaviate/Qdrant (scalable, self-hosted)\n",
    "- **Self-hosted**: Qdrant or Milvus (control, performance)\n",
    "- **Schema-aware**: Weaviate (rich metadata)\n",
    "\n",
    "### Index Configuration\n",
    "\n",
    "- **Small datasets (<10K)**: Flat index\n",
    "- **Medium datasets (10K-1M)**: HNSW\n",
    "- **Large datasets (>1M)**: IVF + PQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "In this notebook, you've learned how to:\n",
    "\n",
    "- Store and search vectors with VectorStore\n",
    "- Create indices for performance optimization\n",
    "- Use hybrid search with metadata filtering\n",
    "- Manage metadata separately from vectors\n",
    "- Rank and fuse search results\n",
    "- Implement namespace isolation\n",
    "- Use convenience functions for quick operations\n",
    "- Work with multiple backend adapters\n",
    "- Apply best practices for production use\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Multi-Backend**: Choose the right backend for your needs\n",
    "2. **Hybrid Search**: Combine vectors with metadata for precision\n",
    "3. **Indexing**: Use appropriate index types for performance\n",
    "4. **Metadata**: Separate metadata management for flexibility\n",
    "5. **Namespaces**: Isolate vectors for multi-tenancy\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Further Reading**:\n",
    "- [Vector Store API Reference](https://semantica.readthedocs.io/reference/vector_store/)\n",
    "- [Advanced Vector Store Notebook](../advanced/Advanced_Vector_Store_and_Search.ipynb)\n",
    "- [Embedding Generation](12_Embedding_Generation.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or Issues?** Check out our [GitHub repository](https://github.com/Hawksight-AI/semantica) or [documentation](https://semantica.readthedocs.io)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

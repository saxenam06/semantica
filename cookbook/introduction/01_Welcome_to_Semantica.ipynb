{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/introduction/01_Welcome_to_Semantica.ipynb)\n",
    "\n",
    "# Welcome to Semantica\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces you to the **Semantica framework** - a comprehensive knowledge graph and semantic processing framework for building production-ready semantic AI applications.\n",
    "\n",
    "**Documentation**: [Getting Started](https://semantica.readthedocs.io/getting-started/) • [Concepts](https://semantica.readthedocs.io/concepts/) • [API Reference](https://semantica.readthedocs.io/reference/)\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- What Semantica is and why it's useful\n",
    "- How to install and configure the framework\n",
    "- Understanding the framework architecture\n",
    "- Key concepts and terminology\n",
    "- Next steps for getting started\n",
    "\n",
    "## What is Semantica?\n",
    "\n",
    "**Semantica** is a production-ready framework for:\n",
    "\n",
    "- **Building Knowledge Graphs**: Transform unstructured data into structured knowledge graphs\n",
    "- **Semantic Processing**: Extract entities, relationships, and meaning from text, images, and audio\n",
    "- **GraphRAG**: Graph-based retrieval augmented generation\n",
    "- **Temporal Analysis**: Time-aware knowledge graphs\n",
    "- **Multi-Modal Processing**: Handle text, images, audio, and structured data\n",
    "- **Enterprise Features**: Quality assurance, conflict resolution, ontology generation\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Threat intelligence and cybersecurity\n",
    "- Healthcare and medical research\n",
    "- Financial analysis and fraud detection\n",
    "- Supply chain optimization\n",
    "- Research and knowledge management\n",
    "- Multi-agent AI systems\n",
    "\n",
    "\n",
    "## Installation & Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before installing Semantica, ensure you have:\n",
    "- Python 3.8 or higher\n",
    "- pip package manager\n",
    "- (Optional) Virtual environment for isolation\n",
    "\n",
    "### Installation Methods\n",
    "\n",
    "```bash\n",
    "# Method 1: Install from PyPI (Recommended)\n",
    "pip install semantica\n",
    "\n",
    "# Or install with all optional dependencies:\n",
    "pip install semantica[all]\n",
    "\n",
    "# Method 2: Install from source (development version)\n",
    "git clone https://github.com/Hawksight-AI/semantica.git\n",
    "cd semantica\n",
    "pip install -e .\n",
    "\n",
    "# Or with all optional dependencies:\n",
    "pip install -e \".[all]\"\n",
    "\n",
    "# Verify installation\n",
    "import semantica\n",
    "print(semantica.__version__)\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "\n",
    "```bash\n",
    "# Set up environment variables for API keys and configuration\n",
    "# export SEMANTICA_API_KEY=your_openai_key\n",
    "# export SEMANTICA_EMBEDDING_PROVIDER=openai\n",
    "# export SEMANTICA_MODEL_NAME=gpt-4\n",
    "\n",
    "# Or use a config file (config.yaml):\n",
    "# api_keys:\n",
    "#   openai: your_key_here\n",
    "#   anthropic: your_key_here\n",
    "# embedding:\n",
    "#   provider: openai\n",
    "#   model: text-embedding-3-large\n",
    "#   dimensions: 3072\n",
    "# knowledge_graph:\n",
    "#   backend: networkx  # or neo4j, arangodb\n",
    "#   temporal: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Framework Architecture Overview\n",
    "\n",
    "Semantica is organized into modular components, each handling a specific aspect of semantic processing:\n",
    "\n",
    "### 1. INGEST MODULE - Data Ingestion\n",
    "**Purpose**: Ingest data from various sources\n",
    "**Components**:\n",
    "- `FileIngestor`: Read files (PDF, DOCX, HTML, JSON, CSV, etc.)\n",
    "- `WebIngestor`: Scrape and ingest web pages\n",
    "- `FeedIngestor`: Process RSS/Atom feeds\n",
    "- `StreamIngestor`: Real-time data streaming (Kafka, RabbitMQ, Kinesis, Pulsar)\n",
    "- `DBIngestor`: Database queries and ingestion (PostgreSQL, MySQL, SQLite, Oracle, SQL Server)\n",
    "- `EmailIngestor`: Process email messages (IMAP, POP3)\n",
    "- `RepoIngestor`: Git repository analysis\n",
    "- `MCPIngestor`: Model Context Protocol server integration\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.ingest import FileIngestor, WebIngestor, FeedIngestor, StreamIngestor, DBIngestor, EmailIngestor, RepoIngestor, MCPIngestor\n",
    "file_ingestor = FileIngestor()\n",
    "web_ingestor = WebIngestor()\n",
    "documents = file_ingestor.ingest(\"data/\")\n",
    "web_docs = web_ingestor.ingest(\"https://example.com\")\n",
    "```\n",
    "\n",
    "### 2. PARSE MODULE - Document Parsing\n",
    "**Purpose**: Parse and extract content from various formats\n",
    "**Components**:\n",
    "- `DocumentParser`: Main parser orchestrator\n",
    "- `PDFParser`: Extract text, tables, images from PDFs\n",
    "- `DOCXParser`: Parse Word documents\n",
    "- `HTMLParser`: Extract content from HTML\n",
    "- `JSONParser`: Parse structured JSON data\n",
    "- `ExcelParser`: Process spreadsheets\n",
    "- `ImageParser`: OCR and image analysis\n",
    "- `CodeParser`: Parse source code files\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.parse import DocumentParser\n",
    "parser = DocumentParser()\n",
    "parsed_docs = parser.parse(documents)\n",
    "```\n",
    "\n",
    "### 3. NORMALIZE MODULE - Text Normalization\n",
    "**Purpose**: Clean and normalize text for processing\n",
    "**Components**:\n",
    "- `TextNormalizer`: Main normalization orchestrator\n",
    "- `TextCleaner`: Remove noise, fix encoding\n",
    "- `DataCleaner`: Clean structured data\n",
    "- `EntityNormalizer`: Normalize entity names\n",
    "- `DateNormalizer`: Standardize date formats\n",
    "- `NumberNormalizer`: Normalize numeric values\n",
    "- `LanguageDetector`: Detect document language\n",
    "- `EncodingHandler`: Handle character encoding\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.normalize import TextNormalizer\n",
    "normalizer = TextNormalizer()\n",
    "normalized = normalizer.normalize(parsed_docs)\n",
    "```\n",
    "\n",
    "### 4. SEMANTIC_EXTRACT MODULE - Entity & Relationship Extraction\n",
    "**Purpose**: Extract entities, relationships, and semantic information\n",
    "**Components**:\n",
    "- `NERExtractor`: Named Entity Recognition\n",
    "- `RelationExtractor`: Extract relationships between entities\n",
    "- `SemanticAnalyzer`: Deep semantic analysis\n",
    "- `SemanticNetworkExtractor`: Extract semantic networks\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.semantic_extract import NERExtractor, RelationExtractor\n",
    "extractor = NERExtractor()\n",
    "entities = extractor.extract(normalized_docs)\n",
    "relation_extractor = RelationExtractor()\n",
    "relationships = relation_extractor.extract(normalized_docs, entities)\n",
    "```\n",
    "\n",
    "### 5. KG MODULE - Knowledge Graph Construction\n",
    "**Purpose**: Build and manage knowledge graphs\n",
    "**Components**:\n",
    "- `GraphBuilder`: Construct knowledge graphs from entities/relationships\n",
    "- `GraphAnalyzer`: Analyze graph structure and properties\n",
    "- `GraphValidator`: Validate graph quality and consistency\n",
    "- `EntityResolver`: Resolve entity conflicts and duplicates\n",
    "- `ConflictDetector`: Detect conflicting information\n",
    "- `CentralityCalculator`: Calculate node importance metrics\n",
    "- `CommunityDetector`: Detect communities in graphs\n",
    "- `ConnectivityAnalyzer`: Analyze graph connectivity\n",
    "- `TemporalQuery`: Query temporal knowledge graphs\n",
    "- `Deduplicator`: Remove duplicate entities/relationships\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.kg import GraphBuilder, GraphAnalyzer\n",
    "builder = GraphBuilder()\n",
    "kg = builder.build(entities, relationships)\n",
    "analyzer = GraphAnalyzer()\n",
    "metrics = analyzer.analyze(kg)\n",
    "```\n",
    "\n",
    "### 6. EMBEDDINGS MODULE - Embedding Generation\n",
    "**Purpose**: Generate vector embeddings for various data types\n",
    "**Components**:\n",
    "- `EmbeddingGenerator`: Main embedding orchestrator\n",
    "- `TextEmbedder`: Generate text embeddings\n",
    "- `ImageEmbedder`: Generate image embeddings\n",
    "- `AudioEmbedder`: Generate audio embeddings\n",
    "- `MultimodalEmbedder`: Combine multiple modalities\n",
    "- `EmbeddingOptimizer`: Optimize embedding quality\n",
    "- `ProviderAdapters`: Support for OpenAI, Cohere, etc.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.embeddings import EmbeddingGenerator\n",
    "generator = EmbeddingGenerator()\n",
    "embeddings = generator.generate(documents)\n",
    "```\n",
    "\n",
    "### 7. VECTOR_STORE MODULE - Vector Database Operations\n",
    "**Purpose**: Store and search vector embeddings\n",
    "**Components**:\n",
    "- `VectorStore`: Main vector store interface\n",
    "- `FAISSAdapter`: FAISS integration\n",
    "- `HybridSearch`: Combine vector and keyword search\n",
    "- `VectorRetriever`: Retrieve relevant vectors\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.vector_store import VectorStore, HybridSearch\n",
    "vector_store = VectorStore()\n",
    "vector_store.store(embeddings, documents, metadata)\n",
    "hybrid_search = HybridSearch(vector_store)\n",
    "results = hybrid_search.search(query, top_k=10)\n",
    "```\n",
    "\n",
    "### 8. GRAPH_STORE MODULE - Persistent Graph Database Operations\n",
    "**Purpose**: Store and query property graphs in Neo4j, KuzuDB, or FalkorDB\n",
    "**Components**:\n",
    "- `GraphStore`: Main graph store interface\n",
    "- `Neo4jAdapter`: Neo4j integration (enterprise features)\n",
    "- `KuzuAdapter`: KuzuDB integration (embedded, no server)\n",
    "- `FalkorDBAdapter`: FalkorDB integration (Redis-based, ultra-fast)\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.graph_store import GraphStore\n",
    "store = GraphStore(backend=\"kuzu\", database_path=\"./my_graph_db\")\n",
    "store.connect()\n",
    "node = store.create_node([\"Person\"], {\"name\": \"John\", \"age\": 30})\n",
    "store.create_relationship(node[\"id\"], other_id, \"KNOWS\", {\"since\": 2020})\n",
    "results = store.execute_query(\"MATCH (p:Person) RETURN p.name\")\n",
    "store.close()\n",
    "```\n",
    "\n",
    "### 9. REASONING MODULE - Inference and Reasoning\n",
    "**Purpose**: Perform logical inference and reasoning\n",
    "**Components**:\n",
    "- `InferenceEngine`: Main inference orchestrator\n",
    "- `RuleManager`: Manage inference rules\n",
    "- `DeductiveReasoner`: Deductive reasoning\n",
    "- `AbductiveReasoner`: Abductive reasoning\n",
    "- `ExplanationGenerator`: Generate explanations for inferences\n",
    "- `RETEEngine`: RETE algorithm for rule matching\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.reasoning import InferenceEngine, RuleManager\n",
    "inference_engine = InferenceEngine()\n",
    "rule_manager = RuleManager()\n",
    "new_facts = inference_engine.forward_chain(kg, rule_manager)\n",
    "```\n",
    "\n",
    "### 10. ONTOLOGY MODULE - Ontology Generation\n",
    "**Purpose**: Generate and manage ontologies\n",
    "**Components**:\n",
    "- `OntologyGenerator`: Generate ontologies from knowledge graphs\n",
    "- `OntologyValidator`: Validate ontology structure\n",
    "- `OWLGenerator`: Generate OWL format ontologies\n",
    "- `PropertyGenerator`: Generate ontology properties\n",
    "- `ClassInferrer`: Infer ontology classes\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.ontology import OntologyGenerator\n",
    "generator = OntologyGenerator()\n",
    "ontology = generator.generate_from_graph(kg)\n",
    "```\n",
    "\n",
    "### 11. EXPORT MODULE - Data Export\n",
    "**Purpose**: Export data in various formats\n",
    "**Components**:\n",
    "- `JSONExporter`: Export to JSON\n",
    "- `RDFExporter`: Export to RDF/XML\n",
    "- `CSVExporter`: Export to CSV\n",
    "- `GraphExporter`: Export to graph formats (GraphML, GEXF)\n",
    "- `OWLExporter`: Export to OWL\n",
    "- `VectorExporter`: Export vectors\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.export import JSONExporter, RDFExporter\n",
    "json_exporter = JSONExporter()\n",
    "json_exporter.export(kg, \"output.json\")\n",
    "```\n",
    "\n",
    "### 12. VISUALIZATION MODULE - Graph Visualization\n",
    "**Purpose**: Visualize knowledge graphs and analytics\n",
    "**Components**:\n",
    "- `KGVisualizer`: Visualize knowledge graphs\n",
    "- `EmbeddingVisualizer`: Visualize embeddings (t-SNE, PCA, UMAP)\n",
    "- `QualityVisualizer`: Visualize quality metrics\n",
    "- `AnalyticsVisualizer`: Visualize graph analytics\n",
    "- `TemporalVisualizer`: Visualize temporal data\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.visualization import KGVisualizer\n",
    "visualizer = KGVisualizer()\n",
    "visualizer.visualize(kg)\n",
    "```\n",
    "\n",
    "### 13. PIPELINE MODULE - Pipeline Orchestration\n",
    "**Purpose**: Build and execute processing pipelines\n",
    "**Components**:\n",
    "- `PipelineBuilder`: Build complex pipelines\n",
    "- `ExecutionEngine`: Execute pipelines\n",
    "- `FailureHandler`: Handle pipeline failures\n",
    "- `ParallelismManager`: Enable parallel processing\n",
    "- `ResourceScheduler`: Schedule resources\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from semantica.pipeline import PipelineBuilder\n",
    "builder = PipelineBuilder()\n",
    "pipeline = builder.add_step(\"ingest\", FileIngestor()) \\\\\n",
    "                  .add_step(\"parse\", DocumentParser()) \\\\\n",
    "                  .build()\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

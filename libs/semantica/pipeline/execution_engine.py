"""
Pipeline execution engine for Semantica framework.

This module provides pipeline execution and orchestration
for complex data processing workflows.
"""

from typing import Any, Dict, List, Optional, Callable
from enum import Enum
from dataclasses import dataclass, field
from datetime import datetime
import threading
import time

from ..utils.exceptions import ValidationError, ProcessingError
from ..utils.logging import get_logger
from .pipeline_builder import Pipeline, PipelineStep, StepStatus
from .failure_handler import FailureHandler
from .parallelism_manager import ParallelismManager
from .resource_scheduler import ResourceScheduler


class PipelineStatus(Enum):
    """Pipeline execution status."""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    STOPPED = "stopped"


@dataclass
class ExecutionResult:
    """Pipeline execution result."""
    success: bool
    output: Any
    metadata: Dict[str, Any] = field(default_factory=dict)
    metrics: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)


class ExecutionEngine:
    """
    Pipeline execution engine.
    
    • Pipeline execution and orchestration
    • Task scheduling and management
    • Resource allocation and monitoring
    • Performance optimization
    • Error handling and recovery
    • Parallel and distributed execution
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, **kwargs):
        """
        Initialize execution engine.
        
        Args:
            config: Configuration dictionary
            **kwargs: Additional configuration options:
                - max_workers: Maximum parallel workers
                - retry_on_failure: Enable retry on failure
        """
        self.logger = get_logger("execution_engine")
        self.config = config or {}
        self.config.update(kwargs)
        
        self.failure_handler = FailureHandler(**self.config)
        self.parallelism_manager = ParallelismManager(**self.config)
        self.resource_scheduler = ResourceScheduler(**self.config)
        
        self.running_pipelines: Dict[str, Pipeline] = {}
        self.pipeline_status: Dict[str, PipelineStatus] = {}
        self.pipeline_lock = threading.Lock()
    
    def execute_pipeline(
        self,
        pipeline: Pipeline,
        data: Any = None,
        **options
    ) -> ExecutionResult:
        """
        Execute pipeline.
        
        Args:
            pipeline: Pipeline object
            data: Input data
            **options: Execution options
        
        Returns:
            Execution result
        """
        pipeline_id = pipeline.name
        start_time = time.time()
        
        try:
            self.logger.info(f"Executing pipeline: {pipeline_id}")
            
            # Set status
            with self.pipeline_lock:
                self.pipeline_status[pipeline_id] = PipelineStatus.RUNNING
                self.running_pipelines[pipeline_id] = pipeline
            
            # Allocate resources
            resources = self.resource_scheduler.allocate_resources(pipeline, **options)
            
            try:
                # Execute steps
                result = self._execute_steps(pipeline, data, **options)
                
                # Collect metrics
                execution_time = time.time() - start_time
                metrics = {
                    "execution_time": execution_time,
                    "steps_executed": len([s for s in pipeline.steps if s.status == StepStatus.COMPLETED]),
                    "steps_failed": len([s for s in pipeline.steps if s.status == StepStatus.FAILED])
                }
                
                # Update status
                with self.pipeline_lock:
                    if metrics["steps_failed"] == 0:
                        self.pipeline_status[pipeline_id] = PipelineStatus.COMPLETED
                    else:
                        self.pipeline_status[pipeline_id] = PipelineStatus.FAILED
                
                return ExecutionResult(
                    success=metrics["steps_failed"] == 0,
                    output=result,
                    metadata={
                        "pipeline_id": pipeline_id,
                        "execution_time": execution_time
                    },
                    metrics=metrics
                )
                
            finally:
                # Release resources
                self.resource_scheduler.release_resources(resources)
                
        except Exception as e:
            self.logger.error(f"Pipeline execution failed: {e}")
            with self.pipeline_lock:
                self.pipeline_status[pipeline_id] = PipelineStatus.FAILED
            
            return ExecutionResult(
                success=False,
                output=None,
                errors=[str(e)]
            )
    
    def _execute_steps(
        self,
        pipeline: Pipeline,
        data: Any,
        **options
    ) -> Any:
        """Execute pipeline steps."""
        # Sort steps by dependencies (topological sort)
        sorted_steps = self._topological_sort(pipeline.steps)
        
        # Execute steps
        current_data = data
        for step in sorted_steps:
            if self.pipeline_status.get(pipeline.name) == PipelineStatus.STOPPED:
                break
            
            # Wait if paused
            while self.pipeline_status.get(pipeline.name) == PipelineStatus.PAUSED:
                time.sleep(0.1)
            
            try:
                # Execute step
                step.status = StepStatus.RUNNING
                step_result = self._execute_step(step, current_data, **options)
                step.status = StepStatus.COMPLETED
                step.result = step_result
                current_data = step_result
                
            except Exception as e:
                step.status = StepStatus.FAILED
                step.error = e
                
                # Handle failure
                recovery_result = self.failure_handler.handle_step_failure(step, e)
                if not recovery_result.get("retry", False):
                    raise
                else:
                    # Retry step
                    step.status = StepStatus.RUNNING
                    step_result = self._execute_step(step, current_data, **options)
                    step.status = StepStatus.COMPLETED
                    step.result = step_result
                    current_data = step_result
        
        return current_data
    
    def _execute_step(
        self,
        step: PipelineStep,
        data: Any,
        **options
    ) -> Any:
        """Execute a single step."""
        if step.handler:
            return step.handler(data, **step.config, **options)
        else:
            # Default: pass data through
            return data
    
    def _topological_sort(self, steps: List[PipelineStep]) -> List[PipelineStep]:
        """Sort steps by dependencies (topological sort)."""
        # Build dependency graph
        step_map = {step.name: step for step in steps}
        in_degree = {step.name: len(step.dependencies) for step in steps}
        
        # Find steps with no dependencies
        queue = [step for step in steps if in_degree[step.name] == 0]
        sorted_steps = []
        
        while queue:
            step = queue.pop(0)
            sorted_steps.append(step)
            
            # Update in-degrees of dependent steps
            for other_step in steps:
                if step.name in other_step.dependencies:
                    in_degree[other_step.name] -= 1
                    if in_degree[other_step.name] == 0:
                        queue.append(other_step)
        
        # Check for cycles
        if len(sorted_steps) != len(steps):
            raise ValidationError("Circular dependency detected in pipeline")
        
        return sorted_steps
    
    def pause_pipeline(self, pipeline_id: str) -> None:
        """Pause pipeline execution."""
        with self.pipeline_lock:
            if pipeline_id in self.pipeline_status:
                if self.pipeline_status[pipeline_id] == PipelineStatus.RUNNING:
                    self.pipeline_status[pipeline_id] = PipelineStatus.PAUSED
                    self.logger.info(f"Paused pipeline: {pipeline_id}")
    
    def resume_pipeline(self, pipeline_id: str) -> None:
        """Resume paused pipeline."""
        with self.pipeline_lock:
            if pipeline_id in self.pipeline_status:
                if self.pipeline_status[pipeline_id] == PipelineStatus.PAUSED:
                    self.pipeline_status[pipeline_id] = PipelineStatus.RUNNING
                    self.logger.info(f"Resumed pipeline: {pipeline_id}")
    
    def stop_pipeline(self, pipeline_id: str) -> None:
        """Stop pipeline execution."""
        with self.pipeline_lock:
            if pipeline_id in self.pipeline_status:
                self.pipeline_status[pipeline_id] = PipelineStatus.STOPPED
                self.logger.info(f"Stopped pipeline: {pipeline_id}")
    
    def get_pipeline_status(self, pipeline_id: str) -> Optional[PipelineStatus]:
        """Get pipeline status."""
        return self.pipeline_status.get(pipeline_id)
    
    def get_progress(self, pipeline_id: str) -> Dict[str, Any]:
        """Get pipeline execution progress."""
        if pipeline_id not in self.running_pipelines:
            return {}
        
        pipeline = self.running_pipelines[pipeline_id]
        total_steps = len(pipeline.steps)
        completed_steps = len([s for s in pipeline.steps if s.status == StepStatus.COMPLETED])
        
        return {
            "total_steps": total_steps,
            "completed_steps": completed_steps,
            "progress_percentage": (completed_steps / total_steps * 100) if total_steps > 0 else 0.0,
            "status": self.pipeline_status.get(pipeline_id, PipelineStatus.PENDING).value
        }


class ProgressTracker:
    """Progress tracking for pipeline execution."""
    
    def __init__(self, **config):
        """Initialize progress tracker."""
        self.logger = get_logger("progress_tracker")
        self.config = config
        self.tracking_data: Dict[str, Dict[str, Any]] = {}
    
    def track_progress(self, pipeline_id: str, step_name: str, progress: float) -> None:
        """Track progress for a pipeline step."""
        if pipeline_id not in self.tracking_data:
            self.tracking_data[pipeline_id] = {}
        
        self.tracking_data[pipeline_id][step_name] = {
            "progress": progress,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_completion_percentage(self, pipeline_id: str) -> float:
        """Get overall completion percentage."""
        if pipeline_id not in self.tracking_data:
            return 0.0
        
        steps = self.tracking_data[pipeline_id]
        if not steps:
            return 0.0
        
        total_progress = sum(step["progress"] for step in steps.values())
        return total_progress / len(steps)
    
    def estimate_remaining_time(
        self,
        pipeline_id: str,
        start_time: float
    ) -> Optional[float]:
        """Estimate remaining execution time."""
        completion = self.get_completion_percentage(pipeline_id)
        if completion == 0:
            return None
        
        elapsed = time.time() - start_time
        estimated_total = elapsed / completion
        remaining = estimated_total - elapsed
        
        return max(0, remaining)
